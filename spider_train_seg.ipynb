{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add training py scripts to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "path_prefix = \"/home/kanthoulis/spider/\"\n",
    "\n",
    "training_scripts_dir = path_prefix + \"training\"\n",
    "transforms_dir = path_prefix + \"transforms\"\n",
    "image_dir = path_prefix + \"image\"\n",
    "\n",
    "sys.path.append(training_scripts_dir)\n",
    "sys.path.append(transforms_dir)\n",
    "sys.path.append(image_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanthoulis/jupyter-venv/lib/python3.12/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.23). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x70418fff8d90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UNet \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import json \n",
    "import numpy as np\n",
    "import re \n",
    "import os\n",
    "import pathlib\n",
    "from natsort import natsorted\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "#model imports \n",
    "import unet\n",
    "\n",
    "import dataset_torch_numpy as dataset\n",
    "import dataset_torch_numpy_aug as dataset_aug\n",
    "import metric\n",
    "#import epoch as ep #ep not to conlfict with var name in loop\n",
    "\n",
    "#Set GPU/Cuda Device to run model on\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "torch.manual_seed(46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tensor data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "json_path = \"/home/kanthoulis/spider/tensor_data/tensor_data.json\" \n",
    "\n",
    "#Load tensor parameters from .json\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "#Assign each value to a variable\n",
    "row_max = data[\"row_max\"]\n",
    "col_max = data[\"col_max\"]\n",
    "image_tensor_min = data[\"image_tensor_min\"]\n",
    "image_tensor_max = data[\"image_tensor_max\"]\n",
    "label_tensor_min = data[\"label_tensor_min\"]\n",
    "label_tensor_max = data[\"label_tensor_max\"]\n",
    "masks_no = data[\"masks_no\"]\n",
    "masks_array = data[\"masks_array\"]\n",
    "\n",
    "print(masks_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset len 11484\n",
      "test dataset len 3197\n"
     ]
    }
   ],
   "source": [
    "path_prefix = \"/home/kanthoulis/spider/dataset/\"\n",
    "\n",
    "train_img_slice_dir = pathlib.Path(path_prefix + \"train_image_numpy\") #TODO SET PATH IN JHUB\n",
    "train_label_slice_dir = pathlib.Path(path_prefix + \"train_label_numpy\") #TODO SET PATH IN JHUB \n",
    "test_img_slice_dir = pathlib.Path(path_prefix + \"test_image_numpy\") #TODO SET PATH IN JHUB\n",
    "test_label_slice_dir= pathlib.Path(path_prefix + \"test_label_numpy\") \n",
    "\n",
    "#Create lists of filenames in directories (str)\n",
    "image_path = train_img_slice_dir\n",
    "label_path = test_label_slice_dir\n",
    "\n",
    "image_dir_list = os.listdir(image_path)\n",
    "label_dir_list = os.listdir(label_path)\n",
    "\n",
    "#Sort lists\n",
    "image_dir_list = natsorted(image_dir_list)\n",
    "label_dir_list = natsorted(label_dir_list)\n",
    "\n",
    "print(\"train dataset len\",image_dir_list.__len__())\n",
    "print(\"test dataset len\",label_dir_list.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    # Apply softmax to predictions (logits to probabilities)\n",
    "    #print(\"dims\", pred.shape)  \n",
    "    pred = torch.softmax(pred, dim=1)  # (N, C, H, W)\n",
    "    \n",
    "    # Compute Dice Loss\n",
    "    intersection = torch.sum(pred * target, dim=(2, 3))  # Sum over spatial dimensions\n",
    "    union = torch.sum(pred, dim=(2, 3)) + torch.sum(target, dim=(2, 3))  # Union over spatial dims\n",
    "\n",
    "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return 1.0 - dice.mean()  # Return mean Dice loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nclass CombinedLoss(torch.nn.Module):\\n    def __init__(self, ce_weight=0.5, dice_weight=0.5):\\n        super(CombinedLoss, self).__init__()\\n        self.ce_weight = ce_weight\\n        self.dice_weight = dice_weight\\n        self.ce_loss = torch.nn.CrossEntropyLoss()\\n        #self.ce_loss = nn.BCEWithLogitsLoss() \\n\\n    def forward(self, outputs, targets):\\n        # Cross Entropy Loss expects class probabilities and the raw target labels (not one-hot)\\n        ce_loss = self.ce_loss(outputs, torch.argmax(targets, dim=1))\\n        \\n        # Dice Loss expects one-hot encoded targets and softmax probabilities\\n        dice_loss_val = dice_loss(outputs, targets)\\n        \\n        # Weighted sum of the two losses\\n        return self.ce_weight * ce_loss + self.dice_weight * dice_loss_val\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CrossEntropyLoss\n",
    "\"\"\"\n",
    "\n",
    "class CombinedLoss(torch.nn.Module):\n",
    "    def __init__(self, ce_weight=0.5, dice_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        #self.ce_loss = nn.BCEWithLogitsLoss() \n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Cross Entropy Loss expects class probabilities and the raw target labels (not one-hot)\n",
    "        ce_loss = self.ce_loss(outputs, torch.argmax(targets, dim=1))\n",
    "        \n",
    "        # Dice Loss expects one-hot encoded targets and softmax probabilities\n",
    "        dice_loss_val = dice_loss(outputs, targets)\n",
    "        \n",
    "        # Weighted sum of the two losses\n",
    "        return self.ce_weight * ce_loss + self.dice_weight * dice_loss_val\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BCEWithLogits\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, ce_weight=0.5, dice_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Compute the combined loss.\n",
    "        Args:\n",
    "            outputs: Logits from the model (shape: B x C x H x W).\n",
    "            targets: Ground truth labels (shape: B x C x H x W, binary).\n",
    "        Returns:\n",
    "            Combined loss value.\n",
    "        \"\"\"\n",
    "        # Compute BCEWithLogitsLoss\n",
    "        ce_loss = self.ce_loss(outputs, targets)\n",
    "        \n",
    "        # Compute Dice Loss\n",
    "        dice_loss_val = dice_loss(outputs, targets)\n",
    "        \n",
    "        # Weighted sum of the two losses\n",
    "        return self.ce_weight * ce_loss + self.dice_weight * dice_loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet Torch - Model Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedLoss(\n",
       "  (ce_loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input Output channels\n",
    "input_channels = 1 #Hounsfield scale, do not modify\n",
    "output_channels = masks_no - 1 #-1 to exclude backround value 0, do not modify\\\n",
    "\n",
    "#Model Hyperparams \n",
    "depth = 5\n",
    "start_filts = 32\n",
    "\n",
    "up_mode = 'upsample'\n",
    "lr = 0.0001 #adjust according to batch size \n",
    "batchsize = 16 #find optimal setting for jhub gpu(s)\n",
    "#loss_func = nn.BCEWithLogitsLoss() \n",
    "#loss_func = nn.CrossEntropyLoss()\n",
    "loss_func = CombinedLoss(ce_weight=0.5, dice_weight=0.5)\n",
    "loss_func.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet Torch - Model & Optimizer Init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanthoulis/spider/training/unet.py:213: FutureWarning: `nn.init.xavier_normal` is now deprecated in favor of `nn.init.xavier_normal_`.\n",
      "  init.xavier_normal(m.weight)\n",
      "/home/kanthoulis/spider/training/unet.py:214: FutureWarning: `nn.init.constant` is now deprecated in favor of `nn.init.constant_`.\n",
      "  init.constant(m.bias, 0)\n"
     ]
    }
   ],
   "source": [
    "model = unet.UNet(in_channels= input_channels,num_classes=output_channels, depth= depth, start_filts=start_filts, up_mode=up_mode) \n",
    "#optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = nn.DataParallel(model) #parallelize between the 2 GPUs, not the best iteration but it works for this use case\n",
    "model.to(device)\n",
    "model.to(torch.float32)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.001) #AdamW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet Torch - Load Model and Optim states for resuming training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST TRAINING SESSION ONLY\n",
    "epoch_no = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_606269/2501647748.py:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#last epoch trained path\\npath = \"/home/kanthoulis/spider/models/spider_seg_29\" \\n\\ncheckpoint= torch.load(path)\\nprint(checkpoint.keys())\\n#optim = torch.optim.Adam(model.parameters(), lr=lr)\\noptim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.001)\\n\\ndef extract_number_from_path(path):\\n    match = re.search(r\\'(\\\\d+)$\\', path)\\n    return int(match.group(1)) if match else None\\n\\nepoch_no = extract_number_from_path(path) + 1 #number for plotting in tb\\nprint(epoch_no)\\n\\nmodel.load_state_dict(checkpoint[\\'model_dict\\'])\\noptim.load_state_dict(checkpoint[\\'optimizer_dict\\'])\\n\\n\\nmodel.to(device)\\nmodel.to(torch.float32)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE: a simple script to get the file w the highest idx in the directory wouldn't be hard to iterate but for now\n",
    "    #just focusing on training the model, maybe work on it further down the line \n",
    "\n",
    "#TODO also load scheduler when resuming training\n",
    "\"\"\"\n",
    "#last epoch trained path\n",
    "path = \"/home/kanthoulis/spider/models/spider_seg_29\" \n",
    "\n",
    "checkpoint= torch.load(path)\n",
    "print(checkpoint.keys())\n",
    "#optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "\n",
    "def extract_number_from_path(path):\n",
    "    match = re.search(r'(\\d+)$', path)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "epoch_no = extract_number_from_path(path) + 1 #number for plotting in tb\n",
    "print(epoch_no)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_dict'])\n",
    "optim.load_state_dict(checkpoint['optimizer_dict'])\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "model.to(torch.float32)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet Torch - Datasets & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11488\n",
      "3200\n"
     ]
    }
   ],
   "source": [
    "#Datasets \n",
    "#train_set = dataset.SpiderDatasetNumpy(train_label_slice_dir, train_img_slice_dir)\n",
    "train_set = dataset_aug.SpiderDatasetNumpyAug(train_label_slice_dir, train_img_slice_dir)\n",
    "test_set = dataset.SpiderDatasetNumpy(test_label_slice_dir, test_img_slice_dir)\n",
    "\n",
    "#Dataloaders\n",
    "train_dataloader = DataLoader(train_set, batch_size = batchsize, shuffle=True) \n",
    "test_dataloader = DataLoader(test_set, batch_size = batchsize, shuffle=True)\n",
    "\n",
    "print(len(train_dataloader) * batchsize)\n",
    "print(len(test_dataloader) * batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#CrossEntropyLoss()\\nmetric_calculator = metric.SegmentationMetrics(average=True, ignore_background=True,activation='softmax') #for calculating \\nmetric_calculator_binary = metric.BinaryMetrics() #for calculating spinal canal metrics since it's only 1 class\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BCE\n",
    "metric_calculator = metric.SegmentationMetrics(average=True, ignore_background=True,activation='sigmoid') #for calculating \n",
    "metric_calculator_binary = metric.BinaryMetrics(activation='sigmoid') #for calculating spinal canal metrics since it's only 1 class\n",
    "\"\"\"\n",
    "#CrossEntropyLoss()\n",
    "metric_calculator = metric.SegmentationMetrics(average=True, ignore_background=True,activation='softmax') #for calculating \n",
    "metric_calculator_binary = metric.BinaryMetrics() #for calculating spinal canal metrics since it's only 1 class\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Scheduler setup\n",
    "T_0 = len(train_dataloader)  # One epoch per cycle\n",
    "T_mult = 2                   # Each subsequent cycle is 2x longer than the previous\n",
    "eta_min = 1e-6               # Minimum learning rate\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer=optim,\n",
    "    T_0=T_0,\n",
    "    T_mult=T_mult,\n",
    "    eta_min=eta_min\n",
    ")\n",
    "\n",
    "#scheduler = CosineAnnealingLR(optim, T_max=len(train_dataloader) * epochs, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps  = 4 #4*16 = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set no. of epochs to train for training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 150/718 [01:28<05:31,  1.72it/s]"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H')\n",
    "#writer = SummaryWriter('runs/spider_seg_unet_epochs={}_lr={}_batchsize={}_loss=BCEWithLogits_startfilts={}_upmode={}'.format(epochs,lr, batchsize,start_filts,up_mode))\n",
    "tb_writer = SummaryWriter('/home/kanthoulis/spider/runs/spider_seg'.format(timestamp)) #for creating new runs \n",
    "#tb_writer = SummaryWriter('/home/kanthoulis/spider/runs/spider_seg_20250106_13') #continue where left off\n",
    "#tb_writer = SummaryWriter('/home/kanthoulis/spider/runs/junk') #continue where left off\n",
    "epoch_number = epoch_no #set to no of last epoch completed to have consistent tb plots \n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    #----------------------Train Loop---------------------------------\n",
    "    model.train(True) #set model to train\n",
    "    #Instantiate training metrics per epoch\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    running_accu = 0.\n",
    "    running_dice = 0.\n",
    "    #running_precision = 0.\n",
    "    #running_recall = 0.\n",
    "\n",
    "    vert_running_accu = 0.\n",
    "    vert_running_dice = 0.\n",
    "\n",
    "    sc_running_accu = 0.\n",
    "    sc_running_dice = 0.\n",
    "\n",
    "    ivd_running_accu = 0.\n",
    "    ivd_running_dice = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "\n",
    "    #swap train dataloader for dset\n",
    "    for i, data in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optim.zero_grad()\n",
    "\n",
    "        \n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        #print(\"outputs shape\", outputs.shape)\n",
    "        \n",
    "        #input output label shape [8 batchsize, 19 classes, row, col]\n",
    "\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \"\"\"\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optim.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        \"\"\"\n",
    "\n",
    "                # Scale the loss by the accumulation steps\n",
    "        loss = loss / accumulation_steps  # Scale down the loss by the number of accumulation steps\n",
    "\n",
    "        # Backpropagate (accumulate gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Only update weights after the specified number of accumulation steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optim.step()  # Update model weights\n",
    "            scheduler.step()  # Adjust the learning rate if necessary TESTING NO SCHEDULER \n",
    "            optim.zero_grad()  # Reset gradients for the next accumulation cycle\n",
    "        \n",
    "        #General metrics\n",
    "        accu, dice, prec, recall = metric_calculator(labels, outputs)\n",
    "\n",
    "        #Vertebrae Metrics (0-9)\n",
    "        vert_labels = labels[:, :9, :, :]\n",
    "        vert_outputs = outputs[:, :9, :, :]\n",
    "        vert_accu, vert_dice, vert_prec, vert_recall = metric_calculator(vert_labels, vert_outputs)\n",
    "\n",
    "        #Spinal Canal Metrics (10)\n",
    "        sc_labels = labels[:, 10, :, :].unsqueeze(1)\n",
    "        sc_outputs = outputs[:, 10, :, :].unsqueeze(1)\n",
    "        sc_accu, sc_dice, sc_prec, sc_specificity, sc_recall = metric_calculator_binary(sc_labels, sc_outputs)\n",
    "\n",
    "        #IVD Metrics (11-19)\n",
    "        ivd_labels = labels[:, -9:, :, :]\n",
    "        ivd_outputs = outputs[:, -9:, :, :]\n",
    "        ivd_accu, ivd_dice, ivd_prec, ivd_recall = metric_calculator(ivd_labels, ivd_outputs)\n",
    "\n",
    "\n",
    "        # Gather data and report\n",
    "        #General\n",
    "        running_loss += loss.item()\n",
    "        running_accu += accu\n",
    "        running_dice += dice\n",
    "        #running_dice += dice\n",
    "        #running_precision += prec\n",
    "        #running_recall += recall\n",
    "\n",
    "        #Vertebrae\n",
    "        vert_running_accu += vert_accu\n",
    "        vert_running_dice += vert_dice\n",
    "\n",
    "        #Spinal Canal\n",
    "        sc_running_accu += sc_accu\n",
    "        sc_running_dice += sc_dice\n",
    "\n",
    "        #IVD\n",
    "        ivd_running_accu += ivd_accu\n",
    "        ivd_running_dice += ivd_dice\n",
    "    \n",
    "        #if (i) % 50 == 0:\n",
    "            #print(\"batch\" ,i )\n",
    "\n",
    "        # every 50 batches \n",
    "        if i % 50 == 49: \n",
    "            \n",
    "            last_loss = running_loss / 50\n",
    "            last_accu = running_accu / 50\n",
    "            last_dice = running_dice / 50\n",
    "\n",
    "            vert_last_accu = vert_running_accu / 50\n",
    "            vert_last_dice = vert_running_dice / 50\n",
    "\n",
    "            sc_last_accu = sc_running_accu / 50\n",
    "            sc_last_dice = sc_running_dice / 50\n",
    "\n",
    "            ivd_last_accu = ivd_running_accu / 50\n",
    "            ivd_last_dice = ivd_running_dice / 50\n",
    "\n",
    "            tb_x = epoch_number * len(train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "\n",
    "            tb_writer.add_scalar('General/accuracy_train', last_accu, tb_x)\n",
    "            tb_writer.add_scalar('General/dice_train', last_dice, tb_x)\n",
    "\n",
    "            tb_writer.add_scalar('Vertebrae/accuracy_train', vert_last_accu, tb_x)\n",
    "            tb_writer.add_scalar('Vertebrae/dice_train', vert_last_dice, tb_x)\n",
    "\n",
    "            tb_writer.add_scalar('Spinal Canal/accuracy_train', sc_last_accu, tb_x)\n",
    "            tb_writer.add_scalar('Spinal Canal/dice_train', sc_last_dice, tb_x)\n",
    "\n",
    "            tb_writer.add_scalar('Intervertebral Discs/accuracy_train', ivd_last_accu, tb_x)\n",
    "            tb_writer.add_scalar('Intervertebral Discs/dice_train', ivd_last_dice, tb_x)\n",
    "\n",
    "            running_loss = 0.\n",
    "            running_accu = 0.\n",
    "            running_dice = 0.\n",
    "\n",
    "            vert_running_accu = 0.\n",
    "            vert_running_dice = 0.\n",
    "\n",
    "            sc_running_accu = 0.\n",
    "            sc_running_dice = 0.\n",
    "\n",
    "            ivd_running_accu = 0.\n",
    "            ivd_running_dice = 0.\n",
    "\n",
    "    #-----------------Eval loop------------------------------------\n",
    "    #print(\"avg loss in epoch\", avg_loss)\n",
    "\n",
    "    # Initialize training metrics\n",
    "    running_tloss = 0.0\n",
    "    running_taccu = 0.0\n",
    "    running_tdice = 0.0\n",
    "    \n",
    "    vert_running_taccu = 0.0\n",
    "    vert_running_tdice = 0.0\n",
    "    \n",
    "    sc_running_taccu = 0.0\n",
    "    sc_running_tdice = 0.0\n",
    "    \n",
    "    ivd_running_taccu = 0.0\n",
    "    ivd_running_tdice = 0.0\n",
    "    \n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_vaccu = 0.0\n",
    "    running_vdice = 0.0\n",
    "\n",
    "    vert_running_vaccu = 0.0\n",
    "    vert_running_vdice = 0.0\n",
    "    \n",
    "    sc_running_vaccu = 0.0\n",
    "    sc_running_vdice = 0.0\n",
    "\n",
    "    ivd_running_vaccu = 0.0\n",
    "    ivd_running_vdice = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for t, tdata in enumerate(tqdm(train_dataloader, desc=\"Evaluating Training Set\")):\n",
    "            tinputs, tlabels = tdata\n",
    "    \n",
    "            # Make predictions for this batch\n",
    "            toutputs = model(tinputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            tloss = loss_func(toutputs, tlabels)\n",
    "            \n",
    "            # General Metrics\n",
    "            taccu, tdice, tprec, trecall = metric_calculator(tlabels, toutputs)\n",
    "    \n",
    "            # Vertebrae Metrics (0-9)\n",
    "            vert_tlabels = tlabels[:, :9, :, :]\n",
    "            vert_toutputs = toutputs[:, :9, :, :]\n",
    "            vert_taccu, vert_tdice, vert_tprec, vert_trecall = metric_calculator(vert_tlabels, vert_toutputs)\n",
    "    \n",
    "            # Spinal Canal Metrics (10)\n",
    "            sc_tlabels = tlabels[:, 10, :, :].unsqueeze(1)\n",
    "            sc_toutputs = toutputs[:, 10, :, :].unsqueeze(1)\n",
    "            sc_taccu, sc_tdice, sc_tprec, sc_tspecificity, sc_trecall = metric_calculator_binary(sc_tlabels, sc_toutputs)\n",
    "    \n",
    "            # IVD Metrics (11-19)\n",
    "            ivd_tlabels = tlabels[:, -9:, :, :]\n",
    "            ivd_toutputs = toutputs[:, -9:, :, :]\n",
    "            ivd_taccu, ivd_tdice, ivd_tprec, ivd_trecall = metric_calculator(ivd_tlabels, ivd_toutputs)\n",
    "    \n",
    "            # Accumulate metrics\n",
    "            running_tloss += tloss.item()\n",
    "            running_taccu += taccu\n",
    "            running_tdice += tdice\n",
    "    \n",
    "            vert_running_taccu += vert_taccu\n",
    "            vert_running_tdice += vert_tdice\n",
    "    \n",
    "            sc_running_taccu += sc_taccu\n",
    "            sc_running_tdice += sc_tdice\n",
    "    \n",
    "            ivd_running_taccu += ivd_taccu\n",
    "            ivd_running_tdice += ivd_tdice\n",
    "\n",
    "\n",
    " \n",
    "        #eval metrics\n",
    "        for j, vdata in enumerate(tqdm(test_dataloader, desc=\"Evaluating Test Set\")):\n",
    "\n",
    "           # if( j % 20 == 0):\n",
    "               # print(\"valuation test set batch\", j)\n",
    "            vinputs, vlabels = vdata\n",
    "\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_func(voutputs, vlabels)\n",
    "            \n",
    "            vaccu, vdice, vprec, vrecall = metric_calculator(vlabels, voutputs)\n",
    "\n",
    "            #Vertebrae Metrics (0-9)\n",
    "            vert_vlabels = vlabels[:, :9, :, :]\n",
    "            vert_voutputs = voutputs[:, :9, :, :]\n",
    "            vert_vaccu, vert_vdice, vert_vprec, vert_vrecall = metric_calculator(vert_vlabels, vert_voutputs)\n",
    "\n",
    "            #Spinal Canal Metrics (10)\n",
    "            sc_vlabels = vlabels[:, 10, :, :].unsqueeze(1)\n",
    "            sc_voutputs = voutputs[:, 10, :, :].unsqueeze(1)\n",
    "            sc_vaccu, sc_vdice, sc_vprec, sv_vspecif, sc_vrecall = metric_calculator_binary(sc_vlabels, sc_voutputs)\n",
    "\n",
    "            #IVD Metrics (11-19)\n",
    "            ivd_vlabels = vlabels[:, -9:, :, :]\n",
    "            ivd_voutputs = voutputs[:, -9:, :, :]\n",
    "            ivd_vaccu, ivd_vdice, ivd_vprec, ivd_vrecall = metric_calculator(ivd_vlabels, ivd_voutputs)\n",
    "\n",
    "\n",
    "            running_vloss += vloss\n",
    "            running_vaccu += vaccu\n",
    "            running_vdice += vdice\n",
    "\n",
    "            vert_running_vaccu += vert_vaccu\n",
    "            vert_running_vdice += vert_vdice\n",
    "\n",
    "            sc_running_vaccu += sc_vaccu\n",
    "            sc_running_vdice += vert_vdice\n",
    "\n",
    "            ivd_running_vaccu += ivd_vaccu\n",
    "            ivd_running_vdice += ivd_vdice\n",
    "\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_tloss = running_tloss / (t + 1)\n",
    "    avg_taccu = running_taccu / (t + 1)\n",
    "    avg_tdice = running_tdice / (t + 1)\n",
    "\n",
    "    vert_avg_taccu = vert_running_taccu / (t + 1)\n",
    "    vert_avg_tdice = vert_running_tdice / (t + 1)\n",
    "\n",
    "    sc_avg_taccu = sc_running_taccu / (t + 1)\n",
    "    sc_avg_tdice = sc_running_tdice / (t + 1)\n",
    "\n",
    "    ivd_avg_taccu = ivd_running_taccu / (t + 1)\n",
    "    ivd_avg_tdice = ivd_running_tdice / (t + 1)\n",
    "\n",
    "    avg_vloss = running_vloss / (j + 1)\n",
    "    avg_vaccu = running_vaccu / (j + 1)\n",
    "    avg_vdice = running_vdice / (j + 1)\n",
    "\n",
    "    vert_avg_vaccu = vert_running_vaccu / (j + 1)\n",
    "    vert_avg_vdice = vert_running_vdice / (j + 1)\n",
    "\n",
    "    sc_avg_vaccu = sc_running_vaccu / (j + 1)\n",
    "    sc_avg_vdice = sc_running_vdice / (j + 1)\n",
    "\n",
    "    ivd_avg_vaccu = ivd_running_vaccu / (j + 1)\n",
    "    ivd_avg_vdice = ivd_running_vdice / (j + 1)\n",
    "\n",
    "\n",
    "    print('Loss: Train {} Valid {}'.format(avg_tloss, avg_vloss))\n",
    "    print('Accuracy: Train {} Valid {}'.format(avg_taccu, avg_vaccu))\n",
    "    print('Dice: Train {} Valid {}'.format(avg_tdice, avg_vdice))\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    \n",
    "    tb_writer.add_scalars('Loss/train_vs_valid',\n",
    "                    {'Training' : avg_tloss, 'Validation' : avg_vloss},\n",
    "                    epoch_number + 1)\n",
    "    \n",
    "    tb_writer.add_scalars('General/accuracy_train_vs_valid',\n",
    "                    {'Training' : avg_taccu, 'Validation': avg_vaccu},\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    tb_writer.add_scalars('General/dice_train_vs_valid',\n",
    "                    {'Training' : avg_tdice, 'Validation': avg_vdice},\n",
    "                    epoch_number + 1)\n",
    "    #vert\n",
    "    tb_writer.add_scalars('Vertebrae/accuracy_train_vs_valid',\n",
    "                    {'Training' : vert_avg_taccu , 'Validation': vert_avg_vaccu},\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    tb_writer.add_scalars('Vertebrae/dice_train_vs_valid',\n",
    "                    {'Training' : vert_avg_tdice, 'Validation': vert_avg_vdice},\n",
    "                    epoch_number + 1)\n",
    "    #spinal canal\n",
    "    tb_writer.add_scalars('Spinal Canal/accuracy_train_vs_valid',\n",
    "                    {'Training': sc_avg_taccu, 'Validation': sc_avg_vaccu},\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    tb_writer.add_scalars('Spinal Canal/dice_train_vs_valid',\n",
    "                    {'Training': sc_avg_tdice ,'Validation': sc_avg_vdice},\n",
    "                    epoch_number + 1)\n",
    "    #ivd\n",
    "    tb_writer.add_scalars('Intervertebral Discs/accuracy_train_vs_valid',\n",
    "                    {'Training' : ivd_avg_taccu ,'Validation': ivd_avg_vaccu},\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    tb_writer.add_scalars('Intervertebral Discs/dice_valid',\n",
    "                    {'Training' : ivd_avg_tdice ,'Validation': ivd_avg_vdice},\n",
    "                    epoch_number + 1)\n",
    "    \n",
    "    tb_writer.flush()\n",
    "    \n",
    "    #Change path to save model accordingly     \n",
    "    model_path = '/home/kanthoulis/spider/models/spider_seg_{}'.format(epoch_number)\n",
    "\n",
    "    torch.save({\n",
    "        'model_dict': model.state_dict(),\n",
    "        'optimizer_dict': optim.state_dict(),\n",
    "        'scheduler_dict': scheduler.state_dict()  \n",
    "    }, model_path)\n",
    "    \n",
    "    #torch.save({'model_dict': model.state_dict(), 'optimizer_dict': optim.state_dict()}, model_path)\n",
    "        \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHUT DOWN KERNELS AFTER TRAINING TO FREE UP MEMORY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-venv",
   "language": "python",
   "name": "jupyter-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
