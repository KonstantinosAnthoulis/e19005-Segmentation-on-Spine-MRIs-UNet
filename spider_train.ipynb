{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies Install (needed for colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install SimpleITK\n",
    "#anything else needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNet \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import json \n",
    "\n",
    "#Ducknet (not used)\n",
    "#import tensorflow\n",
    "\n",
    "import SimpleITK as sitk\n",
    "#explicitly set reader just in case \n",
    "reader = sitk.ImageFileReader()\n",
    "reader.SetImageIO(\"MetaImageIO\")\n",
    "\n",
    "import numpy as np\n",
    "import re \n",
    "import os\n",
    "import pathlib\n",
    "from natsort import natsorted\n",
    "\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#model imports \n",
    "from models import unet\n",
    "#from models import +for ducknet \n",
    "\n",
    "from training import dataset_torch_numpy as dataset #change middle import according to data file type: tensor, numpy, cupy \n",
    "from training import metric\n",
    "from training import epoch as ep #ep not to conlfict with var name in loop\n",
    "\n",
    "#Set GPU/Cuda Device to run model on\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "torch.manual_seed(46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tensor data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"path/to/data.json\" #RERUN creating the dset \n",
    "\n",
    "#Load tensor parameters from .json\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Assign each value to a variable\n",
    "row_max = data[\"row_max\"]\n",
    "col_max = data[\"col_max\"]\n",
    "image_tensor_min = data[\"image_tensor_min\"]\n",
    "image_tensor_max = data[\"image_tensor_max\"]\n",
    "label_tensor_min = data[\"label_tensor_min\"]\n",
    "label_tensor_max = data[\"label_tensor_max\"]\n",
    "masks_no = data[\"masks_no\"]\n",
    "masks_array = data[\"masks_array\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_slice_dir = pathlib.Path(r\"\") #TODO SET PATH IN JHUB\n",
    "train_label_slice_dir = pathlib.Path(r\"\") #TODO SET PATH IN JHUB \n",
    "test_img_slice_dir = pathlib.Path(r\"\") #TODO SET PATH IN JHUB\n",
    "test_label_slice_dir= pathlib.Path(r\"\") #TODO SET PATH IN JHUB\n",
    "\n",
    "#Create lists of filenames in directories (str)\n",
    "image_path = train_img_slice_dir\n",
    "label_path = train_label_slice_dir\n",
    "\n",
    "image_dir_list = os.listdir(image_path)\n",
    "label_dir_list = os.listdir(label_path)\n",
    "\n",
    "#Sort lists\n",
    "image_dir_list = natsorted(image_dir_list)\n",
    "label_dir_list = natsorted(label_dir_list)\n",
    "\n",
    "print(\"train dataset len\",image_dir_list.__len__())\n",
    "print(\"test dataset len\",label_dir_list.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet Torch - Model Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input Output channels\n",
    "input_channels = 1 #Hounsfield scale, do not modify\n",
    "output_channels = masks_no - 1 #-1 to exclude backround value 0, do not modify\\\n",
    "\n",
    "#Model Hyperparams \n",
    "depth = 5\n",
    "start_filts = 64  \n",
    "up_mode = 'upsample'\n",
    "lr = 0.0001 #adjust according to batch size \n",
    "batchsize = 12 #find optimal setting for jhub gpu(s)\n",
    "loss_func = nn.BCEWithLogitsLoss() \n",
    "loss_func.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet Torch - Model & Optimizer Init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet.UNet(in_channels= input_channels,num_classes=output_channels, depth= depth, start_filts=start_filts, up_mode=up_mode) \n",
    "model.to(device)\n",
    "model.to(torch.float32)\n",
    "\n",
    "#optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1) #AdamW \n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet Torch - Load Model and Optim states for resuming training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: a simple script to get the file w the highest idx in the directory wouldn't be hard to iterate but for now\n",
    "    #just focusing on training the model, maybe work on it further down the line \n",
    "\n",
    "#last epoch trained path\n",
    "path = \"path/to/last/model\" #TODO SET\n",
    "\n",
    "checkpoint= torch.load(path)\n",
    "print(checkpoint.keys())\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def extract_number_from_path(path):\n",
    "    match = re.search(r'(\\d+)$', path)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "epoch_no = extract_number_from_path(path) + 1 #number for plotting in tb\n",
    "\n",
    "model.load_state_dict(checkpoint['model_dict'])\n",
    "optim.load_state_dict(checkpoint['optimizer_dict'])\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet Torch - Datasets & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets \n",
    "train_set = dataset.SpiderDataset(train_label_slice_dir, train_img_slice_dir)\n",
    "test_set = dataset.SpiderDataset(test_label_slice_dir, test_img_slice_dir)\n",
    "\n",
    "#Dataloaders\n",
    "train_dataloader = DataLoader(train_set, batch_size = batchsize, shuffle=True) \n",
    "test_dataloader = DataLoader(test_set, batch_size = batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_calculator = metric.SegmentationMetrics(average=True, ignore_background=True,activation='sigmoid') #for calculating \n",
    "metric_calculator_binary = metric.BinaryMetrics(activation='sigmoid') #for calculating spinal canal metrics since it's only 1 class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set no. of epochs to train for training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H')\n",
    "#writer = SummaryWriter('runs/spider_seg_unet_epochs={}_lr={}_batchsize={}_loss=BCEWithLogits_startfilts={}_upmode={}'.format(epochs,lr, batchsize,start_filts,up_mode))\n",
    "writer = SummaryWriter('runs/spider_seg_{}'.format(timestamp))\n",
    "epoch_number = epoch_no #set to no of last epoch completed to have consistent tb plots \n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = ep.train_one_epoch(epoch_number, writer, optim = optim, loss_func = loss_func, train_dataloader=train_dataloader, model = model,\n",
    "                               metric_calculator=metric_calculator, metric_calculator_binary=metric_calculator_binary, device = device)\n",
    "    #print(\"avg loss in epoch\", avg_loss)\n",
    "    '''\n",
    "    running_accu = 0.0\n",
    "    running_dice = 0.0\n",
    "\n",
    "    vert_running_accu = 0.0\n",
    "    vert_running_dice = 0.0\n",
    "\n",
    "    sc_running_accu = 0.0\n",
    "    sc_running_dice = 0.0\n",
    "\n",
    "    ivd_running_accu = 0.0\n",
    "    ivd_running_dice = 0.0\n",
    "    '''\n",
    "    running_vloss = 0.0\n",
    "    running_vaccu = 0.0\n",
    "    running_vdice = 0.0\n",
    "\n",
    "    vert_running_vaccu = 0.0\n",
    "    vert_running_vdice = 0.0\n",
    "    \n",
    "    sc_running_vaccu = 0.0\n",
    "    sc_running_vdice = 0.0\n",
    "\n",
    "    ivd_running_vaccu = 0.0\n",
    "    ivd_running_vdice = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        '''\n",
    "        #training metrics on fully trained model up to that epoch\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            \n",
    "            print(\"valuation training set batch\", i)\n",
    "\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            accu, dice, prec, recall = metric_calculator(labels, outputs)\n",
    "\n",
    "            #Vertebrae metrics (0-9)\n",
    "            vert_labels = labels[:, :9, :, :]\n",
    "            vert_outputs = outputs[: ,:9, :, :]\n",
    "            vert_accu, vert_dice, vert_prec, vert_recall = metric_calculator(vert_labels, vert_outputs)\n",
    "\n",
    "            #Spinal Canal Metrics (10)\n",
    "            sc_labels = labels[: , 10, :, :].unsqueeze(1)\n",
    "            sc_outputs = outputs[:, 10, :, :].unsqueeze(1)\n",
    "            sc_accu, sc_dice, sc_prec, sc_specif, sc_recall = metric_calculator_binary(sc_labels, sc_outputs)\n",
    "\n",
    "            #IVD metrics (11-19)\n",
    "            ivd_labels = labels[:, -9:, : ,:]\n",
    "            ivd_outputs = outputs[:, -9:, :, :]\n",
    "            ivd_accu, ivd_dice, ivd_prec, ivd_recall = metric_calculator(ivd_labels, ivd_outputs)\n",
    "\n",
    "            running_accu += accu\n",
    "            running_dice += dice\n",
    "\n",
    "            vert_running_accu += vert_accu\n",
    "            vert_running_dice += vert_dice\n",
    "\n",
    "            sc_running_accu += sc_accu\n",
    "            sc_running_dice += vert_dice\n",
    "\n",
    "            ivd_running_accu += ivd_accu\n",
    "            ivd_running_dice += ivd_dice\n",
    "        '''\n",
    "\n",
    "        #eval metrics\n",
    "        for j, vdata in enumerate(test_dataloader):\n",
    "\n",
    "            print(\"valuation test set batch\", j)\n",
    "            vinputs, vlabels = vdata\n",
    "\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_func(voutputs, vlabels)\n",
    "            \n",
    "            vaccu, vdice, vprec, vrecall = metric_calculator(vlabels, voutputs)\n",
    "\n",
    "            #Vertebrae Metrics (0-9)\n",
    "            vert_vlabels = vlabels[:, :9, :, :]\n",
    "            vert_voutputs = voutputs[:, :9, :, :]\n",
    "            vert_vaccu, vert_vdice, vert_vprec, vert_vrecall = metric_calculator(vert_vlabels, vert_voutputs)\n",
    "\n",
    "            #Spinal Canal Metrics (10)\n",
    "            sc_vlabels = vlabels[:, 10, :, :].unsqueeze(1)\n",
    "            sc_voutputs = voutputs[:, 10, :, :].unsqueeze(1)\n",
    "            sc_vaccu, sc_vdice, sc_vprec, sv_vspecif, sc_vrecall = metric_calculator_binary(sc_vlabels, sc_voutputs)\n",
    "\n",
    "            #IVD Metrics (11-19)\n",
    "            ivd_vlabels = vlabels[:, -9:, :, :]\n",
    "            ivd_voutputs = voutputs[:, -9:, :, :]\n",
    "            ivd_vaccu, ivd_vdice, ivd_vprec, ivd_vrecall = metric_calculator(ivd_vlabels, ivd_voutputs)\n",
    "\n",
    "\n",
    "            running_vloss += vloss\n",
    "            running_vaccu += vaccu\n",
    "            running_vdice += vdice\n",
    "\n",
    "            vert_running_vaccu += vert_vaccu\n",
    "            vert_running_vdice += vert_vdice\n",
    "\n",
    "            sc_running_vaccu += sc_vaccu\n",
    "            sc_running_vdice += vert_vdice\n",
    "\n",
    "            ivd_running_vaccu += ivd_vaccu\n",
    "            ivd_running_vdice += ivd_vdice\n",
    "\n",
    "    '''\n",
    "    avg_accu = running_accu / (i + 1)\n",
    "    avg_dice = running_dice / (i + 1)\n",
    "\n",
    "    vert_avg_accu = vert_running_accu / (i + 1)\n",
    "    vert_avg_dice = vert_running_dice / (i + 1)\n",
    "\n",
    "    sc_avg_accu = sc_running_accu / (i + 1)\n",
    "    sc_avg_dice = sc_running_dice / (i + 1)\n",
    "\n",
    "    ivd_avg_accu = ivd_running_accu / (i + 1)\n",
    "    ivd_avg_dice = ivd_running_dice / (i + 1)\n",
    "    '''\n",
    "\n",
    "    avg_vloss = running_vloss / (j + 1)\n",
    "    avg_vaccu = running_vaccu / (j + 1)\n",
    "    avg_vdice = running_vdice / (j + 1)\n",
    "\n",
    "    vert_avg_vaccu = vert_running_vaccu / (j + 1)\n",
    "    vert_avg_vdice = vert_running_vdice / (j + 1)\n",
    "\n",
    "    sc_avg_vaccu = sc_running_vaccu / (j + 1)\n",
    "    sc_avg_vdice = sc_running_vdice / (j + 1)\n",
    "\n",
    "    ivd_avg_vaccu = ivd_running_vaccu / (j + 1)\n",
    "    ivd_avg_vdice = ivd_running_vdice / (j + 1)\n",
    "\n",
    "\n",
    "    #print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    \n",
    "    writer.add_scalars('Loss/validation',\n",
    "                    { 'Validation' : avg_vloss},\n",
    "                    epoch_number + 1)\n",
    "    \n",
    "    writer.add_scalars('General/accuracy_valid',\n",
    "                    {'Validation': avg_vaccu},\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    writer.add_scalars('General/dice_valid',\n",
    "                    {'Validation': avg_vdice},\n",
    "                    epoch_number + 1)\n",
    "    #vert\n",
    "    writer.add_scalars('Vertebrae/accuracy_valid',\n",
    "                    {'Validation': vert_avg_vaccu},\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    writer.add_scalars('Vertebrae/dice_valid',\n",
    "                    {'Validation': vert_avg_vdice},\n",
    "                    epoch_number + 1)\n",
    "    #spinal canal\n",
    "    writer.add_scalars('Spinal Canal/accuracy_valid',\n",
    "                    {'Validation': sc_avg_vaccu},\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    writer.add_scalars('Spinal Canal/dice_valid',\n",
    "                    {'Validation': sc_avg_vdice},\n",
    "                    epoch_number + 1)\n",
    "    #ivd\n",
    "    writer.add_scalars('Intervertebral Discs/accuracy_valid',\n",
    "                    {'Validation': ivd_avg_vaccu},\n",
    "                    epoch_number + 1)\n",
    "\n",
    "    writer.add_scalars('Intervertebral Discs/dice_valid',\n",
    "                    {'Validation': ivd_avg_vdice},\n",
    "                    epoch_number + 1)\n",
    "    \n",
    "    writer.flush()\n",
    "    \n",
    "    #Change path to save model accordingly     \n",
    "    model_path = 'models/spider_{}'.format(epoch_number)\n",
    "    \n",
    "    torch.save({'model_dict': model.state_dict(), 'optimizer_dict': optim.state_dict()}, model_path)\n",
    "        \n",
    "    epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider-torch-tb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
