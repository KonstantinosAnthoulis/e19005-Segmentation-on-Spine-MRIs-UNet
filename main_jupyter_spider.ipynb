{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFL78HZcbmSP"
      },
      "source": [
        "\n",
        "Connect to Google Drive for datasets (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcBj2OvobfY3",
        "outputId": "28f8c7e7-eb9b-490c-bd3c-674444fbe332"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1Wa9Ngfw6QC"
      },
      "source": [
        "Install dependencies (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOCzMWFLw53J",
        "outputId": "1a06cc2b-3e31-4c79-8917-9c8f90e50b52"
      },
      "outputs": [],
      "source": [
        "#!pip install simpleitk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVzzg3MJnhFU"
      },
      "source": [
        "Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QMgkyFD0Hm3",
        "outputId": "50d2df41-f013-4d5e-8c9a-b2d356fb8a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.widgets import Slider, Button, RadioButtons\n",
        "\n",
        "import SimpleITK as sitk\n",
        "reader = sitk.ImageFileReader()\n",
        "reader.SetImageIO(\"MetaImageIO\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import pathlib\n",
        "\n",
        "from natsort import natsorted\n",
        "\n",
        "#Set GPU/Cuda Device to run model on\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "np.random.seed(46)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H48GvrcPnhFh"
      },
      "source": [
        "Dataset Directories <Br>\n",
        "Comment out directory not in use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "35vekrfdnhFi"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Toy Dataset Slices - Removed Slices\n",
        "'''\n",
        "train_img_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_train_img_slices\")\n",
        "train_label_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_train_label_slices\")\n",
        "\n",
        "test_img_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_test_img_slices\")\n",
        "test_label_slice_dir= pathlib.Path(r\"spider_toy_dset_slices/dummy_test_label_slices\")\n",
        "'''\n",
        "\n",
        "'''\n",
        "#Local Dataset Slices - Removed Slices\n",
        "train_img_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/train_image_slices\")\n",
        "train_label_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/train_label_slices\")\n",
        "\n",
        "test_img_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/test_image_slices\")\n",
        "test_label_slice_dir= pathlib.Path(\"D:/Spider Slice Directories/test_label_slices\")\n",
        "'''\n",
        "#Local Dataset Slices - Voxel Resampling [2, 0.6, 0.6] Bspline Interpolation \n",
        "train_img_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/train_image_bspline_slices\")\n",
        "train_label_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/train_label_bspline_slices\")\n",
        "\n",
        "test_img_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/test_image_bspline_slices\")\n",
        "test_label_slice_dir= pathlib.Path(\"D:/Spider Slice Directories/test_label_bspline_slices\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMoYlYbYnhFl"
      },
      "source": [
        "Image Slice Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mR75OjC82JOO"
      },
      "outputs": [],
      "source": [
        "from transforms import mri_transforms\n",
        "\n",
        "\n",
        "#TODO add bool image label for interp \n",
        "class Mri_Slice:\n",
        "    def __init__(self, path):\n",
        "        mri_mha = sitk.ReadImage(path, imageIO = \"MetaImageIO\") #explicitly setting ioreader just in case\n",
        "\n",
        "        #grabbing the image as well for downsampling testing\n",
        "        self.mri_mha = mri_mha\n",
        "       \n",
        "        #get 2d array from mri slice\n",
        "        mri_a = np.array(sitk.GetArrayFromImage(mri_mha)) #mri_array\n",
        "        \n",
        "        mri_a_float32 = mri_a.astype(dtype = np.float32)\n",
        "        #TODO: set bounds to [-1000, 2000] https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        self.hu_a = mri_a_float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zero crop sanity check\n",
        "Testing Linspace and value mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "[  0.   1.   2.   3.   4.   5.   6. 100. 201. 202. 203. 204. 205. 206.]\n",
            "test arr after mapping [ 0  1  2  3  4  5  6 10 11 12 13 14 15 16]\n"
          ]
        }
      ],
      "source": [
        "from transforms import array_transforms\n",
        "\n",
        "test_image_slice = Mri_Slice(r\"spider_toy_dset_slices/dummy_train_img_slices/1_t1_30.mha\")\n",
        "test_label_slice = Mri_Slice(r\"spider_toy_dset_slices/dummy_train_label_slices/1_t1_30.mha\")\n",
        "\n",
        "#print(test_label_slice.mri_mha)\n",
        "\n",
        "test_image_a = test_image_slice.hu_a\n",
        "test_label_a = test_label_slice.hu_a\n",
        "\n",
        "test_image_a, test_label_a = array_transforms.crop_zero(test_image_a, test_label_a)\n",
        "\n",
        "test_image_crop_mha = sitk.GetImageFromArray(test_image_a)\n",
        "\n",
        "test_label_tensor = torch.from_numpy(test_label_a)\n",
        "        \n",
        "test_label_tensor = test_label_tensor.to(torch.int)\n",
        "\n",
        "val_range = np.arange(20)\n",
        "print(val_range)\n",
        "key_range = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 100, 201, 202, 203, 204, 205, 206, 207, 208, 209])\n",
        "\n",
        "#pad to max resolution of slice in dset \n",
        "#label_tensor = tensor_transforms.pad_to_resolution(label_tensor, [row_dim_max, col_dim_max]) #torch.functional.pad takes care of cropping, no need to call array_transforms.crop_zero()\n",
        "\n",
        "print(np.unique(test_label_a))\n",
        "\n",
        "mapping_dict = dict(zip(key_range, val_range))\n",
        "\n",
        "# Define a function to perform the mapping using the dictionary\n",
        "def map_to_specified_set(value):\n",
        "    return mapping_dict.get(value, np.nan)  # Return np.nan for values not found in the mapping_dict\n",
        "\n",
        "# Create a vectorized version of the mapping function\n",
        "mapped_function = np.vectorize(map_to_specified_set)\n",
        "\n",
        "# Apply the mapping function to the array\n",
        "mapped_array = mapped_function(test_label_a)\n",
        "\n",
        "print(\"test arr after mapping\", np.unique(mapped_array))\n",
        "#sitk.WriteImage(test_image_crop_mha, \"D:/sitk dump/1_t1_30_cropzero.mha\")\n",
        "\n",
        "#TODO continue here make images 1:1 scale and downsample to 64x64\n",
        "#if(sitk_slice.GetSize()[0] != sitk_slice.GetSize()[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX_8rD_AyhSQ"
      },
      "source": [
        "Sort directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMiefYLdyRbn",
        "outputId": "9df4b03e-3cff-4f1a-ec74-55a8cc1ee98b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:\\Spider Slice Directories\\train_image_bspline_slices\n",
            "D:\\Spider Slice Directories\\train_label_bspline_slices\n",
            "train images count 15662\n"
          ]
        }
      ],
      "source": [
        "#get lists from directories\n",
        "\n",
        "#toy dset slices\n",
        "image_path = train_img_slice_dir\n",
        "label_path = train_label_slice_dir\n",
        "\n",
        "image_dir_list = os.listdir(image_path)\n",
        "label_dir_list = os.listdir(label_path)\n",
        "\n",
        "print(image_path) \n",
        "print(label_path)\n",
        "\n",
        "#local dset\n",
        "'''\n",
        "image_dir_list = os.listdir(local_img_idr)\n",
        "label_dir_list = os.listdir(local_label_dir)\n",
        "'''\n",
        "#sort lists\n",
        "image_dir_list = natsorted(image_dir_list)\n",
        "label_dir_list = natsorted(label_dir_list)\n",
        "#empty lists to hold x and y dimensions of images\n",
        "row_list = []\n",
        "col_list = []\n",
        "\n",
        "#dirlen = len(os.listdir(dummy_train_label_dir))\n",
        "dirlen = len(os.listdir(label_path))\n",
        "\n",
        "print(\"train images count\", dirlen)\n",
        "\n",
        "#print(local_label_idr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get max dimension of slice in dset for x y padding <br>\n",
        "Images have slices with 0 label info removed and are cropped using zero crop <br>\n",
        "Also get min max values of dset for tensor normalization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "row max: 464\n",
            "col max: 224\n",
            "image tensor min -2230.0\n",
            "image tensor max 4385.0\n",
            "label tensor min 0.0\n",
            "label tensor max 209.0\n",
            "amount of masks 20\n"
          ]
        }
      ],
      "source": [
        "from transforms import array_transforms\n",
        "\n",
        "\n",
        "\n",
        "row_list = []\n",
        "col_list = []\n",
        "\n",
        "for idx in range(0, dirlen):\n",
        "  #print(\"dirlen\", dirlen)\n",
        "  \n",
        " #toy dset \n",
        "  \n",
        "  img_path = image_path.joinpath(image_dir_list[idx])\n",
        "  lbl_path = label_path.joinpath(label_dir_list[idx])#first part before joinpath is pathlib.Path, second part is the directory of hte file \n",
        "  '''\n",
        "  img_path = local_img_idr.joinpath(image_dir_list[idx])\n",
        "  label_path = local_label_dir.joinpath(label_dir_list[idx]) #first part before joinpath is pathlib.Path, second part is the directory of hte file \n",
        "  '''\n",
        "  image = Mri_Slice(img_path)\n",
        "  label = Mri_Slice(lbl_path)\n",
        "\n",
        "  image_a = image.hu_a\n",
        "  label_a = label.hu_a\n",
        "\n",
        "  #crop zero\n",
        "  image_a_cropzero, label_a_cropzero = array_transforms.crop_zero(image_a, label_a)\n",
        "\n",
        "  #find array min max values for normalising in Dataset class\n",
        "  if(idx ==0):\n",
        "    \n",
        "    image_tensor_min = np.min(image_a_cropzero)\n",
        "    image_tensor_max = np.max(label_a_cropzero)\n",
        "    label_tensor_min = np.min(label_a_cropzero)\n",
        "    label_tensor_max = np.max(label_a_cropzero)\n",
        "    \n",
        "    unique_masks_a = np.unique(label_a)\n",
        "  else:\n",
        "    \n",
        "    if(np.min(image_a_cropzero) < image_tensor_min):\n",
        "      image_tensor_min = np.min(image_a_cropzero)\n",
        "      image_tensor_min_dir = img_path\n",
        "    if(np.min(label_a_cropzero) < label_tensor_min):\n",
        "      label_tensor_min = np.min(label_a_cropzero)\n",
        "    if(np.max(image_a_cropzero) > image_tensor_max):\n",
        "      image_tensor_max = np.max(image_a_cropzero)\n",
        "    if(np.max(label_a_cropzero) > label_tensor_max):\n",
        "      label_tensor_max = np.max(label_a_cropzero)\n",
        "    \n",
        "  #find amount of unique masks for one-hot encoding \n",
        "    current_masks_a = np.unique(label_a)\n",
        "    if(len(current_masks_a) > len(unique_masks_a)):\n",
        "      unique_masks_a = current_masks_a\n",
        "  #print(\"image res\", image_a_cropzero.shape)\n",
        "  \n",
        "  \n",
        "  row_list.append(image_a_cropzero.shape[0]) #add row value to list\n",
        "  #print(image_a_cropzero.shape[0])\n",
        "  col_list.append(image_a_cropzero.shape[1]) #add col value to list \n",
        "  \n",
        "  \n",
        "\n",
        "#calculate max \n",
        "\n",
        "row_dim_max = max(row_list)\n",
        "col_dim_max = max(col_list)\n",
        "\n",
        "row_dim_max = ((row_dim_max + 15) // 16) * 16 #nearest multiple of 16 \n",
        "col_dim_max = ((col_dim_max + 15) // 16) * 16 #nearest multiple of 16\n",
        "\n",
        "print(\"row max:\", max(row_list))\n",
        "print(\"col max:\", max(col_list))\n",
        "\n",
        "print(\"image tensor min\", image_tensor_min)\n",
        "print(\"image tensor max\", image_tensor_max)\n",
        "print(\"label tensor min\", label_tensor_min)\n",
        "print(\"label tensor max\", label_tensor_max)\n",
        "\n",
        "print(\"amount of masks\", len(unique_masks_a))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9. 100. 201. 202. 203.\n",
            " 204. 205. 206. 207. 208. 209.]\n"
          ]
        }
      ],
      "source": [
        "print(unique_masks_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensor Dims cropping so that i don't waste 10 mins of my life every time i train on local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nrow_dim_max = 464\\ncol_dim_max = 224\\n\\nimage_tensor_min = -2230.0\\nimage_tensor_max = 4385.0\\nlabel_tensor_min = 0.0\\nlabel_tensor_max = 209.0\\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Toy Dset Bspline Resample \n",
        "row_dim_max = 448\n",
        "col_dim_max = 224\n",
        "\n",
        "image_tensor_min = -2057.0\n",
        "image_tensor_max = 4292.0\n",
        "label_tensor_min = 0.0\n",
        "label_tensor_max = 208.0 #IVD 209 IS NOT EXISTENT NO CLUE WHY \n",
        "\n",
        "'''\n",
        "#Local Dset not resampled\n",
        "row_dim_max = 928\n",
        "col_dim_max = 400\n",
        "\n",
        "image_tensor_min = -1000\n",
        "image_tensor_max = 3096\n",
        "label_tensor_min = 0.0\n",
        "label_tensor_max = 209.0\n",
        "'''\n",
        "\n",
        "#Local dset bspline resample\n",
        "'''\n",
        "row_dim_max = 464\n",
        "col_dim_max = 224\n",
        "\n",
        "image_tensor_min = -2230.0\n",
        "image_tensor_max = 4385.0\n",
        "label_tensor_min = 0.0\n",
        "label_tensor_max = 209.0\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVwddxJgnhFn"
      },
      "source": [
        "Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9X6rHdnnhFo",
        "outputId": "5e9fab66-6c87-4081-eaee-cd059d8a4445"
      },
      "outputs": [],
      "source": [
        "from transforms import tensor_transforms\n",
        "\n",
        "class SpiderDataset(Dataset):\n",
        "    def __init__(self, labels_dir, img_dir, transform=None, target_transform=None):\n",
        "        self.labels_dir = labels_dir\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.labels_dir))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label_dir_list = os.listdir(self.labels_dir)\n",
        "        image_dir_list = os.listdir(self.img_dir)\n",
        "\n",
        "        image_dir_list = natsorted(image_dir_list)\n",
        "        label_dir_list = natsorted(label_dir_list)\n",
        "\n",
        "        img_path = self.img_dir.joinpath(image_dir_list[idx])\n",
        "        label_path = self.labels_dir.joinpath(label_dir_list[idx])\n",
        "\n",
        "        image = Mri_Slice(img_path)\n",
        "        label = Mri_Slice(label_path)\n",
        "\n",
        "        image_a = image.hu_a\n",
        "        label_a = label.hu_a\n",
        "\n",
        "        image_tensor = torch.from_numpy(image_a)\n",
        "        label_tensor = torch.from_numpy(label_a)\n",
        "        \n",
        "        image_tensor = image_tensor.to(torch.float32)\n",
        "        label_tensor = label_tensor.to(torch.float32)\n",
        "\n",
        "        #pad to max resolution of slice in dset \n",
        "        image_tensor = tensor_transforms.pad_to_resolution(image_tensor, [row_dim_max, col_dim_max])\n",
        "        label_tensor = tensor_transforms.pad_to_resolution(label_tensor, [row_dim_max, col_dim_max]) #torch.functional.pad takes care of cropping, no need to call array_transforms.crop_zero()\n",
        "\n",
        "        #normalise\n",
        "        image_tensor = (image_tensor - image_tensor_min) / (image_tensor_max - image_tensor_min)\n",
        "        #label_tensor = (label_tensor - label_tensor_min) / (label_tensor_max - label_tensor_min) #INTS HERE DO NOT NORMALISE \n",
        "\n",
        "        #TODO clamp values 0 to 19 \n",
        "\n",
        "        #min = torch.linspace(min = 0, max = 19, steps =1)\n",
        "        #torch.clamp(label_tensor, min)\n",
        "\n",
        "        print(\"tensor dims\", image_tensor.shape)\n",
        "        #print(\"label tensor min clamp\",torch.min(label_tensor))\n",
        "        #print(\"label tensor max clamp\", torch.max(label_tensor))\n",
        "\n",
        "        #one hot label, this works only if the range on the label tensors is from 0 to 19 steps of 1 \n",
        "        #label_tensor = nn.functional.one_hot(label_tensor.long(), num_classes= 19)\n",
        "\n",
        "        image_tensor = image_tensor.unsqueeze(0)\n",
        "        label_tensor = label_tensor.unsqueeze(0)\n",
        "        \n",
        "        image_tensor = image_tensor.to(device)\n",
        "        label_tensor = label_tensor.to(device)\n",
        "\n",
        "        #print(image_tensor.shape)\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train dataset len 2100\n",
            "test dataset len 215\n"
          ]
        }
      ],
      "source": [
        "#toy train test dataset to test network running\n",
        "#local_train_set = SpiderDataset(local_img_idr, local_label_idr)\n",
        "dummy_train_set = SpiderDataset(train_label_slice_dir, train_img_slice_dir)\n",
        "\n",
        "dummy_test_set = SpiderDataset(test_label_slice_dir, test_img_slice_dir)\n",
        "\n",
        "print(\"train dataset len\",dummy_train_set.__len__())\n",
        "print(\"test dataset len\",dummy_test_set.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zsRlBNG-B6Q"
      },
      "source": [
        "Create Unet Instance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CT7aKn7S-F6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Github Repos\\Spider Seg e19005\\spider-seg-e19005\\models\\unet.py:213: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "  init.xavier_normal(m.weight)\n",
            "d:\\Github Repos\\Spider Seg e19005\\spider-seg-e19005\\models\\unet.py:214: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  init.constant(m.bias, 0)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (conv_final): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (down_convs): ModuleList(\n",
              "    (0): DownConv(\n",
              "      (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (1): DownConv(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (2): DownConv(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (3): DownConv(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (4): DownConv(\n",
              "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (up_convs): ModuleList(\n",
              "    (0): UpConv(\n",
              "      (upconv): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (1): UpConv(\n",
              "      (upconv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (2): UpConv(\n",
              "      (upconv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (3): UpConv(\n",
              "      (upconv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from models import unet \n",
        "\n",
        "input_channels = 1 #Hounsfield scale\n",
        "#output_channels = 3 #Vertebra, disc and spinal canal masks #\n",
        "#output_channels = 19 #one for every class\n",
        "output_channels = 1 #will try this so that torch doesn't complain about broadcasting \n",
        "model = unet.UNet(in_channels= input_channels,num_classes=output_channels)\n",
        "model.to(device)\n",
        "model.to(torch.float32)\n",
        "#for param in model.parameters():\n",
        " #   print(param.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Faster-RCNN Instance trained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\\nfrom torchvision.utils import draw_bounding_boxes\\n\\nweights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\\nmodel = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Mask RCNN Instance Trained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights\\n\\nmodel = maskrcnn_resnet50_fpn_v2(\\n    weights = MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,\\n    num_classes = 19\\n    #weights_backbone = \\n)\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights\n",
        "\n",
        "model = maskrcnn_resnet50_fpn_v2(\n",
        "    weights = MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,\n",
        "    num_classes = 19\n",
        "    #weights_backbone = \n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcuWKZAD9pHD"
      },
      "source": [
        "Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4lnCiKNW9sJN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n",
            "0.001\n",
            "8\n",
            "BCEWithLogitsLoss()\n"
          ]
        }
      ],
      "source": [
        "epochs = 30 #testing\n",
        "lr = 0.001 #testing\n",
        "batchsize = 8 #testing\n",
        "loss_func = nn.BCEWithLogitsLoss() #testing\n",
        "loss_func.to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "print(epochs)\n",
        "print(lr)\n",
        "print(batchsize)\n",
        "print(loss_func)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWO0s80znhFr"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UQL_nxpCnhFs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor batch in dummy_train_dataloader:\\n    for tensor in batch:\\n        print(\"min\", torch.min(tensor))\\n        print(\"max\", torch.max(tensor))\\n'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dummy_train_dataloader = DataLoader(dummy_train_set, batch_size = batchsize, shuffle=True)\n",
        "\n",
        "dummy_test_dataloader = DataLoader(dummy_test_set, batch_size = batchsize, shuffle=True)\n",
        "\n",
        "\n",
        "'''\n",
        "for batch in dummy_train_dataloader:\n",
        "    for tensor in batch:\n",
        "        print(\"min\", torch.min(tensor))\n",
        "        print(\"max\", torch.max(tensor))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensor Dimensions Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor idx_slice in dummy_train_dataloader:\\n    for tensor in idx_slice:\\n        print(tensor.shape)\\n        break\\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "for idx_slice in dummy_train_dataloader:\n",
        "    for tensor in idx_slice:\n",
        "        print(tensor.shape)\n",
        "        break\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxAIK14tWJbp"
      },
      "source": [
        "One Epoch <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jxCZ41T_WOBF"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "\n",
        "    #swap train dataloader for dset\n",
        "    for i, data in enumerate(dummy_train_dataloader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        #print(\"labels\", labels.shape)\n",
        "        #print(inputs.shape)\n",
        "        #print(device)\n",
        "\n",
        "        #inputs = inputs.transpose(-1,0)\n",
        "        #labels = labels.transpose(-1,0)\n",
        "        #inputs = inputs.reshape(inputs.shape(1), inputs.shape(0), inputs.shape(3), inputs.shape(4))\n",
        "        #labels = labels.reshape(labels.shape(1), labels.shape(0), labels.shape(3), labels.shape(4))\n",
        "\n",
        "      \n",
        "        # Zero your gradients for every batch!\n",
        "        optim.zero_grad()\n",
        "\n",
        "        #2d\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        print(\"inputs\", inputs.shape)\n",
        "        print(\"outputs\", outputs.shape)\n",
        "        print(\"labels\", labels.shape)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optim.step()\n",
        "\n",
        "        #3d tensor, probably won't use keeping here just in case  \n",
        "        '''\n",
        "        #run everything while indexing through z axis\n",
        "        for axis in images, masks:\n",
        "          for idx in range(0, axis.size(1)):\n",
        "            # Zero your gradients for every batch!\n",
        "            optim.zero_grad()\n",
        "            print(inputs[:, idx, : ,:])\n",
        "            # Make predictions for this batch\n",
        "            outputs = model(inputs[:, idx, : ,:])\n",
        "\n",
        "            # Compute the loss and its gradients\n",
        "            loss = loss_func(outputs[:, idx, : ,:], labels[:, idx, : ,:])\n",
        "            loss.backward()\n",
        "\n",
        "            # Adjust learning weights\n",
        "            optim.step()\n",
        "        '''\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        #if i % 1000 == 999:\n",
        "            #print(\"goes in\")\n",
        "        last_loss = running_loss / 1000 # loss per batch\n",
        "        print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "        tb_x = epoch_index * len(dummy_train_dataloader) + i + 1\n",
        "        tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "        running_loss = 0.\n",
        "        #if ends here\n",
        "\n",
        "    print(\"loss\", loss)\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyaaV1lYYiFc"
      },
      "source": [
        "Train Loop <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7i7TB_oOYhxQ",
        "outputId": "bfdaa28e-8cbc-4f46-be69-0979a6d3ceff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n",
            "label tensor dim before crop torch.Size([469, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([475, 472])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([467, 466])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([500, 150])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([470, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([500, 150])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([476, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([515, 517])\n",
            "tensor dims torch.Size([448, 224])\n",
            "inputs torch.Size([8, 1, 448, 224])\n",
            "outputs torch.Size([8, 1, 448, 224])\n",
            "labels torch.Size([8, 1, 448, 224])\n",
            "  batch 1 loss: 0.0008068006038665772\n",
            "label tensor dim before crop torch.Size([469, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([500, 150])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([482, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([471, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([470, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([502, 500])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([471, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([467, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "inputs torch.Size([8, 1, 448, 224])\n",
            "outputs torch.Size([8, 1, 448, 224])\n",
            "labels torch.Size([8, 1, 448, 224])\n",
            "  batch 2 loss: 7.0915900869295e-07\n",
            "label tensor dim before crop torch.Size([471, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([470, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([476, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([474, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([471, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([500, 150])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([500, 500])\n",
            "tensor dims torch.Size([448, 224])\n",
            "label tensor dim before crop torch.Size([474, 467])\n",
            "tensor dims torch.Size([448, 224])\n",
            "inputs torch.Size([8, 1, 448, 224])\n",
            "outputs torch.Size([8, 1, 448, 224])\n",
            "labels torch.Size([8, 1, 448, 224])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg loss in epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_loss)\n\u001b[0;32m     19\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
            "Cell \u001b[1;32mIn[19], line 63\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m#run everything while indexing through z axis\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03mfor axis in images, masks:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    optim.step()\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Gather data and report\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m#if i % 1000 == 999:\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m#print(\"goes in\")\u001b[39;00m\n\u001b[0;32m     66\u001b[0m last_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;66;03m# loss per batch\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/spider_seg_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "    print(\"avg loss in epoch\", avg_loss)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(dummy_test_dataloader):\n",
        "            vinputs, vlabels = vdata\n",
        "\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = loss_func(voutputs, vlabels)\n",
        "            running_vloss += vloss\n",
        "            \n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    \n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        #commented out saving the model for now to debug loss being 0 \n",
        "        '''\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        '''\n",
        "    epoch_number += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
