{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFL78HZcbmSP"
      },
      "source": [
        "\n",
        "Connect to Google Drive for datasets (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcBj2OvobfY3",
        "outputId": "28f8c7e7-eb9b-490c-bd3c-674444fbe332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1Wa9Ngfw6QC"
      },
      "source": [
        "Install dependencies (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOCzMWFLw53J",
        "outputId": "1a06cc2b-3e31-4c79-8917-9c8f90e50b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting simpleitk\n",
            "  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simpleitk\n",
            "Successfully installed simpleitk-2.3.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install simpleitk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVzzg3MJnhFU"
      },
      "source": [
        "Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QMgkyFD0Hm3",
        "outputId": "50d2df41-f013-4d5e-8c9a-b2d356fb8a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.widgets import Slider, Button, RadioButtons\n",
        "\n",
        "import scipy\n",
        "\n",
        "import SimpleITK as sitk\n",
        "reader = sitk.ImageFileReader()\n",
        "reader.SetImageIO(\"MetaImageIO\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import pathlib\n",
        "\n",
        "from natsort import natsorted\n",
        "\n",
        "#Set GPU/Cuda Device to run model on\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "np.random.seed(46)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXIx79uOnhFe"
      },
      "source": [
        "Resampling MRI image to specified voxel spacing <br>\n",
        "https://gist.github.com/mrajchl/ccbd5ed12eb68e0c1afc5da116af614a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8TUITSPkjTXd"
      },
      "outputs": [],
      "source": [
        "def resample_img(itk_image, out_spacing=[2.0, 0.6, 0.6], is_label=False):\n",
        "\n",
        "    # Resample images to 2 0.6 0.6 spacing with SimpleITK\n",
        "    original_spacing = itk_image.GetSpacing()\n",
        "    original_size = itk_image.GetSize()\n",
        "\n",
        "    out_size = [\n",
        "        int(np.round(original_size[0] * (original_spacing[0] / out_spacing[0]))),\n",
        "        int(np.round(original_size[1] * (original_spacing[1] / out_spacing[1]))),\n",
        "        int(np.round(original_size[2] * (original_spacing[2] / out_spacing[2])))]\n",
        "\n",
        "    resample = sitk.ResampleImageFilter()\n",
        "    resample.SetOutputSpacing(out_spacing)\n",
        "    resample.SetSize(out_size)\n",
        "    resample.SetOutputDirection(itk_image.GetDirection())\n",
        "    resample.SetOutputOrigin(itk_image.GetOrigin())\n",
        "    resample.SetTransform(sitk.Transform())\n",
        "    resample.SetDefaultPixelValue(itk_image.GetPixelIDValue())\n",
        "\n",
        "    if is_label:\n",
        "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
        "    else:\n",
        "        resample.SetInterpolator(sitk.sitkLinear)\n",
        "\n",
        "    return resample.Execute(itk_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H48GvrcPnhFh"
      },
      "source": [
        "Dataset Directories <Br>\n",
        "Comment out directory not in use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "35vekrfdnhFi"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n#Colab Google Drive Directories Toy Dataset\\ndummy_train_img_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_train_images\")\\ndummy_train_label_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_train_labels\")\\ndummy_test_img_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_test_images\")\\ndummy_test_label_dir= pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_test_labels\")\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Local Directories Toy Dataset\n",
        "dummy_train_img_dir = pathlib.Path(r\"D:\\Spider Data/dummy_train_images\")\n",
        "dummy_train_label_dir = pathlib.Path(r\"D:\\Spider Data/dummy_train_labels\")\n",
        "dummy_test_img_dir = pathlib.Path(r\"D:\\Spider Data/dummy_test_images\")\n",
        "dummy_test_label_dir= pathlib.Path(r\"D:\\Spider Data/dummy_test_labels\")\n",
        "\n",
        "'''\n",
        "#Colab Google Drive Directories Toy Dataset\n",
        "dummy_train_img_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_train_images\")\n",
        "dummy_train_label_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_train_labels\")\n",
        "dummy_test_img_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_test_images\")\n",
        "dummy_test_label_dir= pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_test_labels\")\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMoYlYbYnhFl"
      },
      "source": [
        "Image class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mR75OjC82JOO"
      },
      "outputs": [],
      "source": [
        "class Mri:\n",
        "    def __init__(self, path):\n",
        "        mri_mha = sitk.ReadImage(path, imageIO = \"MetaImageIO\") #explicitly setting ioreader just in case\n",
        "\n",
        "        #resampling\n",
        "        #mri_mha_resampled = resample_img(mri_mha)\n",
        "        #TODO separate resample (bilinear, nearestNeighbor) for images and labels\n",
        "\n",
        "        mri_a = np.array(sitk.GetArrayFromImage(mri_mha)) #mri_array\n",
        "\n",
        "        #transpose array to format z x y\n",
        "        if(mri_a.shape[0] > mri_a.shape[1] or mri_a.shape[0] > mri_a.shape[2]): #if z axis isn't first\n",
        "          mri_a = np.transpose(mri_a, (2, 0, 1))\n",
        "\n",
        "        mri_a_float32 = mri_a.astype(dtype = np.float32)\n",
        "        #TODO: set bounds to [-1000, 2000] https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        self.hu_a = mri_a_float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX_8rD_AyhSQ"
      },
      "source": [
        "Check for sorted directory and dimension order <Br>\n",
        "Get max dimension on x y for zero padding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMiefYLdyRbn",
        "outputId": "9df4b03e-3cff-4f1a-ec74-55a8cc1ee98b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[578, 578, 294, 346, 389, 486, 553, 553, 512, 512, 512, 512, 384, 384, 464, 610, 896, 768, 590, 589, 320, 384, 748, 748, 899, 899, 448, 448, 495, 495, 512, 512, 351, 351, 512, 512, 323, 323, 344, 344, 462, 462, 600, 600, 402, 402]\n",
            "x max: 899\n",
            "y max: 896\n"
          ]
        }
      ],
      "source": [
        "label_dir_list = os.listdir(dummy_train_label_dir)\n",
        "image_dir_list = os.listdir(dummy_train_img_dir)\n",
        "\n",
        "image_dir_list = natsorted(image_dir_list)\n",
        "label_dir_list = natsorted(label_dir_list)\n",
        "\n",
        "dim1_list = []\n",
        "dim2_list = []\n",
        "\n",
        "dirlen = len(os.listdir(dummy_train_label_dir))\n",
        "\n",
        "for idx in range(0, dirlen):\n",
        "  img_path = dummy_train_img_dir.joinpath(image_dir_list[idx])\n",
        "  label_path = dummy_train_label_dir.joinpath(label_dir_list[idx])\n",
        "\n",
        "  image = Mri(img_path)\n",
        "  label = Mri(label_path)\n",
        "  '''\n",
        "  print(idx, \"image: \", image.hu_a.shape)\n",
        "  print(idx, \"label: \", label.hu_a.shape)\n",
        "  '''\n",
        "  dim1_list.append(image.hu_a.shape[1])\n",
        "  dim2_list.append(image.hu_a.shape[2])\n",
        "\n",
        "\n",
        "print(dim1_list)\n",
        "\n",
        "x_dim_max = max(dim1_list)\n",
        "y_dim_max = max(dim2_list)\n",
        "\n",
        "print(\"x max:\", max(dim1_list))\n",
        "print(\"y max:\", max(dim2_list))\n",
        "\n",
        "x_dim_max = 912\n",
        "y_dim_max = 912\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zero Padding to max res of dataset  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_to_resolution(input_tensor, target_resolution):\n",
        "    current_resolution = input_tensor.shape[-2:]\n",
        "\n",
        "    if current_resolution == target_resolution:\n",
        "        return input_tensor  # No padding needed\n",
        "\n",
        "    pad_height = target_resolution[0] - current_resolution[0]\n",
        "    pad_width = target_resolution[1] - current_resolution[1]\n",
        "\n",
        "    # Calculate pad values\n",
        "    top_pad = pad_height // 2\n",
        "    bottom_pad = pad_height - top_pad\n",
        "    left_pad = pad_width // 2\n",
        "    right_pad = pad_width - left_pad\n",
        "\n",
        "    # Apply padding\n",
        "    padded_tensor = torch.nn.functional.pad(input_tensor, (left_pad, right_pad, top_pad, bottom_pad), mode='constant', value=0)\n",
        "\n",
        "    return padded_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVwddxJgnhFn"
      },
      "source": [
        "Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9X6rHdnnhFo",
        "outputId": "5e9fab66-6c87-4081-eaee-cd059d8a4445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train dataset len 46\n",
            "test dataset len 10\n"
          ]
        }
      ],
      "source": [
        "class SpiderDataset(Dataset):\n",
        "    def __init__(self, labels_dir, img_dir, transform=None, target_transform=None):\n",
        "        self.labels_dir = labels_dir\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.labels_dir))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label_dir_list = os.listdir(self.labels_dir)\n",
        "        image_dir_list = os.listdir(self.img_dir)\n",
        "\n",
        "        image_dir_list = natsorted(image_dir_list)\n",
        "        label_dir_list = natsorted(label_dir_list)\n",
        "\n",
        "        img_path = self.img_dir.joinpath(image_dir_list[idx])\n",
        "        label_path = self.labels_dir.joinpath(label_dir_list[idx])\n",
        "\n",
        "        image = Mri(img_path)\n",
        "        label = Mri(label_path)\n",
        "\n",
        "        #image = self.transform(image)\n",
        "        #label = self.target_transform(label)\n",
        "\n",
        "        #comment out the part not being used whether for 3d or 2d model \n",
        "    \n",
        "        '''\n",
        "        #3d tensor for 3D CNN\n",
        "        image_tensor = torch.from_numpy(image.hu_a)\n",
        "        label_tensor = torch.from_numpy(label.hu_a)\n",
        "        '''\n",
        "\n",
        "        #2d tensor for 2D CNN, get random slice from image \n",
        "        rand_idx = np.random.randint(0, image.hu_a.shape[0])\n",
        "\n",
        "        image_tensor = torch.from_numpy(image.hu_a[rand_idx])\n",
        "        label_tensor = torch.from_numpy(label.hu_a[rand_idx])\n",
        "        \n",
        "        image_tensor = image_tensor.to(torch.float32)\n",
        "        label_tensor = label_tensor.to(torch.float32)\n",
        "        \n",
        "        image_tensor = pad_to_resolution(image_tensor, [x_dim_max, y_dim_max])\n",
        "        label_tensor = pad_to_resolution(label_tensor, [x_dim_max, y_dim_max])\n",
        "\n",
        "        image_tensor = image_tensor.unsqueeze(0)\n",
        "        label_tensor = label_tensor.unsqueeze(0)\n",
        "\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "#toy train test dataset to test network running\n",
        "dummy_train_set = SpiderDataset(dummy_train_label_dir, dummy_train_img_dir)\n",
        "dummy_test_set = SpiderDataset(dummy_test_label_dir, dummy_test_img_dir)\n",
        "\n",
        "print(\"train dataset len\",dummy_train_set.__len__())\n",
        "print(\"test dataset len\",dummy_test_set.__len__())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1lWIMzvnhFp"
      },
      "source": [
        "Testing on single image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n-W5pn65Lhm"
      },
      "outputs": [],
      "source": [
        "dir_glob= dummy_train_img_dir.glob(\"*.mha\")\n",
        "\n",
        "for idx in dir_glob:\n",
        "  image = Mri(path = \"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_test_images/30_t1.mha\")\n",
        "  print(image.hu_a)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unet LabML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "---\n",
        "title: U-Net\n",
        "summary: >\n",
        "    PyTorch implementation and tutorial of U-Net model.\n",
        "---\n",
        "\n",
        "# U-Net\n",
        "\n",
        "This is an implementation of the U-Net model from the paper,\n",
        "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597).\n",
        "\n",
        "U-Net consists of a contracting path and an expansive path.\n",
        "The contracting path is a series of convolutional layers and pooling layers,\n",
        "where the resolution of the feature map gets progressively reduced.\n",
        "Expansive path is a series of up-sampling layers and convolutional layers\n",
        "where the resolution of the feature map gets progressively increased.\n",
        "\n",
        "At every step in the expansive path the corresponding feature map from the contracting path\n",
        "concatenated with the current feature map.\n",
        "\n",
        "![U-Net diagram from paper](unet.png)\n",
        "\n",
        "Here is the [training code](experiment.html) for an experiment that trains a U-Net\n",
        "on [Carvana dataset](carvana.html).\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision.transforms.functional\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class DoubleConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Two $3 \\times 3$ Convolution Layers\n",
        "\n",
        "    Each step in the contraction path and expansive path have two $3 \\times 3$\n",
        "    convolutional layers followed by ReLU activations.\n",
        "\n",
        "    In the U-Net paper they used $0$ padding,\n",
        "    but we use $1$ padding so that final feature map is not cropped.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        :param in_channels: is the number of input channels\n",
        "        :param out_channels: is the number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # First $3 \\times 3$ convolutional layer\n",
        "        self.first = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.ReLU()\n",
        "        # Second $3 \\times 3$ convolutional layer\n",
        "        self.second = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Apply the two convolution layers and activations\n",
        "        x = self.first(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.second(x)\n",
        "        return self.act2(x)\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Down-sample\n",
        "\n",
        "    Each step in the contracting path down-samples the feature map with\n",
        "    a $2 \\times 2$ max pooling layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Max pooling layer\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.pool(x)\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Up-sample\n",
        "\n",
        "    Each step in the expansive path up-samples the feature map with\n",
        "    a $2 \\times 2$ up-convolution.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # Up-convolution\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.up(x)\n",
        "\n",
        "\n",
        "class CropAndConcat(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Crop and Concatenate the feature map\n",
        "\n",
        "    At every step in the expansive path the corresponding feature map from the contracting path\n",
        "    concatenated with the current feature map.\n",
        "    \"\"\"\n",
        "    def forward(self, x: torch.Tensor, contracting_x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: current feature map in the expansive path\n",
        "        :param contracting_x: corresponding feature map from the contracting path\n",
        "        \"\"\"\n",
        "\n",
        "        # Crop the feature map from the contracting path to the size of the current feature map\n",
        "        contracting_x = torchvision.transforms.functional.center_crop(contracting_x, [x.shape[2], x.shape[3]])\n",
        "        # Concatenate the feature maps\n",
        "        x = torch.cat([x, contracting_x], dim=1)\n",
        "        #\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    ## U-Net\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        :param in_channels: number of channels in the input image\n",
        "        :param out_channels: number of channels in the result feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Double convolution layers for the contracting path.\n",
        "        # The number of features gets doubled at each step starting from $64$.\n",
        "        self.down_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in\n",
        "                                        [(in_channels, 64), (64, 128), (128, 256), (256, 512)]])\n",
        "        # Down sampling layers for the contracting path\n",
        "        self.down_sample = nn.ModuleList([DownSample() for _ in range(4)])\n",
        "\n",
        "        # The two convolution layers at the lowest resolution (the bottom of the U).\n",
        "        self.middle_conv = DoubleConvolution(512, 1024)\n",
        "\n",
        "        # Up sampling layers for the expansive path.\n",
        "        # The number of features is halved with up-sampling.\n",
        "        self.up_sample = nn.ModuleList([UpSample(i, o) for i, o in\n",
        "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n",
        "        # Double convolution layers for the expansive path.\n",
        "        # Their input is the concatenation of the current feature map and the feature map from the\n",
        "        # contracting path. Therefore, the number of input features is double the number of features\n",
        "        # from up-sampling.\n",
        "        self.up_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in\n",
        "                                      [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n",
        "        # Crop and concatenate layers for the expansive path.\n",
        "        self.concat = nn.ModuleList([CropAndConcat() for _ in range(4)])\n",
        "        # Final $1 \\times 1$ convolution layer to produce the output\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        :param x: input image\n",
        "        \"\"\"\n",
        "        # To collect the outputs of contracting path for later concatenation with the expansive path.\n",
        "        pass_through = []\n",
        "        # Contracting path\n",
        "        for i in range(len(self.down_conv)):\n",
        "            # Two $3 \\times 3$ convolutional layers\n",
        "            x = self.down_conv[i](x)\n",
        "            # Collect the output\n",
        "            pass_through.append(x)\n",
        "            # Down-sample\n",
        "            x = self.down_sample[i](x)\n",
        "\n",
        "        # Two $3 \\times 3$ convolutional layers at the bottom of the U-Net\n",
        "        x = self.middle_conv(x)\n",
        "\n",
        "        # Expansive path\n",
        "        for i in range(len(self.up_conv)):\n",
        "            # Up-sample\n",
        "            x = self.up_sample[i](x)\n",
        "            # Concatenate the output of the contracting path\n",
        "            x = self.concat[i](x, pass_through.pop())\n",
        "            # Two $3 \\times 3$ convolutional layers\n",
        "            x = self.up_conv[i](x)\n",
        "\n",
        "        # Final $1 \\times 1$ convolution layer\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        #\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zsRlBNG-B6Q"
      },
      "source": [
        "Create Unet Instance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CT7aKn7S-F6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (down_conv): ModuleList(\n",
              "    (0): DoubleConvolution(\n",
              "      (first): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (1): DoubleConvolution(\n",
              "      (first): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (2): DoubleConvolution(\n",
              "      (first): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (3): DoubleConvolution(\n",
              "      (first): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (down_sample): ModuleList(\n",
              "    (0-3): 4 x DownSample(\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (middle_conv): DoubleConvolution(\n",
              "    (first): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act1): ReLU()\n",
              "    (second): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act2): ReLU()\n",
              "  )\n",
              "  (up_sample): ModuleList(\n",
              "    (0): UpSample(\n",
              "      (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (1): UpSample(\n",
              "      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (2): UpSample(\n",
              "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (3): UpSample(\n",
              "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "  )\n",
              "  (up_conv): ModuleList(\n",
              "    (0): DoubleConvolution(\n",
              "      (first): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (1): DoubleConvolution(\n",
              "      (first): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (2): DoubleConvolution(\n",
              "      (first): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (3): DoubleConvolution(\n",
              "      (first): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (concat): ModuleList(\n",
              "    (0-3): 4 x CropAndConcat()\n",
              "  )\n",
              "  (final_conv): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_channels = 1 #Hounsfield scale\n",
        "output_channels = 3 #Vertebra, disc and spinal canal masks\n",
        "model = UNet(in_channels = input_channels, out_channels = output_channels)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcuWKZAD9pHD"
      },
      "source": [
        "Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4lnCiKNW9sJN"
      },
      "outputs": [],
      "source": [
        "epochs = 1 #testing\n",
        "lr = 0.001 #testing\n",
        "batchsize = 2 #testing\n",
        "loss_func = nn.MSELoss()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWO0s80znhFr"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UQL_nxpCnhFs"
      },
      "outputs": [],
      "source": [
        "dummy_train_dataloader = DataLoader(dummy_train_set, batch_size = batchsize, shuffle=True)\n",
        "dummy_test_dataloader = DataLoader(dummy_test_set, batch_size = batchsize, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArQ0KlDx4IiP"
      },
      "source": [
        "Dataloader Iterate through Z Axis of tensor (3D tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsnKSM1S4QCa",
        "outputId": "de0bea75-56c2-4ee9-c19d-4fd27643e95a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 899, 896])\n"
          ]
        }
      ],
      "source": [
        "for images, masks in dummy_test_dataloader:\n",
        "  for i in images, masks:\n",
        "    print(i.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxAIK14tWJbp"
      },
      "source": [
        "One Epoch <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jxCZ41T_WOBF"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(dummy_train_dataloader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        print(inputs.shape)\n",
        "\n",
        "        #inputs = inputs.transpose(-1,0)\n",
        "        #labels = labels.transpose(-1,0)\n",
        "        #inputs = inputs.reshape(inputs.shape(1), inputs.shape(0), inputs.shape(3), inputs.shape(4))\n",
        "        #labels = labels.reshape(labels.shape(1), labels.shape(0), labels.shape(3), labels.shape(4))\n",
        "\n",
        "      \n",
        "        # Zero your gradients for every batch!\n",
        "        optim.zero_grad()\n",
        "\n",
        "        #2d\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optim.step()\n",
        "        \n",
        "        '''\n",
        "        #run everything while indexing through z axis\n",
        "        for axis in images, masks:\n",
        "          for idx in range(0, axis.size(1)):\n",
        "            # Zero your gradients for every batch!\n",
        "            optim.zero_grad()\n",
        "            print(inputs[:, idx, : ,:])\n",
        "            # Make predictions for this batch\n",
        "            outputs = model(inputs[:, idx, : ,:])\n",
        "\n",
        "            # Compute the loss and its gradients\n",
        "            loss = loss_func(outputs[:, idx, : ,:], labels[:, idx, : ,:])\n",
        "            loss.backward()\n",
        "\n",
        "            # Adjust learning weights\n",
        "            optim.step()\n",
        "        '''\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(dummy_train_dataloader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyaaV1lYYiFc"
      },
      "source": [
        "Train Loop <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7i7TB_oOYhxQ",
        "outputId": "bfdaa28e-8cbc-4f46-be69-0979a6d3ceff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n",
            "torch.Size([2, 1, 912, 912])\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 6.00 GiB total capacity; 5.20 GiB already allocated; 0 bytes free; 5.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(epoch_number, writer)\n\u001b[0;32m     20\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# statistics for batch normalization.\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[12], line 26\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m     22\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#2d\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(outputs, labels)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[8], line 181\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    179\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat[i](x, pass_through\u001b[38;5;241m.\u001b[39mpop())\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# Two $3 \\times 3$ convolutional layers\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_conv[i](x)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Final $1 \\times 1$ convolution layer\u001b[39;00m\n\u001b[0;32m    184\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(x)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[8], line 62\u001b[0m, in \u001b[0;36mDoubleConvolution.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact1(x)\n\u001b[0;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond(x)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact2(x)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 6.00 GiB total capacity; 5.20 GiB already allocated; 0 bytes free; 5.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/spider_seg_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 1\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(dummy_test_dataloader):\n",
        "            vinputs, vlabels = vdata\n",
        "\n",
        "            #from tutorial code start\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = loss_func(voutputs, vlabels)\n",
        "            running_vloss += vloss\n",
        "            #from tutorial code end\n",
        "\n",
        "            for slices in vdata.size(1):\n",
        "              voutputs = model(vinputs)\n",
        "              vloss = loss_func(voutputs, vlabels)\n",
        "              running_vloss += vloss\n",
        "\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
