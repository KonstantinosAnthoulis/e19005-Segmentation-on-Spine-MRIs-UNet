{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFL78HZcbmSP"
      },
      "source": [
        "\n",
        "Connect to Google Drive for datasets (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcBj2OvobfY3",
        "outputId": "28f8c7e7-eb9b-490c-bd3c-674444fbe332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1Wa9Ngfw6QC"
      },
      "source": [
        "Install dependencies (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOCzMWFLw53J",
        "outputId": "1a06cc2b-3e31-4c79-8917-9c8f90e50b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting simpleitk\n",
            "  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simpleitk\n",
            "Successfully installed simpleitk-2.3.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install simpleitk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVzzg3MJnhFU"
      },
      "source": [
        "Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QMgkyFD0Hm3",
        "outputId": "50d2df41-f013-4d5e-8c9a-b2d356fb8a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.widgets import Slider, Button, RadioButtons\n",
        "\n",
        "import scipy\n",
        "\n",
        "import SimpleITK as sitk\n",
        "reader = sitk.ImageFileReader()\n",
        "reader.SetImageIO(\"MetaImageIO\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import pathlib\n",
        "\n",
        "from natsort import natsorted\n",
        "\n",
        "#Set GPU/Cuda Device to run model on\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "np.random.seed(46)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H48GvrcPnhFh"
      },
      "source": [
        "Dataset Directories <Br>\n",
        "Comment out directory not in use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "35vekrfdnhFi"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n#Colab Google Drive Directories Toy Dataset\\ndummy_train_img_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_train_images\")\\ndummy_train_label_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_train_labels\")\\ndummy_test_img_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_test_images\")\\ndummy_test_label_dir= pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_test_labels\")\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Toy Dataset Directory from Spider | Grand Challenge \n",
        "dummy_train_img_dir = pathlib.Path(r\"spider_toy_dset/dummy_train_images\")\n",
        "dummy_train_label_dir = pathlib.Path(r\"D:\\Spider Data/dummy_train_labels\")\n",
        "dummy_test_img_dir = pathlib.Path(r\"D:\\Spider Data/dummy_test_images\")\n",
        "dummy_test_label_dir= pathlib.Path(r\"D:\\Spider Data/dummy_test_labels\")\n",
        "\n",
        "'''\n",
        "#Colab Google Drive Directories Toy Dataset\n",
        "dummy_train_img_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_train_images\")\n",
        "dummy_train_label_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_train_labels\")\n",
        "dummy_test_img_dir = pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_test_images\")\n",
        "dummy_test_label_dir= pathlib.Path(r\"/content/gdrive/MyDrive/Spider Dummy Dataset/dummy_test_labels\")\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMoYlYbYnhFl"
      },
      "source": [
        "Image class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mR75OjC82JOO"
      },
      "outputs": [],
      "source": [
        "class Mri:\n",
        "    def __init__(self, path):\n",
        "        mri_mha = sitk.ReadImage(path, imageIO = \"MetaImageIO\") #explicitly setting ioreader just in case\n",
        "\n",
        "        #resampling\n",
        "        #mri_mha_resampled = resample_img(mri_mha)\n",
        "        #TODO separate resample (bilinear, nearestNeighbor) for images and labels\n",
        "\n",
        "        mri_a = np.array(sitk.GetArrayFromImage(mri_mha)) #mri_array\n",
        "\n",
        "        #transpose array to format z x y\n",
        "        if(mri_a.shape[0] > mri_a.shape[1] or mri_a.shape[0] > mri_a.shape[2]): #if z axis isn't first\n",
        "          mri_a = np.transpose(mri_a, (2, 0, 1))\n",
        "      \n",
        "        mri_a_float32 = mri_a.astype(dtype = np.float32)\n",
        "        #TODO: set bounds to [-1000, 2000] https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        self.hu_a = mri_a_float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX_8rD_AyhSQ"
      },
      "source": [
        "Check for sorted directory and dimension order <Br>\n",
        "Get max dimension on x y for zero padding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMiefYLdyRbn",
        "outputId": "9df4b03e-3cff-4f1a-ec74-55a8cc1ee98b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x max: 899\n",
            "y max: 896\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nx_dim_max = 912 \\ny_dim_max = 912\\n#912 was done by calculating the nearest multiple of 16 **above** x and y\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#get lists from directories\n",
        "label_dir_list = os.listdir(dummy_train_label_dir)\n",
        "image_dir_list = os.listdir(dummy_train_img_dir) \n",
        "#sort lists\n",
        "image_dir_list = natsorted(image_dir_list)\n",
        "label_dir_list = natsorted(label_dir_list)\n",
        "#empty lists to hold x and y dimensions of images\n",
        "dim1_list = []\n",
        "dim2_list = []\n",
        "\n",
        "dirlen = len(os.listdir(dummy_train_label_dir))\n",
        "\n",
        "for idx in range(0, dirlen):\n",
        "  img_path = dummy_train_img_dir.joinpath(image_dir_list[idx])\n",
        "  label_path = dummy_train_label_dir.joinpath(label_dir_list[idx])\n",
        "\n",
        "  image = Mri(img_path)\n",
        "  label = Mri(label_path)\n",
        "  '''\n",
        "  print(idx, \"image: \", image.hu_a.shape)\n",
        "  print(idx, \"label: \", label.hu_a.shape)\n",
        "  '''\n",
        "  dim1_list.append(image.hu_a.shape[1]) #add x value to list\n",
        "  dim2_list.append(image.hu_a.shape[2]) #add y value to list \n",
        "\n",
        "#calculate max \n",
        "x_dim_max = max(dim1_list)\n",
        "y_dim_max = max(dim2_list)\n",
        "\n",
        "print(\"x max:\", max(dim1_list))\n",
        "print(\"y max:\", max(dim2_list))\n",
        "'''\n",
        "x_dim_max = 912 \n",
        "y_dim_max = 912\n",
        "#912 was done by calculating the nearest multiple of 16 **above** x and y\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find empty slices in labels <br>\n",
        "Create new label and image arrays without the slices w/o mask info <br>\n",
        "Cell works for 1 image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size of array before trimming 50\n",
            "size of array after trimming what it should be 29\n",
            "True\n",
            "idx of slice being added 0\n",
            "idx of slices 8\n",
            "True\n",
            "idx of slice being added 1\n",
            "idx of slices 9\n",
            "True\n",
            "idx of slice being added 2\n",
            "idx of slices 10\n",
            "True\n",
            "idx of slice being added 3\n",
            "idx of slices 11\n",
            "True\n",
            "idx of slice being added 4\n",
            "idx of slices 12\n",
            "True\n",
            "idx of slice being added 5\n",
            "idx of slices 13\n",
            "True\n",
            "idx of slice being added 6\n",
            "idx of slices 14\n",
            "True\n",
            "idx of slice being added 7\n",
            "idx of slices 15\n",
            "True\n",
            "idx of slice being added 8\n",
            "idx of slices 16\n",
            "True\n",
            "idx of slice being added 9\n",
            "idx of slices 17\n",
            "True\n",
            "idx of slice being added 10\n",
            "idx of slices 18\n",
            "True\n",
            "idx of slice being added 11\n",
            "idx of slices 19\n",
            "True\n",
            "idx of slice being added 12\n",
            "idx of slices 20\n",
            "True\n",
            "idx of slice being added 13\n",
            "idx of slices 21\n",
            "True\n",
            "idx of slice being added 14\n",
            "idx of slices 22\n",
            "True\n",
            "idx of slice being added 15\n",
            "idx of slices 23\n",
            "True\n",
            "idx of slice being added 16\n",
            "idx of slices 24\n",
            "True\n",
            "idx of slice being added 17\n",
            "idx of slices 25\n",
            "True\n",
            "idx of slice being added 18\n",
            "idx of slices 26\n",
            "True\n",
            "idx of slice being added 19\n",
            "idx of slices 27\n",
            "True\n",
            "idx of slice being added 20\n",
            "idx of slices 28\n",
            "(29, 578, 448)\n"
          ]
        }
      ],
      "source": [
        "label_dir_list = os.listdir(dummy_train_label_dir)\n",
        "image_dir_list = os.listdir(dummy_train_img_dir)\n",
        "\n",
        "image_dir_list = natsorted(image_dir_list)\n",
        "label_dir_list = natsorted(label_dir_list)\n",
        "\n",
        "dim1_list = []\n",
        "dim2_list = []\n",
        "\n",
        "dirlen = len(os.listdir(dummy_train_label_dir))\n",
        "\n",
        "'''\n",
        "for idx in range(0, dirlen):# for every label image in directory \n",
        "  img_path = dummy_train_img_dir.joinpath(image_dir_list[idx]) \n",
        "  label_path = dummy_train_label_dir.joinpath(label_dir_list[idx])\n",
        "\n",
        "  image = Mri(img_path)\n",
        "  label = Mri(label_path)\n",
        "  break\n",
        " #print(idx, \"image: \", image.hu_a.shape)\n",
        "  #print(idx, \"label: \", label.hu_a.shape)\n",
        "'''\n",
        "img_path = dummy_train_img_dir.joinpath(image_dir_list[0]) \n",
        "label_path = dummy_train_label_dir.joinpath(label_dir_list[0])\n",
        "\n",
        "image = Mri(img_path)\n",
        "label = Mri(label_path)\n",
        "\n",
        "#creating new array instead of deleting from old one \n",
        "slice_num = -1 #negative 1 because +1 brings us to [0] as first slice \n",
        "x = label.hu_a.shape[1]\n",
        "y = label.hu_a.shape[2]\n",
        "\n",
        "print(\"size of array before trimming\", label.hu_a.shape[0])\n",
        "\n",
        "#loop to count non empty slices in file \n",
        "for idx in range(label.hu_a.shape[0]): #go through slices \n",
        "    #print(\"array shape before adding slice\" , new_label.shape) #print array shape before removing slice \n",
        "    if (np.any(label.hu_a[idx])): #slice is NOT empty\n",
        "       #print(\"non empty slice at idx\", idx)\n",
        "       slice_num = slice_num +1 \n",
        "\n",
        "new_label_hu = np.empty((slice_num, x, y))\n",
        "new_image_hu = np.empty((slice_num, x, y))\n",
        "\n",
        "print(\"size of array after trimming what it should be\", new_label_hu.shape[0])\n",
        "\n",
        "new_index = 0\n",
        "for idx in range(new_label_hu.shape[0]): #go through slices of original array\n",
        "    if (np.any(label.hu_a[idx])): #slice is NOT empty\n",
        "       print(np.any(label.hu_a[idx]))\n",
        "       new_label_hu[new_index] = label.hu_a[idx] #add non empty slice to array\n",
        "       new_image_hu[new_index] = image.hu_a[idx]\n",
        "       #print(\"slice added\", new_label_hu[new_index].shape)\n",
        "       print(\"idx of slice being added\", new_index)\n",
        "       print(\"idx of slices\", idx)\n",
        "       new_index = new_index +1\n",
        "\n",
        "\n",
        "print(new_label_hu.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For slices that aren't empty delete the surrounding 0s to bring resolution down <br>\n",
        "Will work on the test arrays from the cell above "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "slice and non 0 linse cols [0, 17, 9]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [1, 31, 14]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [2, 33, 17]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [3, 52, 31]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [4, 81, 47]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [5, 108, 58]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [6, 122, 67]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [7, 136, 73]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [8, 163, 81]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [9, 179, 105]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [10, 182, 124]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [11, 189, 126]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [12, 217, 126]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [13, 253, 131]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [14, 290, 134]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [15, 346, 143]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [16, 383, 140]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [17, 400, 125]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [18, 406, 120]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [19, 381, 118]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [20, 365, 125]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [21, 0, 0]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [22, 0, 0]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [23, 0, 0]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [24, 0, 0]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [25, 0, 0]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [26, 0, 0]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [27, 0, 0]\n",
            "original image res (50, 578, 448)\n",
            "slice and non 0 linse cols [28, 0, 0]\n",
            "original image res (50, 578, 448)\n"
          ]
        }
      ],
      "source": [
        "#TODO make this work \n",
        "row_max = 0\n",
        "col_max = 0 #max of non empty rows and columns in the slices \n",
        "\n",
        "max_list = [] #will hold [idx, count of rows, count of cols] \n",
        "\n",
        "print(np.any(new_label_hu))\n",
        "\n",
        "for idx in range(new_label_hu.shape[0]): #go through non empty slices\n",
        "    row_count = 0 #reset for each slice\n",
        "    col_count = 0 #reset for each slice \n",
        "\n",
        "    row_start = 0 #reset for each slice\n",
        "    row_end = 0\n",
        "    col_start = 0\n",
        "\n",
        "    for idx_row in range(new_label_hu.shape[1]):\n",
        "        #print(type(idx_row))\n",
        "        if  np.any(new_label_hu[idx, idx_row, :]):\n",
        "            row_count = row_count + 1\n",
        "    for idx_col in range(new_label_hu.shape[2]):\n",
        "        if  np.any(new_label_hu[idx, : ,idx_col]):\n",
        "            col_count = col_count +1\n",
        "    max_list.append([idx, row_count, col_count]) #index of slice in mri, row count, col count  \n",
        "    print(\"slice and non 0 linse cols\",max_list[idx])   \n",
        "\n",
        "    \n",
        "     \n",
        "print(\"original image res\", label.hu_a.shape) \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVwddxJgnhFn"
      },
      "source": [
        "Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9X6rHdnnhFo",
        "outputId": "5e9fab66-6c87-4081-eaee-cd059d8a4445"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dummy_train_label_dir' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image_tensor, label_tensor\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#toy train test dataset to test network running\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m dummy_train_set \u001b[38;5;241m=\u001b[39m SpiderDataset(dummy_train_label_dir, dummy_train_img_dir)\n\u001b[0;32m     56\u001b[0m dummy_test_set \u001b[38;5;241m=\u001b[39m SpiderDataset(dummy_test_label_dir, dummy_test_img_dir)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain dataset len\u001b[39m\u001b[38;5;124m\"\u001b[39m,dummy_train_set\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m())\n",
            "\u001b[1;31mNameError\u001b[0m: name 'dummy_train_label_dir' is not defined"
          ]
        }
      ],
      "source": [
        "from transforms import tensor_transforms\n",
        "\n",
        "class SpiderDataset(Dataset):\n",
        "    def __init__(self, labels_dir, img_dir, transform=None, target_transform=None):\n",
        "        self.labels_dir = labels_dir\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.labels_dir))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label_dir_list = os.listdir(self.labels_dir)\n",
        "        image_dir_list = os.listdir(self.img_dir)\n",
        "\n",
        "        image_dir_list = natsorted(image_dir_list)\n",
        "        label_dir_list = natsorted(label_dir_list)\n",
        "\n",
        "        img_path = self.img_dir.joinpath(image_dir_list[idx])\n",
        "        label_path = self.labels_dir.joinpath(label_dir_list[idx])\n",
        "\n",
        "        image = Mri(img_path)\n",
        "        label = Mri(label_path)\n",
        "\n",
        "        #image = self.transform(image)\n",
        "        #label = self.target_transform(label)\n",
        "\n",
        "        #comment out the part not being used whether for 3d or 2d model \n",
        "    \n",
        "        '''\n",
        "        #3d tensor for 3D CNN\n",
        "        image_tensor = torch.from_numpy(image.hu_a)\n",
        "        label_tensor = torch.from_numpy(label.hu_a)\n",
        "        '''\n",
        "\n",
        "        #2d tensor for 2D CNN, get random slice from image \n",
        "        rand_idx = np.random.randint(0, image.hu_a.shape[0])\n",
        "\n",
        "        image_tensor = torch.from_numpy(image.hu_a[rand_idx])\n",
        "        label_tensor = torch.from_numpy(label.hu_a[rand_idx])\n",
        "        \n",
        "        image_tensor = image_tensor.to(torch.float32)\n",
        "        label_tensor = label_tensor.to(torch.float32)\n",
        "        \n",
        "        #pad to max resolution of slice in dset \n",
        "        image_tensor = tensor_transforms.pad_to_resolution(image_tensor, [x_dim_max, y_dim_max])\n",
        "        label_tensor = tensor_transforms.pad_to_resolution(label_tensor, [x_dim_max, y_dim_max])\n",
        "\n",
        "        image_tensor = image_tensor.unsqueeze(0)\n",
        "        label_tensor = label_tensor.unsqueeze(0)\n",
        "\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "#toy train test dataset to test network running\n",
        "dummy_train_set = SpiderDataset(dummy_train_label_dir, dummy_train_img_dir)\n",
        "dummy_test_set = SpiderDataset(dummy_test_label_dir, dummy_test_img_dir)\n",
        "\n",
        "print(\"train dataset len\",dummy_train_set.__len__())\n",
        "print(\"test dataset len\",dummy_test_set.__len__())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zsRlBNG-B6Q"
      },
      "source": [
        "Create Unet Instance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CT7aKn7S-F6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (down_conv): ModuleList(\n",
              "    (0): DoubleConvolution(\n",
              "      (first): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (1): DoubleConvolution(\n",
              "      (first): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (2): DoubleConvolution(\n",
              "      (first): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (3): DoubleConvolution(\n",
              "      (first): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (down_sample): ModuleList(\n",
              "    (0-3): 4 x DownSample(\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (middle_conv): DoubleConvolution(\n",
              "    (first): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act1): ReLU()\n",
              "    (second): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act2): ReLU()\n",
              "  )\n",
              "  (up_sample): ModuleList(\n",
              "    (0): UpSample(\n",
              "      (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (1): UpSample(\n",
              "      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (2): UpSample(\n",
              "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (3): UpSample(\n",
              "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "  )\n",
              "  (up_conv): ModuleList(\n",
              "    (0): DoubleConvolution(\n",
              "      (first): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (1): DoubleConvolution(\n",
              "      (first): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (2): DoubleConvolution(\n",
              "      (first): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (3): DoubleConvolution(\n",
              "      (first): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (concat): ModuleList(\n",
              "    (0-3): 4 x CropAndConcat()\n",
              "  )\n",
              "  (final_conv): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from models import unet \n",
        "\n",
        "input_channels = 1 #Hounsfield scale\n",
        "output_channels = 3 #Vertebra, disc and spinal canal masks\n",
        "model = unet.UNet(in_channels = input_channels, out_channels = output_channels)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcuWKZAD9pHD"
      },
      "source": [
        "Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4lnCiKNW9sJN"
      },
      "outputs": [],
      "source": [
        "epochs = 1 #testing\n",
        "lr = 0.001 #testing\n",
        "batchsize = 2 #testing\n",
        "loss_func = nn.MSELoss()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWO0s80znhFr"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UQL_nxpCnhFs"
      },
      "outputs": [],
      "source": [
        "dummy_train_dataloader = DataLoader(dummy_train_set, batch_size = batchsize, shuffle=True)\n",
        "dummy_test_dataloader = DataLoader(dummy_test_set, batch_size = batchsize, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArQ0KlDx4IiP"
      },
      "source": [
        "Dataloader Iterate through Z Axis of tensor (3D tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsnKSM1S4QCa",
        "outputId": "de0bea75-56c2-4ee9-c19d-4fd27643e95a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 899, 896])\n"
          ]
        }
      ],
      "source": [
        "for images, masks in dummy_test_dataloader:\n",
        "  for i in images, masks:\n",
        "    print(i.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxAIK14tWJbp"
      },
      "source": [
        "One Epoch <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jxCZ41T_WOBF"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(dummy_train_dataloader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        print(inputs.shape)\n",
        "\n",
        "        #inputs = inputs.transpose(-1,0)\n",
        "        #labels = labels.transpose(-1,0)\n",
        "        #inputs = inputs.reshape(inputs.shape(1), inputs.shape(0), inputs.shape(3), inputs.shape(4))\n",
        "        #labels = labels.reshape(labels.shape(1), labels.shape(0), labels.shape(3), labels.shape(4))\n",
        "\n",
        "      \n",
        "        # Zero your gradients for every batch!\n",
        "        optim.zero_grad()\n",
        "\n",
        "        #2d\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optim.step()\n",
        "        \n",
        "        '''\n",
        "        #run everything while indexing through z axis\n",
        "        for axis in images, masks:\n",
        "          for idx in range(0, axis.size(1)):\n",
        "            # Zero your gradients for every batch!\n",
        "            optim.zero_grad()\n",
        "            print(inputs[:, idx, : ,:])\n",
        "            # Make predictions for this batch\n",
        "            outputs = model(inputs[:, idx, : ,:])\n",
        "\n",
        "            # Compute the loss and its gradients\n",
        "            loss = loss_func(outputs[:, idx, : ,:], labels[:, idx, : ,:])\n",
        "            loss.backward()\n",
        "\n",
        "            # Adjust learning weights\n",
        "            optim.step()\n",
        "        '''\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(dummy_train_dataloader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyaaV1lYYiFc"
      },
      "source": [
        "Train Loop <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7i7TB_oOYhxQ",
        "outputId": "bfdaa28e-8cbc-4f46-be69-0979a6d3ceff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n",
            "torch.Size([2, 1, 912, 912])\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 6.00 GiB total capacity; 5.20 GiB already allocated; 0 bytes free; 5.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(epoch_number, writer)\n\u001b[0;32m     20\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# statistics for batch normalization.\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[12], line 26\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m     22\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#2d\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(outputs, labels)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[8], line 181\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    179\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat[i](x, pass_through\u001b[38;5;241m.\u001b[39mpop())\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# Two $3 \\times 3$ convolutional layers\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_conv[i](x)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Final $1 \\times 1$ convolution layer\u001b[39;00m\n\u001b[0;32m    184\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(x)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[8], line 62\u001b[0m, in \u001b[0;36mDoubleConvolution.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact1(x)\n\u001b[0;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond(x)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact2(x)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 6.00 GiB total capacity; 5.20 GiB already allocated; 0 bytes free; 5.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/spider_seg_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 1\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(dummy_test_dataloader):\n",
        "            vinputs, vlabels = vdata\n",
        "\n",
        "            #from tutorial code start\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = loss_func(voutputs, vlabels)\n",
        "            running_vloss += vloss\n",
        "            #from tutorial code end\n",
        "\n",
        "            for slices in vdata.size(1):\n",
        "              voutputs = model(vinputs)\n",
        "              vloss = loss_func(voutputs, vlabels)\n",
        "              running_vloss += vloss\n",
        "\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
