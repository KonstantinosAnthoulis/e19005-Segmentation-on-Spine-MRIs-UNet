{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFL78HZcbmSP"
      },
      "source": [
        "\n",
        "Connect to Google Drive for datasets (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcBj2OvobfY3",
        "outputId": "28f8c7e7-eb9b-490c-bd3c-674444fbe332"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1Wa9Ngfw6QC"
      },
      "source": [
        "Install dependencies (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOCzMWFLw53J",
        "outputId": "1a06cc2b-3e31-4c79-8917-9c8f90e50b52"
      },
      "outputs": [],
      "source": [
        "#!pip install simpleitk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVzzg3MJnhFU"
      },
      "source": [
        "Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QMgkyFD0Hm3",
        "outputId": "50d2df41-f013-4d5e-8c9a-b2d356fb8a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.widgets import Slider, Button, RadioButtons\n",
        "\n",
        "import SimpleITK as sitk\n",
        "reader = sitk.ImageFileReader()\n",
        "reader.SetImageIO(\"MetaImageIO\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import pathlib\n",
        "\n",
        "from natsort import natsorted\n",
        "\n",
        "#Set GPU/Cuda Device to run model on\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "np.random.seed(46)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H48GvrcPnhFh"
      },
      "source": [
        "Dataset Directories <Br>\n",
        "Comment out directory not in use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "35vekrfdnhFi"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Toy Dataset Slices \n",
        "'''\n",
        "dummy_train_img_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_train_img_slices\")\n",
        "dummy_train_label_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_train_label_slices\")\n",
        "dummy_test_img_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_test_img_slices\")\n",
        "dummy_test_label_slice_dir= pathlib.Path(r\"spider_toy_dset_slices/dummy_test_label_slices\")\n",
        "\n",
        "'''\n",
        "'''\n",
        "#Local Dataset Slices - Removed Slices\n",
        "dummy_train_img_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/train_image_slices\")\n",
        "dummy_train_label_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/train_label_slices\")\n",
        "\n",
        "dummy_test_img_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/test_image_slices\")\n",
        "dummy_test_label_slice_dir= pathlib.Path(\"D:/Spider Slice Directories/test_label_slices\")\n",
        "\n",
        "'''\n",
        "#Local Dataset Slices - Voxel Resampling [2, 0.6, 0.6] Bspline Interpolation \n",
        "dummy_train_img_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/train_image_bspline_slices\")\n",
        "dummy_train_label_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/train_label_bspline_slices\")\n",
        "\n",
        "dummy_test_img_slice_dir = pathlib.Path(\"D:/Spider Slice Directories/test_image_bspline_slices\")\n",
        "dummy_test_label_slice_dir= pathlib.Path(\"D:/Spider Slice Directories/test_label_bspline_slices\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMoYlYbYnhFl"
      },
      "source": [
        "Image Slice Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mR75OjC82JOO"
      },
      "outputs": [],
      "source": [
        "from transforms import mri_transforms\n",
        "\n",
        "\n",
        "#TODO add bool image label for interp \n",
        "class Mri_Slice:\n",
        "    def __init__(self, path):\n",
        "        mri_mha = sitk.ReadImage(path, imageIO = \"MetaImageIO\") #explicitly setting ioreader just in case\n",
        "\n",
        "        #grabbing the image as well for downsampling testing\n",
        "        self.mri_mha = mri_mha\n",
        "       \n",
        "        #get 2d array from mri slice\n",
        "        mri_a = np.array(sitk.GetArrayFromImage(mri_mha)) #mri_array\n",
        "        \n",
        "        mri_a_float32 = mri_a.astype(dtype = np.float32)\n",
        "        #TODO: set bounds to [-1000, 2000] https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        self.hu_a = mri_a_float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zero crop sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image (0000011A85763D80)\n",
            "  RTTI typeinfo:   class itk::Image<double,2>\n",
            "  Reference Count: 1\n",
            "  Modified Time: 2067\n",
            "  Debug: Off\n",
            "  Object Name: \n",
            "  Observers: \n",
            "    none\n",
            "  Source: (none)\n",
            "  Source output name: (none)\n",
            "  Release Data: Off\n",
            "  Data Released: False\n",
            "  Global Release Data: Off\n",
            "  PipelineMTime: 2044\n",
            "  UpdateMTime: 2063\n",
            "  RealTimeStamp: 0 seconds \n",
            "  LargestPossibleRegion: \n",
            "    Dimension: 2\n",
            "    Index: [0, 0]\n",
            "    Size: [467, 482]\n",
            "  BufferedRegion: \n",
            "    Dimension: 2\n",
            "    Index: [0, 0]\n",
            "    Size: [467, 482]\n",
            "  RequestedRegion: \n",
            "    Dimension: 2\n",
            "    Index: [0, 0]\n",
            "    Size: [467, 482]\n",
            "  Spacing: [1, 1]\n",
            "  Origin: [0, 0]\n",
            "  Direction: \n",
            "1 0\n",
            "0 1\n",
            "\n",
            "  IndexToPointMatrix: \n",
            "1 0\n",
            "0 1\n",
            "\n",
            "  PointToIndexMatrix: \n",
            "1 0\n",
            "0 1\n",
            "\n",
            "  Inverse Direction: \n",
            "1 0\n",
            "0 1\n",
            "\n",
            "  PixelContainer: \n",
            "    ImportImageContainer (0000011A8C778CE0)\n",
            "      RTTI typeinfo:   class itk::ImportImageContainer<unsigned __int64,double>\n",
            "      Reference Count: 1\n",
            "      Modified Time: 2060\n",
            "      Debug: Off\n",
            "      Object Name: \n",
            "      Observers: \n",
            "        none\n",
            "      Pointer: 0000011A93E16040\n",
            "      Container manages memory: true\n",
            "      Size: 225094\n",
            "      Capacity: 225094\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transforms import array_transforms\n",
        "\n",
        "test_image_slice = Mri_Slice(r\"spider_toy_dset_slices/dummy_train_img_slices/1_t1_30.mha\")\n",
        "test_label_slice = Mri_Slice(r\"spider_toy_dset_slices/dummy_train_label_slices/1_t1_30.mha\")\n",
        "\n",
        "print(test_label_slice.mri_mha)\n",
        "\n",
        "test_image_a = test_image_slice.hu_a\n",
        "test_label_a = test_label_slice.hu_a\n",
        "\n",
        "test_image_a, test_label_a = array_transforms.crop_zero(test_image_a, test_label_a)\n",
        "\n",
        "test_image_crop_mha = sitk.GetImageFromArray(test_image_a)\n",
        "\n",
        "sitk.WriteImage(test_image_crop_mha, \"D:/sitk dump/1_t1_30_cropzero.mha\")\n",
        "\n",
        "#TODO continue here make images 1:1 scale and downsample to 64x64\n",
        "#if(sitk_slice.GetSize()[0] != sitk_slice.GetSize()[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX_8rD_AyhSQ"
      },
      "source": [
        "Sort directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMiefYLdyRbn",
        "outputId": "9df4b03e-3cff-4f1a-ec74-55a8cc1ee98b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D:\\Spider Slice Directories\\train_image_bspline_slices\n",
            "D:\\Spider Slice Directories\\train_label_bspline_slices\n",
            "15662\n"
          ]
        }
      ],
      "source": [
        "#get lists from directories\n",
        "\n",
        "#toy dset slices\n",
        "image_path = dummy_train_img_slice_dir\n",
        "label_path = dummy_train_label_slice_dir\n",
        "\n",
        "image_dir_list = os.listdir(image_path)\n",
        "label_dir_list = os.listdir(label_path)\n",
        "\n",
        "print(image_path) \n",
        "print(label_path)\n",
        "\n",
        "#local dset\n",
        "'''\n",
        "image_dir_list = os.listdir(local_img_idr)\n",
        "label_dir_list = os.listdir(local_label_dir)\n",
        "'''\n",
        "#sort lists\n",
        "image_dir_list = natsorted(image_dir_list)\n",
        "label_dir_list = natsorted(label_dir_list)\n",
        "#empty lists to hold x and y dimensions of images\n",
        "row_list = []\n",
        "col_list = []\n",
        "\n",
        "#dirlen = len(os.listdir(dummy_train_label_dir))\n",
        "dirlen = len(os.listdir(label_path))\n",
        "\n",
        "print(dirlen)\n",
        "\n",
        "#print(local_label_idr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get max dimension of slice in dset for x y padding <br>\n",
        "Images have slices with 0 label info removed and are cropped using zero crop <br>\n",
        "Also get min max values of dset for tensor normalization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19040\\701229525.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m   \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal_img_idr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_dir_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[0mlabel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal_label_dir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_dir_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#first part before joinpath is pathlib.Path, second part is the directory of hte file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m   '''\n\u001b[0;32m     19\u001b[0m   \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMri_Slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m   \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMri_Slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlbl_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m   \u001b[0mimage_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhu_a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m   \u001b[0mlabel_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhu_a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19040\\4177542513.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mmri_mha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msitk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReadImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimageIO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"MetaImageIO\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#explicitly setting ioreader just in case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#grabbing the image as well for downsampling testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmri_mha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmri_mha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\SimpleITK\\extra.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(fileName, outputPixelType, imageIO)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetFileNames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileName\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetImageIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageIO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetOutputPixelType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputPixelType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32md:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\SimpleITK\\SimpleITK.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   8426\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msame\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpixel\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mitk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mConvertPixelBuffer\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpixels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8429\u001b[0m         \"\"\"\n\u001b[1;32m-> 8430\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_SimpleITK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFileReader_Execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transforms import array_transforms\n",
        "\n",
        "\n",
        "\n",
        "row_list = []\n",
        "col_list = []\n",
        "\n",
        "for idx in range(0, dirlen):\n",
        "  #print(\"dirlen\", dirlen)\n",
        "  \n",
        " #toy dset \n",
        "  \n",
        "  img_path = image_path.joinpath(image_dir_list[idx])\n",
        "  lbl_path = label_path.joinpath(label_dir_list[idx])#first part before joinpath is pathlib.Path, second part is the directory of hte file \n",
        "  '''\n",
        "  img_path = local_img_idr.joinpath(image_dir_list[idx])\n",
        "  label_path = local_label_dir.joinpath(label_dir_list[idx]) #first part before joinpath is pathlib.Path, second part is the directory of hte file \n",
        "  '''\n",
        "  image = Mri_Slice(img_path)\n",
        "  label = Mri_Slice(lbl_path)\n",
        "\n",
        "  image_a = image.hu_a\n",
        "  label_a = label.hu_a\n",
        "\n",
        "  #crop zero\n",
        "  image_a_cropzero, label_a_cropzero = array_transforms.crop_zero(image_a, label_a)\n",
        "\n",
        "  #resampling code will go here instead of crop-zero\n",
        "\n",
        "\n",
        "  if(idx ==0):\n",
        "    image_tensor_min = np.min(image_a_cropzero)\n",
        "    image_tensor_max = np.max(label_a_cropzero)\n",
        "    label_tensor_min = np.min(label_a_cropzero)\n",
        "    label_tensor_max = np.max(label_a_cropzero)\n",
        "\n",
        "    unique_masks, masks_counts = np.unique(label_a, return_counts=True)\n",
        "  else:\n",
        "    if(np.min(image_a_cropzero) < image_tensor_min):\n",
        "      image_tensor_min = np.min(image_a_cropzero)\n",
        "      image_tensor_min_dir = img_path\n",
        "    if(np.min(label_a_cropzero) < label_tensor_min):\n",
        "      label_tensor_min = np.min(label_a_cropzero)\n",
        "    if(np.max(image_a_cropzero) > image_tensor_max):\n",
        "      image_tensor_max = np.max(image_a_cropzero)\n",
        "    if(np.max(label_a_cropzero) > label_tensor_max):\n",
        "      label_tensor_max = np.max(label_a_cropzero)\n",
        "\n",
        "    current_masks, current_mask_counts = np.unique(label_a, return_counts=True)\n",
        "    if(len(current_masks > len(unique_masks))):\n",
        "      unique_masks = current_masks\n",
        "  #print(\"image res\", image_a_cropzero.shape)\n",
        "  \n",
        "  \n",
        "  row_list.append(image_a_cropzero.shape[0]) #add row value to list\n",
        "  #print(image_a_cropzero.shape[0])\n",
        "  col_list.append(image_a_cropzero.shape[1]) #add col value to list \n",
        "  \n",
        "\n",
        "#calculate max \n",
        "row_dim_max = max(row_list)\n",
        "col_dim_max = max(col_list)\n",
        "\n",
        "row_dim_max = ((row_dim_max + 15) // 16) * 16 #nearest multiple of 16\n",
        "col_dim_max = ((col_dim_max + 15) // 16) * 16 #nearest multiple of 16\n",
        "\n",
        "print(\"row max:\", max(row_list))\n",
        "print(\"col max:\", max(col_list))\n",
        "\n",
        "print(\"image tensor min\", image_tensor_min)\n",
        "print(\"image tensor max\", image_tensor_max)\n",
        "print(\"label tensor min\", label_tensor_min)\n",
        "print(\"label tensor max\", label_tensor_max)\n",
        "\n",
        "print(\"amount of masks\", unique_masks)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensor Dims cropping so that i don't waste 10 mins of my life every time i train on local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "#Local Dset not resampled\n",
        "row_dim_max = 928\n",
        "col_dim_max = 400\n",
        "\n",
        "image_tensor_min = -1000\n",
        "image_tensor_max = 3096\n",
        "label_tensor_min = 0.0\n",
        "label_tensor_max = 209.0\n",
        "'''\n",
        "\n",
        "#Local dset bspline resample\n",
        "row_dim_max = 464\n",
        "col_dim_max = 224\n",
        "\n",
        "image_tensor_min = -2230.0\n",
        "image_tensor_max = 4385.0\n",
        "label_tensor_min = 0.0\n",
        "label_tensor_max = 209.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVwddxJgnhFn"
      },
      "source": [
        "Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9X6rHdnnhFo",
        "outputId": "5e9fab66-6c87-4081-eaee-cd059d8a4445"
      },
      "outputs": [],
      "source": [
        "from transforms import tensor_transforms\n",
        "\n",
        "class SpiderDataset(Dataset):\n",
        "    def __init__(self, labels_dir, img_dir, transform=None, target_transform=None):\n",
        "        self.labels_dir = labels_dir\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.labels_dir))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label_dir_list = os.listdir(self.labels_dir)\n",
        "        image_dir_list = os.listdir(self.img_dir)\n",
        "\n",
        "        image_dir_list = natsorted(image_dir_list)\n",
        "        label_dir_list = natsorted(label_dir_list)\n",
        "\n",
        "        img_path = self.img_dir.joinpath(image_dir_list[idx])\n",
        "        label_path = self.labels_dir.joinpath(label_dir_list[idx])\n",
        "\n",
        "        image = Mri_Slice(img_path)\n",
        "        label = Mri_Slice(label_path)\n",
        "\n",
        "        image_a = image.hu_a\n",
        "        label_a = label.hu_a\n",
        "\n",
        "        #comment out the part not being used whether for 3d or 2d model \n",
        "    \n",
        "        '''\n",
        "        #3d tensor for 3D CNN\n",
        "        image_tensor = torch.from_numpy(image.hu_a)\n",
        "        label_tensor = torch.from_numpy(label.hu_a)\n",
        "        '''\n",
        "\n",
        "        image_tensor = torch.from_numpy(image_a)\n",
        "        label_tensor = torch.from_numpy(label_a)\n",
        "        \n",
        "        image_tensor = image_tensor.to(torch.float32)\n",
        "        label_tensor = label_tensor.to(torch.float32)\n",
        "        \n",
        "        #pad to max resolution of slice in dset \n",
        "        image_tensor = tensor_transforms.pad_to_resolution(image_tensor, [row_dim_max, col_dim_max])\n",
        "        label_tensor = tensor_transforms.pad_to_resolution(label_tensor, [row_dim_max, col_dim_max])\n",
        "\n",
        "        \n",
        "         #normalise\n",
        "        image_tensor = (image_tensor - image_tensor_min) / (image_tensor_max - image_tensor_min)\n",
        "        label_tensor = (label_tensor - label_tensor_min) / (label_tensor_max - label_tensor_min)\n",
        "\n",
        "        #comment out unsqueeze testing\n",
        "        \n",
        "        image_tensor = image_tensor.unsqueeze(0)\n",
        "        label_tensor = label_tensor.unsqueeze(0)\n",
        "        \n",
        "\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        label_tensor = label_tensor.to(device)\n",
        "\n",
        "        #print(image_tensor.shape)\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train dataset len 15662\n",
            "test dataset len 800\n"
          ]
        }
      ],
      "source": [
        "#toy train test dataset to test network running\n",
        "#local_train_set = SpiderDataset(local_img_idr, local_label_idr)\n",
        "dummy_train_set = SpiderDataset(dummy_train_label_slice_dir, dummy_train_img_slice_dir)\n",
        "\n",
        "dummy_test_set = SpiderDataset(dummy_test_label_slice_dir, dummy_test_img_slice_dir)\n",
        "\n",
        "print(\"train dataset len\",dummy_train_set.__len__())\n",
        "print(\"test dataset len\",dummy_test_set.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zsRlBNG-B6Q"
      },
      "source": [
        "Create Unet Instance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CT7aKn7S-F6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Github Repos\\Spider Seg e19005\\spider-seg-e19005\\models\\unet.py:213: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "  init.xavier_normal(m.weight)\n",
            "d:\\Github Repos\\Spider Seg e19005\\spider-seg-e19005\\models\\unet.py:214: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  init.constant(m.bias, 0)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (conv_final): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (down_convs): ModuleList(\n",
              "    (0): DownConv(\n",
              "      (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (1): DownConv(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (2): DownConv(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (3): DownConv(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (4): DownConv(\n",
              "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (up_convs): ModuleList(\n",
              "    (0): UpConv(\n",
              "      (upconv): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (1): UpConv(\n",
              "      (upconv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (2): UpConv(\n",
              "      (upconv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (3): UpConv(\n",
              "      (upconv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from models import unet \n",
        "\n",
        "input_channels = 1 #Hounsfield scale\n",
        "#output_channels = 3 #Vertebra, disc and spinal canal masks SHOULD BE 3 FOR 3 MASKS\n",
        "#output_channels = 18 #one for every mask\n",
        "output_channels = 1 #will try this so that torch doesn't complain about broadcasting \n",
        "model = unet.UNet(in_channels= input_channels,num_classes=output_channels)\n",
        "model.to(device)\n",
        "model.to(torch.float32)\n",
        "#for param in model.parameters():\n",
        " #   print(param.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Faster-RCNN Instance trained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\\nfrom torchvision.utils import draw_bounding_boxes\\n\\nweights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\\nmodel = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Mask RCNN Instance Trained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights\\n\\nmodel = maskrcnn_resnet50_fpn_v2(\\n    weights = MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,\\n    num_classes = 19\\n    #weights_backbone = \\n)\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights\n",
        "\n",
        "model = maskrcnn_resnet50_fpn_v2(\n",
        "    weights = MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,\n",
        "    num_classes = 19\n",
        "    #weights_backbone = \n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcuWKZAD9pHD"
      },
      "source": [
        "Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4lnCiKNW9sJN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n",
            "0.001\n",
            "8\n",
            "BCEWithLogitsLoss()\n"
          ]
        }
      ],
      "source": [
        "epochs = 30 #testing\n",
        "lr = 0.001 #testing\n",
        "batchsize = 8 #testing\n",
        "loss_func = nn.BCEWithLogitsLoss() #testing\n",
        "loss_func.to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "print(epochs)\n",
        "print(lr)\n",
        "print(batchsize)\n",
        "print(loss_func)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWO0s80znhFr"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UQL_nxpCnhFs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor batch in dummy_train_dataloader:\\n    for tensor in batch:\\n        print(\"min\", torch.min(tensor))\\n        print(\"max\", torch.max(tensor))\\n'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dummy_train_dataloader = DataLoader(dummy_train_set, batch_size = batchsize, shuffle=True)\n",
        "\n",
        "dummy_test_dataloader = DataLoader(dummy_test_set, batch_size = batchsize, shuffle=True)\n",
        "\n",
        "\n",
        "'''\n",
        "for batch in dummy_train_dataloader:\n",
        "    for tensor in batch:\n",
        "        print(\"min\", torch.min(tensor))\n",
        "        print(\"max\", torch.max(tensor))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensor Dimensions Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor idx_slice in dummy_train_dataloader:\\n    for tensor in idx_slice:\\n        print(tensor.shape)\\n        break\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "for idx_slice in dummy_train_dataloader:\n",
        "    for tensor in idx_slice:\n",
        "        print(tensor.shape)\n",
        "        break\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxAIK14tWJbp"
      },
      "source": [
        "One Epoch <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jxCZ41T_WOBF"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "\n",
        "    #swap train dataloader for dset\n",
        "    for i, data in enumerate(dummy_train_dataloader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        #print(\"labels\", labels.shape)\n",
        "        #print(inputs.shape)\n",
        "        #print(device)\n",
        "\n",
        "        #inputs = inputs.transpose(-1,0)\n",
        "        #labels = labels.transpose(-1,0)\n",
        "        #inputs = inputs.reshape(inputs.shape(1), inputs.shape(0), inputs.shape(3), inputs.shape(4))\n",
        "        #labels = labels.reshape(labels.shape(1), labels.shape(0), labels.shape(3), labels.shape(4))\n",
        "\n",
        "      \n",
        "        # Zero your gradients for every batch!\n",
        "        optim.zero_grad()\n",
        "\n",
        "        #2d\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        #print(\"outputs\", outputs.shape)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optim.step()\n",
        "\n",
        "        #3d tensor, probably won't use keeping here just in case  \n",
        "        '''\n",
        "        #run everything while indexing through z axis\n",
        "        for axis in images, masks:\n",
        "          for idx in range(0, axis.size(1)):\n",
        "            # Zero your gradients for every batch!\n",
        "            optim.zero_grad()\n",
        "            print(inputs[:, idx, : ,:])\n",
        "            # Make predictions for this batch\n",
        "            outputs = model(inputs[:, idx, : ,:])\n",
        "\n",
        "            # Compute the loss and its gradients\n",
        "            loss = loss_func(outputs[:, idx, : ,:], labels[:, idx, : ,:])\n",
        "            loss.backward()\n",
        "\n",
        "            # Adjust learning weights\n",
        "            optim.step()\n",
        "        '''\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        #if i % 1000 == 999:\n",
        "            #print(\"goes in\")\n",
        "        last_loss = running_loss / 1000 # loss per batch\n",
        "        print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "        tb_x = epoch_index * len(dummy_train_dataloader) + i + 1\n",
        "        tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "        running_loss = 0.\n",
        "        #if ends here\n",
        "\n",
        "    print(\"loss\", loss)\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyaaV1lYYiFc"
      },
      "source": [
        "Train Loop <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7i7TB_oOYhxQ",
        "outputId": "bfdaa28e-8cbc-4f46-be69-0979a6d3ceff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n",
            "  batch 1 loss: 0.000702250361442566\n",
            "  batch 2 loss: 0.0006139333248138428\n",
            "  batch 3 loss: 0.0003416965901851654\n",
            "  batch 4 loss: 0.026826019287109374\n",
            "  batch 5 loss: 0.0002107972353696823\n",
            "  batch 6 loss: 0.00046968397498130796\n",
            "  batch 7 loss: 0.0005092196464538574\n",
            "  batch 8 loss: 0.0005233551263809204\n",
            "  batch 9 loss: 0.0005351534485816956\n",
            "  batch 10 loss: 0.0005455968379974365\n",
            "  batch 11 loss: 0.0005472083687782287\n",
            "  batch 12 loss: 0.0005554996728897094\n",
            "  batch 13 loss: 0.0005451845526695252\n",
            "  batch 14 loss: 0.0005538294911384583\n",
            "  batch 15 loss: 0.0005435720086097717\n",
            "  batch 16 loss: 0.0005324057340621948\n",
            "  batch 17 loss: 0.000526832938194275\n",
            "  batch 18 loss: 0.0005087307095527648\n",
            "  batch 19 loss: 0.0004611001908779144\n",
            "  batch 20 loss: 0.00031449508666992187\n",
            "  batch 21 loss: 0.0002265590727329254\n",
            "  batch 22 loss: 0.0001913864016532898\n",
            "  batch 23 loss: 0.00014165286719799042\n",
            "  batch 24 loss: 0.0001590975672006607\n",
            "  batch 25 loss: 0.0001434231400489807\n",
            "  batch 26 loss: 0.00018263329565525055\n",
            "  batch 27 loss: 8.368495851755142e-05\n",
            "  batch 28 loss: 0.00011673998832702636\n",
            "  batch 29 loss: 0.00011786730587482452\n",
            "  batch 30 loss: 0.0001148550659418106\n",
            "  batch 31 loss: 0.0001223983019590378\n",
            "  batch 32 loss: 0.00010375560820102692\n",
            "  batch 33 loss: 8.021114766597748e-05\n",
            "  batch 34 loss: 0.00012963342666625976\n",
            "  batch 35 loss: 0.00013396209478378296\n",
            "  batch 36 loss: 0.00010941436886787415\n",
            "  batch 37 loss: 0.00016003277897834778\n",
            "  batch 38 loss: 0.00010777232050895691\n",
            "  batch 39 loss: 0.00016400685906410218\n",
            "  batch 40 loss: 0.00010879467427730561\n",
            "  batch 41 loss: 8.490487188100815e-05\n",
            "  batch 42 loss: 0.00013158755004405976\n",
            "  batch 43 loss: 0.0001639942079782486\n",
            "  batch 44 loss: 8.21434035897255e-05\n",
            "  batch 45 loss: 0.0001239546239376068\n",
            "  batch 46 loss: 0.00013322587311267854\n",
            "  batch 47 loss: 6.636079400777817e-05\n",
            "  batch 48 loss: 3.6146029829978944e-05\n",
            "  batch 49 loss: 0.0003101211786270142\n",
            "  batch 50 loss: 9.692112356424331e-05\n",
            "  batch 51 loss: 0.00011352433264255523\n",
            "  batch 52 loss: 0.0001303974837064743\n",
            "  batch 53 loss: 0.0001945762187242508\n",
            "  batch 54 loss: 0.00013852158188819885\n",
            "  batch 55 loss: 7.049499452114105e-05\n",
            "  batch 56 loss: 0.0001455395668745041\n",
            "  batch 57 loss: 5.331123247742653e-05\n",
            "  batch 58 loss: 0.00011642178893089295\n",
            "  batch 59 loss: 0.00020442597568035126\n",
            "  batch 60 loss: 0.00014457814395427704\n",
            "  batch 61 loss: 0.0001633431613445282\n",
            "  batch 62 loss: 0.00012696872651576997\n",
            "  batch 63 loss: 0.00010866043716669082\n",
            "  batch 64 loss: 7.257770746946335e-05\n",
            "  batch 65 loss: 0.00012242989987134933\n",
            "  batch 66 loss: 5.6339535862207414e-05\n",
            "  batch 67 loss: 2.1014325320720672e-05\n",
            "  batch 68 loss: 0.0001041245460510254\n",
            "  batch 69 loss: 0.00013804467022418977\n",
            "  batch 70 loss: 0.00011620795726776123\n",
            "  batch 71 loss: 8.867882192134858e-05\n",
            "  batch 72 loss: 0.0001145225688815117\n",
            "  batch 73 loss: 0.00013569670915603637\n",
            "  batch 74 loss: 0.00014540280401706694\n",
            "  batch 75 loss: 9.413497895002366e-05\n",
            "  batch 76 loss: 0.00011823953688144684\n",
            "  batch 77 loss: 0.00013608460128307343\n",
            "  batch 78 loss: 0.000107949860394001\n",
            "  batch 79 loss: 8.230684697628022e-05\n",
            "  batch 80 loss: 0.00010724299401044846\n",
            "  batch 81 loss: 0.00017602625489234925\n",
            "  batch 82 loss: 8.586572110652923e-05\n",
            "  batch 83 loss: 0.00010697722434997558\n",
            "  batch 84 loss: 4.958929121494293e-05\n",
            "  batch 85 loss: 9.114629030227661e-05\n",
            "  batch 86 loss: 0.00011021878570318222\n",
            "  batch 87 loss: 9.591183811426162e-05\n",
            "  batch 88 loss: 0.00011961373686790466\n",
            "  batch 89 loss: 0.00010293131321668625\n",
            "  batch 90 loss: 0.00015572716295719148\n",
            "  batch 91 loss: 9.907064586877823e-05\n",
            "  batch 92 loss: 6.589885801076889e-05\n",
            "  batch 93 loss: 0.00012954677641391753\n",
            "  batch 94 loss: 0.00010123554617166519\n",
            "  batch 95 loss: 8.03145244717598e-05\n",
            "  batch 96 loss: 9.657313674688339e-05\n",
            "  batch 97 loss: 8.412002027034759e-05\n",
            "  batch 98 loss: 6.634387373924255e-05\n",
            "  batch 99 loss: 8.010729402303695e-05\n",
            "  batch 100 loss: 0.00010517685860395432\n",
            "  batch 101 loss: 8.903118222951889e-05\n",
            "  batch 102 loss: 7.024124264717103e-05\n",
            "  batch 103 loss: 7.352997362613678e-05\n",
            "  batch 104 loss: 0.00016119517385959625\n",
            "  batch 105 loss: 9.447275847196579e-05\n",
            "  batch 106 loss: 0.00012134292721748352\n",
            "  batch 107 loss: 0.00010776145756244659\n",
            "  batch 108 loss: 0.00010782425105571746\n",
            "  batch 109 loss: 0.0001497538685798645\n",
            "  batch 110 loss: 0.00011524298042058945\n",
            "  batch 111 loss: 8.567187935113906e-05\n",
            "  batch 112 loss: 4.8193596303462985e-05\n",
            "  batch 113 loss: 0.0001269032508134842\n",
            "  batch 114 loss: 6.34753704071045e-05\n",
            "  batch 115 loss: 0.00010948940366506576\n",
            "  batch 116 loss: 0.00012821650505065918\n",
            "  batch 117 loss: 9.612304717302322e-05\n",
            "  batch 118 loss: 8.062302321195603e-05\n",
            "  batch 119 loss: 0.00016943028569221496\n",
            "  batch 120 loss: 0.00013632261753082276\n",
            "  batch 121 loss: 0.00011632488667964935\n",
            "  batch 122 loss: 7.221946865320206e-05\n",
            "  batch 123 loss: 6.486441940069199e-05\n",
            "  batch 124 loss: 0.00011188875138759613\n",
            "  batch 125 loss: 0.00012015575170516967\n",
            "  batch 126 loss: 0.00012168554961681366\n",
            "  batch 127 loss: 9.690448641777038e-05\n",
            "  batch 128 loss: 4.85304705798626e-05\n",
            "  batch 129 loss: 8.210264891386032e-05\n",
            "  batch 130 loss: 0.00014924092590808867\n",
            "  batch 131 loss: 9.962214529514313e-05\n",
            "  batch 132 loss: 7.334192842245102e-05\n",
            "  batch 133 loss: 0.00010165194422006607\n",
            "  batch 134 loss: 0.0001403963565826416\n",
            "  batch 135 loss: 0.00013380244374275207\n",
            "  batch 136 loss: 9.47960540652275e-05\n",
            "  batch 137 loss: 0.00011521357297897339\n",
            "  batch 138 loss: 0.00017853343486785888\n",
            "  batch 139 loss: 0.00012114881724119186\n",
            "  batch 140 loss: 8.455503731966019e-05\n",
            "  batch 141 loss: 0.00012897954881191254\n",
            "  batch 142 loss: 0.00013763634860515595\n",
            "  batch 143 loss: 7.70208016037941e-05\n",
            "  batch 144 loss: 8.233308792114257e-05\n",
            "  batch 145 loss: 6.415612995624542e-05\n",
            "  batch 146 loss: 5.354735255241394e-05\n",
            "  batch 147 loss: 9.076724201440811e-05\n",
            "  batch 148 loss: 0.00011115341633558273\n",
            "  batch 149 loss: 0.0001228819191455841\n",
            "  batch 150 loss: 7.890987396240234e-05\n",
            "  batch 151 loss: 7.482825219631195e-05\n",
            "  batch 152 loss: 7.76842013001442e-05\n",
            "  batch 153 loss: 7.966376096010209e-05\n",
            "  batch 154 loss: 0.00011013128608465195\n",
            "  batch 155 loss: 7.043191045522689e-05\n",
            "  batch 156 loss: 0.00012408405542373656\n",
            "  batch 157 loss: 0.00013877861201763153\n",
            "  batch 158 loss: 7.067080587148666e-05\n",
            "  batch 159 loss: 0.00012477613985538481\n",
            "  batch 160 loss: 0.00012904468178749084\n",
            "  batch 161 loss: 7.553784549236298e-05\n",
            "  batch 162 loss: 9.921883046627044e-05\n",
            "  batch 163 loss: 8.382884413003921e-05\n",
            "  batch 164 loss: 0.0001106458380818367\n",
            "  batch 165 loss: 8.688455820083618e-05\n",
            "  batch 166 loss: 7.807897031307221e-05\n",
            "  batch 167 loss: 8.616110682487488e-05\n",
            "  batch 168 loss: 0.00011077024042606354\n",
            "  batch 169 loss: 8.164599537849426e-05\n",
            "  batch 170 loss: 0.000141697958111763\n",
            "  batch 171 loss: 0.00010814225673675537\n",
            "  batch 172 loss: 0.00011550608277320862\n",
            "  batch 173 loss: 9.867680072784423e-05\n",
            "  batch 174 loss: 0.00010932012647390366\n",
            "  batch 175 loss: 0.00010129176825284958\n",
            "  batch 176 loss: 8.19096639752388e-05\n",
            "  batch 177 loss: 7.234984636306762e-05\n",
            "  batch 178 loss: 0.0001010463610291481\n",
            "  batch 179 loss: 5.808594822883606e-05\n",
            "  batch 180 loss: 0.00011066150665283204\n",
            "  batch 181 loss: 6.672881543636322e-05\n",
            "  batch 182 loss: 5.718843638896942e-05\n",
            "  batch 183 loss: 0.00015803270041942597\n",
            "  batch 184 loss: 8.763788640499116e-05\n",
            "  batch 185 loss: 6.094345077872276e-05\n",
            "  batch 186 loss: 7.282537966966629e-05\n",
            "  batch 187 loss: 0.00012618298828601836\n",
            "  batch 188 loss: 6.292955577373505e-05\n",
            "  batch 189 loss: 0.000170566126704216\n",
            "  batch 190 loss: 0.00011680340021848678\n",
            "  batch 191 loss: 4.1018620133399966e-05\n",
            "  batch 192 loss: 8.794843405485153e-05\n",
            "  batch 193 loss: 9.0008944272995e-05\n",
            "  batch 194 loss: 3.472260758280754e-05\n",
            "  batch 195 loss: 0.00010282721370458603\n",
            "  batch 196 loss: 5.803797394037247e-05\n",
            "  batch 197 loss: 0.00010052650421857834\n",
            "  batch 198 loss: 9.710892289876938e-05\n",
            "  batch 199 loss: 0.00012034747004508972\n",
            "  batch 200 loss: 6.25590980052948e-05\n",
            "  batch 201 loss: 7.727236300706863e-05\n",
            "  batch 202 loss: 9.107706695795059e-05\n",
            "  batch 203 loss: 9.986605495214462e-05\n",
            "  batch 204 loss: 0.00011201561242341995\n",
            "  batch 205 loss: 7.652512937784195e-05\n",
            "  batch 206 loss: 0.0001253506988286972\n",
            "  batch 207 loss: 9.463536739349366e-05\n",
            "  batch 208 loss: 8.563786000013352e-05\n",
            "  batch 209 loss: 0.00014324331283569337\n",
            "  batch 210 loss: 9.857438504695893e-05\n",
            "  batch 211 loss: 7.296472787857056e-05\n",
            "  batch 212 loss: 8.592835068702698e-05\n",
            "  batch 213 loss: 0.00010712289065122604\n",
            "  batch 214 loss: 9.553369879722596e-05\n",
            "  batch 215 loss: 0.00010264844447374344\n",
            "  batch 216 loss: 0.00010344437509775161\n",
            "  batch 217 loss: 0.0001188202053308487\n",
            "  batch 218 loss: 9.538063406944276e-05\n",
            "  batch 219 loss: 8.294889330863952e-05\n",
            "  batch 220 loss: 9.181080758571625e-05\n",
            "  batch 221 loss: 8.497878909111023e-05\n",
            "  batch 222 loss: 8.56567993760109e-05\n",
            "  batch 223 loss: 0.00013880820572376252\n",
            "  batch 224 loss: 5.872149392962456e-05\n",
            "  batch 225 loss: 6.548506766557693e-05\n",
            "  batch 226 loss: 0.00011676878482103348\n",
            "  batch 227 loss: 7.215929031372071e-05\n",
            "  batch 228 loss: 9.63384136557579e-05\n",
            "  batch 229 loss: 8.900232613086701e-05\n",
            "  batch 230 loss: 0.0001192547082901001\n",
            "  batch 231 loss: 0.00013039907813072204\n",
            "  batch 232 loss: 6.311619281768799e-05\n",
            "  batch 233 loss: 0.00011619216203689576\n",
            "  batch 234 loss: 4.653479531407356e-05\n",
            "  batch 235 loss: 8.080460876226425e-05\n",
            "  batch 236 loss: 8.17427858710289e-05\n",
            "  batch 237 loss: 0.00015799900889396668\n",
            "  batch 238 loss: 9.831313788890839e-05\n",
            "  batch 239 loss: 7.678037136793137e-05\n",
            "  batch 240 loss: 0.00010139790177345276\n",
            "  batch 241 loss: 9.800802916288377e-05\n",
            "  batch 242 loss: 7.587093859910965e-05\n",
            "  batch 243 loss: 0.00010828562825918198\n",
            "  batch 244 loss: 0.00011278226226568223\n",
            "  batch 245 loss: 9.974346309900284e-05\n",
            "  batch 246 loss: 5.965706333518028e-05\n",
            "  batch 247 loss: 7.446669787168503e-05\n",
            "  batch 248 loss: 9.356031566858291e-05\n",
            "  batch 249 loss: 5.500736832618713e-05\n",
            "  batch 250 loss: 8.848550915718079e-05\n",
            "  batch 251 loss: 9.206708520650864e-05\n",
            "  batch 252 loss: 0.0001054755300283432\n",
            "  batch 253 loss: 0.0002017066478729248\n",
            "  batch 254 loss: 0.00013631387054920197\n",
            "  batch 255 loss: 0.00016204577684402467\n",
            "  batch 256 loss: 9.582138061523437e-05\n",
            "  batch 257 loss: 6.886888295412063e-05\n",
            "  batch 258 loss: 7.94854760169983e-05\n",
            "  batch 259 loss: 0.00027385222911834717\n",
            "  batch 260 loss: 8.927708864212036e-05\n",
            "  batch 261 loss: 0.000158107191324234\n",
            "  batch 262 loss: 0.00015889962017536164\n",
            "  batch 263 loss: 0.0001519220620393753\n",
            "  batch 264 loss: 0.00012667880952358247\n",
            "  batch 265 loss: 8.956903219223023e-05\n",
            "  batch 266 loss: 0.00010167860984802246\n",
            "  batch 267 loss: 0.0001103098839521408\n",
            "  batch 268 loss: 0.00012939685583114623\n",
            "  batch 269 loss: 0.00010310519486665726\n",
            "  batch 270 loss: 0.00011165381222963333\n",
            "  batch 271 loss: 0.00011946386098861694\n",
            "  batch 272 loss: 8.238657563924789e-05\n",
            "  batch 273 loss: 9.691423177719116e-05\n",
            "  batch 274 loss: 0.00012577275931835175\n",
            "  batch 275 loss: 7.173055410385132e-05\n",
            "  batch 276 loss: 4.480006545782089e-05\n",
            "  batch 277 loss: 7.33289048075676e-05\n",
            "  batch 278 loss: 0.00015301114320755006\n",
            "  batch 279 loss: 0.00011652042716741562\n",
            "  batch 280 loss: 6.396620720624923e-05\n",
            "  batch 281 loss: 0.00016114477813243867\n",
            "  batch 282 loss: 9.663438796997071e-05\n",
            "  batch 283 loss: 0.00014471502602100372\n",
            "  batch 284 loss: 0.00012148395925760269\n",
            "  batch 285 loss: 8.293041586875916e-05\n",
            "  batch 286 loss: 0.00012098629772663116\n",
            "  batch 287 loss: 0.00012707586586475372\n",
            "  batch 288 loss: 7.94287547469139e-05\n",
            "  batch 289 loss: 0.00010955414175987244\n",
            "  batch 290 loss: 8.468569815158844e-05\n",
            "  batch 291 loss: 8.741084486246108e-05\n",
            "  batch 292 loss: 0.00017668159306049347\n",
            "  batch 293 loss: 8.732275664806366e-05\n",
            "  batch 294 loss: 6.940971314907073e-05\n",
            "  batch 295 loss: 9.419729560613632e-05\n",
            "  batch 296 loss: 5.947953462600708e-05\n",
            "  batch 297 loss: 0.00012405402958393097\n",
            "  batch 298 loss: 6.39999881386757e-05\n",
            "  batch 299 loss: 0.00014294406771659852\n",
            "  batch 300 loss: 0.0001249166652560234\n",
            "  batch 301 loss: 0.00010821915417909622\n",
            "  batch 302 loss: 7.973066717386246e-05\n",
            "  batch 303 loss: 0.00016382139921188355\n",
            "  batch 304 loss: 8.771601319313049e-05\n",
            "  batch 305 loss: 0.00015094517171382904\n",
            "  batch 306 loss: 9.188321232795715e-05\n",
            "  batch 307 loss: 0.00010548246651887894\n",
            "  batch 308 loss: 0.0001140267327427864\n",
            "  batch 309 loss: 9.84557345509529e-05\n",
            "  batch 310 loss: 0.00010549776256084442\n",
            "  batch 311 loss: 7.79496431350708e-05\n",
            "  batch 312 loss: 0.00016929370164871216\n",
            "  batch 313 loss: 0.00014200599491596223\n",
            "  batch 314 loss: 9.677235782146454e-05\n",
            "  batch 315 loss: 0.00011901477724313736\n",
            "  batch 316 loss: 0.00010261309146881103\n",
            "  batch 317 loss: 8.749747276306152e-05\n",
            "  batch 318 loss: 0.00010640972852706909\n",
            "  batch 319 loss: 0.0001426057070493698\n",
            "  batch 320 loss: 0.00010219359397888183\n",
            "  batch 321 loss: 8.134794980287552e-05\n",
            "  batch 322 loss: 5.702666565775871e-05\n",
            "  batch 323 loss: 0.00014126940071582794\n",
            "  batch 324 loss: 0.00011084106564521789\n",
            "  batch 325 loss: 5.079924687743187e-05\n",
            "  batch 326 loss: 6.676536798477173e-05\n",
            "  batch 327 loss: 0.00010745872557163239\n",
            "  batch 328 loss: 0.00014910563826560973\n",
            "  batch 329 loss: 9.340496361255646e-05\n",
            "  batch 330 loss: 5.3545180708169934e-05\n",
            "  batch 331 loss: 8.23977068066597e-05\n",
            "  batch 332 loss: 7.358764111995697e-05\n",
            "  batch 333 loss: 0.00012263776361942291\n",
            "  batch 334 loss: 6.158964708447457e-05\n",
            "  batch 335 loss: 0.00011716970056295395\n",
            "  batch 336 loss: 8.598605543375015e-05\n",
            "  batch 337 loss: 0.00013141171634197235\n",
            "  batch 338 loss: 0.0001306196302175522\n",
            "  batch 339 loss: 9.561057388782501e-05\n",
            "  batch 340 loss: 0.00012260508537292482\n",
            "  batch 341 loss: 6.300666183233262e-05\n",
            "  batch 342 loss: 6.493298709392547e-05\n",
            "  batch 343 loss: 8.240258693695069e-05\n",
            "  batch 344 loss: 7.375978678464889e-05\n",
            "  batch 345 loss: 6.302332878112793e-05\n",
            "  batch 346 loss: 0.00010372637212276458\n",
            "  batch 347 loss: 0.0001406906247138977\n",
            "  batch 348 loss: 0.00021730783581733702\n",
            "  batch 349 loss: 9.53378826379776e-05\n",
            "  batch 350 loss: 0.00013368920981884002\n",
            "  batch 351 loss: 0.00015280506014823914\n",
            "  batch 352 loss: 9.385272860527039e-05\n",
            "  batch 353 loss: 6.690257042646408e-05\n",
            "  batch 354 loss: 5.63507117331028e-05\n",
            "  batch 355 loss: 4.4209711253643035e-05\n",
            "  batch 356 loss: 0.000143166184425354\n",
            "  batch 357 loss: 0.00014026442170143129\n",
            "  batch 358 loss: 7.742025703191757e-05\n",
            "  batch 359 loss: 8.513163775205612e-05\n",
            "  batch 360 loss: 0.00010603409260511399\n",
            "  batch 361 loss: 7.333651185035705e-05\n",
            "  batch 362 loss: 9.361361712217331e-05\n",
            "  batch 363 loss: 0.00016029109060764312\n",
            "  batch 364 loss: 0.00011215934157371522\n",
            "  batch 365 loss: 9.490333497524261e-05\n",
            "  batch 366 loss: 0.00011047720164060592\n",
            "  batch 367 loss: 0.0001884608268737793\n",
            "  batch 368 loss: 4.0301088243722916e-05\n",
            "  batch 369 loss: 0.00012477406859397888\n",
            "  batch 370 loss: 0.0001044379323720932\n",
            "  batch 371 loss: 0.00014746180176734925\n",
            "  batch 372 loss: 0.00010821561515331268\n",
            "  batch 373 loss: 9.664779901504517e-05\n",
            "  batch 374 loss: 0.0001023009642958641\n",
            "  batch 375 loss: 7.78815969824791e-05\n",
            "  batch 376 loss: 0.0001290079802274704\n",
            "  batch 377 loss: 0.00010643725097179413\n",
            "  batch 378 loss: 0.00013715261220932008\n",
            "  batch 379 loss: 7.82342180609703e-05\n",
            "  batch 380 loss: 0.00010826046019792557\n",
            "  batch 381 loss: 8.415834605693817e-05\n",
            "  batch 382 loss: 9.532947093248368e-05\n",
            "  batch 383 loss: 8.35781991481781e-05\n",
            "  batch 384 loss: 0.00011105504631996155\n",
            "  batch 385 loss: 0.00012616412341594696\n",
            "  batch 386 loss: 0.00012816324830055236\n",
            "  batch 387 loss: 6.12763911485672e-05\n",
            "  batch 388 loss: 5.008143186569214e-05\n",
            "  batch 389 loss: 0.00011506140232086182\n",
            "  batch 390 loss: 0.00010596426576375961\n",
            "  batch 391 loss: 5.055313929915428e-05\n",
            "  batch 392 loss: 9.558682888746262e-05\n",
            "  batch 393 loss: 9.446968883275985e-05\n",
            "  batch 394 loss: 0.00010538361221551895\n",
            "  batch 395 loss: 0.00012300775200128555\n",
            "  batch 396 loss: 0.00011184254288673401\n",
            "  batch 397 loss: 9.488360583782196e-05\n",
            "  batch 398 loss: 0.00010003232210874558\n",
            "  batch 399 loss: 0.00012234656512737275\n",
            "  batch 400 loss: 9.32888314127922e-05\n",
            "  batch 401 loss: 0.00014025744795799255\n",
            "  batch 402 loss: 0.0001229870468378067\n",
            "  batch 403 loss: 0.0001424777954816818\n",
            "  batch 404 loss: 0.00010812655836343765\n",
            "  batch 405 loss: 0.00010927212983369827\n",
            "  batch 406 loss: 0.00015678772330284118\n",
            "  batch 407 loss: 0.0001015300452709198\n",
            "  batch 408 loss: 0.0001494433581829071\n",
            "  batch 409 loss: 0.00014112032949924468\n",
            "  batch 410 loss: 9.449297934770584e-05\n",
            "  batch 411 loss: 7.667816430330276e-05\n",
            "  batch 412 loss: 6.29977136850357e-05\n",
            "  batch 413 loss: 0.0001383802592754364\n",
            "  batch 414 loss: 3.960290923714638e-05\n",
            "  batch 415 loss: 0.00014003151655197143\n",
            "  batch 416 loss: 0.0001355561912059784\n",
            "  batch 417 loss: 7.35401138663292e-05\n",
            "  batch 418 loss: 9.576974064111709e-05\n",
            "  batch 419 loss: 9.45325493812561e-05\n",
            "  batch 420 loss: 0.00012951278686523438\n",
            "  batch 421 loss: 9.288240969181061e-05\n",
            "  batch 422 loss: 0.00013209740817546843\n",
            "  batch 423 loss: 0.00013323290646076203\n",
            "  batch 424 loss: 7.516408711671829e-05\n",
            "  batch 425 loss: 0.00010886847227811813\n",
            "  batch 426 loss: 6.652108579874038e-05\n",
            "  batch 427 loss: 8.900567889213562e-05\n",
            "  batch 428 loss: 0.00013986265659332275\n",
            "  batch 429 loss: 5.7674314826726914e-05\n",
            "  batch 430 loss: 9.046502411365509e-05\n",
            "  batch 431 loss: 3.5098973661661145e-05\n",
            "  batch 432 loss: 9.255801141262054e-05\n",
            "  batch 433 loss: 0.00010279667377471924\n",
            "  batch 434 loss: 8.197097480297089e-05\n",
            "  batch 435 loss: 0.00012995849549770356\n",
            "  batch 436 loss: 0.00010196488350629807\n",
            "  batch 437 loss: 0.00010843151062726975\n",
            "  batch 438 loss: 0.00010782617330551147\n",
            "  batch 439 loss: 0.00011584819108247757\n",
            "  batch 440 loss: 0.00010051272064447403\n",
            "  batch 441 loss: 9.236258268356323e-05\n",
            "  batch 442 loss: 6.407056748867036e-05\n",
            "  batch 443 loss: 9.886127710342407e-05\n",
            "  batch 444 loss: 6.042677536606789e-05\n",
            "  batch 445 loss: 3.653111308813095e-05\n",
            "  batch 446 loss: 9.345205873250962e-05\n",
            "  batch 447 loss: 9.296705573797225e-05\n",
            "  batch 448 loss: 6.425575911998749e-05\n",
            "  batch 449 loss: 0.00012137776613235474\n",
            "  batch 450 loss: 4.9517616629600524e-05\n",
            "  batch 451 loss: 9.597189724445342e-05\n",
            "  batch 452 loss: 0.00014782825112342833\n",
            "  batch 453 loss: 0.00012059259414672852\n",
            "  batch 454 loss: 8.182188123464584e-05\n",
            "  batch 455 loss: 7.273474335670471e-05\n",
            "  batch 456 loss: 4.026927798986435e-05\n",
            "  batch 457 loss: 0.0001704758256673813\n",
            "  batch 458 loss: 0.00011323934048414231\n",
            "  batch 459 loss: 5.016291886568069e-05\n",
            "  batch 460 loss: 0.0001743282377719879\n",
            "  batch 461 loss: 0.00012063398957252502\n",
            "  batch 462 loss: 0.00024140127003192903\n",
            "  batch 463 loss: 0.00011009864509105683\n",
            "  batch 464 loss: 0.0001513725221157074\n",
            "  batch 465 loss: 0.00013146753609180451\n",
            "  batch 466 loss: 0.00012246394157409667\n",
            "  batch 467 loss: 8.361400663852692e-05\n",
            "  batch 468 loss: 0.00014982230961322785\n",
            "  batch 469 loss: 7.826559990644455e-05\n",
            "  batch 470 loss: 0.00014978061616420746\n",
            "  batch 471 loss: 6.42441138625145e-05\n",
            "  batch 472 loss: 0.0001452028453350067\n",
            "  batch 473 loss: 0.00010198236256837845\n",
            "  batch 474 loss: 4.5251503586769105e-05\n",
            "  batch 475 loss: 0.00015108542144298554\n",
            "  batch 476 loss: 0.00010878479480743408\n",
            "  batch 477 loss: 9.833991527557372e-05\n",
            "  batch 478 loss: 0.00010092533379793166\n",
            "  batch 479 loss: 0.00010711140185594559\n",
            "  batch 480 loss: 4.673515260219574e-05\n",
            "  batch 481 loss: 9.000666439533233e-05\n",
            "  batch 482 loss: 6.424541771411896e-05\n",
            "  batch 483 loss: 9.174389392137527e-05\n",
            "  batch 484 loss: 8.049561083316803e-05\n",
            "  batch 485 loss: 4.2441554367542264e-05\n",
            "  batch 486 loss: 0.00010280653834342956\n",
            "  batch 487 loss: 8.280979096889495e-05\n",
            "  batch 488 loss: 0.00010258898884057999\n",
            "  batch 489 loss: 9.342010319232941e-05\n",
            "  batch 490 loss: 9.934542328119278e-05\n",
            "  batch 491 loss: 7.13111236691475e-05\n",
            "  batch 492 loss: 0.00014765508472919464\n",
            "  batch 493 loss: 6.643047928810119e-05\n",
            "  batch 494 loss: 5.891429632902145e-05\n",
            "  batch 495 loss: 9.892892837524414e-05\n",
            "  batch 496 loss: 0.00016032911837100981\n",
            "  batch 497 loss: 0.00014496049284934998\n",
            "  batch 498 loss: 8.728072047233582e-05\n",
            "  batch 499 loss: 0.00013911418616771698\n",
            "  batch 500 loss: 0.0001253928542137146\n",
            "  batch 501 loss: 0.00017519330978393554\n",
            "  batch 502 loss: 8.641520142555236e-05\n",
            "  batch 503 loss: 0.0001624775379896164\n",
            "  batch 504 loss: 0.000104749895632267\n",
            "  batch 505 loss: 0.00016321234405040742\n",
            "  batch 506 loss: 7.658170163631439e-05\n",
            "  batch 507 loss: 9.541551768779754e-05\n",
            "  batch 508 loss: 0.00012151028960943221\n",
            "  batch 509 loss: 9.379751980304719e-05\n",
            "  batch 510 loss: 0.00013524091243743896\n",
            "  batch 511 loss: 0.00014373274147510529\n",
            "  batch 512 loss: 8.589283376932144e-05\n",
            "  batch 513 loss: 0.00012106648087501526\n",
            "  batch 514 loss: 8.851679414510726e-05\n",
            "  batch 515 loss: 0.0001053057461977005\n",
            "  batch 516 loss: 0.0001068103164434433\n",
            "  batch 517 loss: 7.943326234817505e-05\n",
            "  batch 518 loss: 8.192793279886246e-05\n",
            "  batch 519 loss: 0.0001243549957871437\n",
            "  batch 520 loss: 0.00011442738771438598\n",
            "  batch 521 loss: 0.0001001361683011055\n",
            "  batch 522 loss: 9.326878190040589e-05\n",
            "  batch 523 loss: 0.00010936202853918076\n",
            "  batch 524 loss: 0.00010658924281597137\n",
            "  batch 525 loss: 0.00012942029535770416\n",
            "  batch 526 loss: 7.345668226480484e-05\n",
            "  batch 527 loss: 4.4518109411001204e-05\n",
            "  batch 528 loss: 0.00012589046359062195\n",
            "  batch 529 loss: 6.428055465221405e-05\n",
            "  batch 530 loss: 0.00014511542022228242\n",
            "  batch 531 loss: 0.0001240207478404045\n",
            "  batch 532 loss: 8.982518315315246e-05\n",
            "  batch 533 loss: 7.28602260351181e-05\n",
            "  batch 534 loss: 0.00011698965728282928\n",
            "  batch 535 loss: 0.0001238023340702057\n",
            "  batch 536 loss: 5.507762357592583e-05\n",
            "  batch 537 loss: 8.616594970226288e-05\n",
            "  batch 538 loss: 0.00010950031131505966\n",
            "  batch 539 loss: 8.539122343063354e-05\n",
            "  batch 540 loss: 6.420716643333435e-05\n",
            "  batch 541 loss: 0.00010421390831470489\n",
            "  batch 542 loss: 0.00017994038760662078\n",
            "  batch 543 loss: 0.0001225811019539833\n",
            "  batch 544 loss: 0.00010500538349151612\n",
            "  batch 545 loss: 9.980282932519912e-05\n",
            "  batch 546 loss: 9.794674813747406e-05\n",
            "  batch 547 loss: 0.00013146498799324036\n",
            "  batch 548 loss: 0.00011877070367336273\n",
            "  batch 549 loss: 9.964513033628464e-05\n",
            "  batch 550 loss: 9.845298528671265e-05\n",
            "  batch 551 loss: 0.00010135652124881745\n",
            "  batch 552 loss: 9.529802203178405e-05\n",
            "  batch 553 loss: 0.00020285162329673767\n",
            "  batch 554 loss: 0.021200626373291016\n",
            "  batch 555 loss: 5.266604945063591e-05\n",
            "  batch 556 loss: 0.00018985089659690857\n",
            "  batch 557 loss: 0.00017608845233917236\n",
            "  batch 558 loss: 0.00014960755407810212\n",
            "  batch 559 loss: 6.663164496421813e-05\n",
            "  batch 560 loss: 0.00018003870546817779\n",
            "  batch 561 loss: 0.0002283596843481064\n",
            "  batch 562 loss: 0.00026078453660011294\n",
            "  batch 563 loss: 0.0001365799754858017\n",
            "  batch 564 loss: 0.0001274915933609009\n",
            "  batch 565 loss: 7.220277190208435e-05\n",
            "  batch 566 loss: 9.826229512691497e-05\n",
            "  batch 567 loss: 0.0002623239159584045\n",
            "  batch 568 loss: 0.00012035002559423447\n",
            "  batch 569 loss: 0.0001529102921485901\n",
            "  batch 570 loss: 0.00017470917105674743\n",
            "  batch 571 loss: 0.00021463409066200257\n",
            "  batch 572 loss: 0.0001778242588043213\n",
            "  batch 573 loss: 0.00018627490103244782\n",
            "  batch 574 loss: 0.00010530836135149002\n",
            "  batch 575 loss: 0.00017701168358325958\n",
            "  batch 576 loss: 5.316386744379997e-05\n",
            "  batch 577 loss: 8.686134964227677e-05\n",
            "  batch 578 loss: 0.00016707494854927062\n",
            "  batch 579 loss: 0.0001971776932477951\n",
            "  batch 580 loss: 0.0003027646541595459\n",
            "  batch 581 loss: 0.0002716791331768036\n",
            "  batch 582 loss: 0.0013005589246749879\n",
            "  batch 583 loss: 0.00010243778675794602\n",
            "  batch 584 loss: 0.00010687556117773056\n",
            "  batch 585 loss: 0.0001260925978422165\n",
            "  batch 586 loss: 0.00013727252185344695\n",
            "  batch 587 loss: 6.161436811089516e-05\n",
            "  batch 588 loss: 0.0001134004220366478\n",
            "  batch 589 loss: 0.00012683252990245818\n",
            "  batch 590 loss: 0.0003164255619049072\n",
            "  batch 591 loss: 9.936729073524476e-05\n",
            "  batch 592 loss: 8.05501714348793e-05\n",
            "  batch 593 loss: 0.00010026220977306366\n",
            "  batch 594 loss: 0.00011599787324666977\n",
            "  batch 595 loss: 0.00011553528159856796\n",
            "  batch 596 loss: 0.00010382819920778274\n",
            "  batch 597 loss: 0.00012991102039813996\n",
            "  batch 598 loss: 0.00016077862679958344\n",
            "  batch 599 loss: 0.0001680298149585724\n",
            "  batch 600 loss: 0.000126302570104599\n",
            "  batch 601 loss: 9.202328324317933e-05\n",
            "  batch 602 loss: 8.320382982492447e-05\n",
            "  batch 603 loss: 0.00010491909086704254\n",
            "  batch 604 loss: 0.0001582655906677246\n",
            "  batch 605 loss: 0.00011259674280881881\n",
            "  batch 606 loss: 8.89037549495697e-05\n",
            "  batch 607 loss: 0.0001349775344133377\n",
            "  batch 608 loss: 0.00012751401960849762\n",
            "  batch 609 loss: 0.00010465744137763977\n",
            "  batch 610 loss: 0.00018062880635261535\n",
            "  batch 611 loss: 0.0001604817360639572\n",
            "  batch 612 loss: 0.0001053103506565094\n",
            "  batch 613 loss: 6.915933638811111e-05\n",
            "  batch 614 loss: 0.0001719146817922592\n",
            "  batch 615 loss: 0.0001075311303138733\n",
            "  batch 616 loss: 0.00015513792634010314\n",
            "  batch 617 loss: 0.0001244286596775055\n",
            "  batch 618 loss: 6.56745433807373e-05\n",
            "  batch 619 loss: 0.00012700895965099334\n",
            "  batch 620 loss: 0.00010963817685842515\n",
            "  batch 621 loss: 0.00013281245529651642\n",
            "  batch 622 loss: 8.708412200212479e-05\n",
            "  batch 623 loss: 4.1420761495828627e-05\n",
            "  batch 624 loss: 0.00012365476042032242\n",
            "  batch 625 loss: 0.00011046650260686874\n",
            "  batch 626 loss: 0.0001215604916214943\n",
            "  batch 627 loss: 0.00011414247006177902\n",
            "  batch 628 loss: 0.0001149030476808548\n",
            "  batch 629 loss: 6.98293149471283e-05\n",
            "  batch 630 loss: 8.375412225723267e-05\n",
            "  batch 631 loss: 7.387963682413101e-05\n",
            "  batch 632 loss: 7.254025340080261e-05\n",
            "  batch 633 loss: 7.624706625938416e-05\n",
            "  batch 634 loss: 0.00016402213275432587\n",
            "  batch 635 loss: 0.0001696547269821167\n",
            "  batch 636 loss: 0.00013873131573200227\n",
            "  batch 637 loss: 6.092048808932304e-05\n",
            "  batch 638 loss: 8.266803622245788e-05\n",
            "  batch 639 loss: 0.0001320122629404068\n",
            "  batch 640 loss: 0.00010812018811702729\n",
            "  batch 641 loss: 0.00011250830441713333\n",
            "  batch 642 loss: 0.0001240019276738167\n",
            "  batch 643 loss: 0.00016064204275608062\n",
            "  batch 644 loss: 7.256955653429031e-05\n",
            "  batch 645 loss: 9.881595522165298e-05\n",
            "  batch 646 loss: 0.0001364886313676834\n",
            "  batch 647 loss: 0.00010100620985031128\n",
            "  batch 648 loss: 0.00010214579105377197\n",
            "  batch 649 loss: 9.744583070278168e-05\n",
            "  batch 650 loss: 0.00011918940395116806\n",
            "  batch 651 loss: 0.0001238171309232712\n",
            "  batch 652 loss: 0.0002025446742773056\n",
            "  batch 653 loss: 9.109709411859512e-05\n",
            "  batch 654 loss: 0.00012214644253253937\n",
            "  batch 655 loss: 8.235804736614227e-05\n",
            "  batch 656 loss: 7.438621670007706e-05\n",
            "  batch 657 loss: 6.664766371250153e-05\n",
            "  batch 658 loss: 0.00013870845735073088\n",
            "  batch 659 loss: 0.00010884494334459305\n",
            "  batch 660 loss: 0.0001076071560382843\n",
            "  batch 661 loss: 0.00015703870356082917\n",
            "  batch 662 loss: 7.538510859012604e-05\n",
            "  batch 663 loss: 5.916556343436241e-05\n",
            "  batch 664 loss: 0.00011167457699775696\n",
            "  batch 665 loss: 0.00013667191565036775\n",
            "  batch 666 loss: 0.0001006530374288559\n",
            "  batch 667 loss: 8.425112068653107e-05\n",
            "  batch 668 loss: 0.0001275726705789566\n",
            "  batch 669 loss: 0.00012232820689678193\n",
            "  batch 670 loss: 0.00013136518001556397\n",
            "  batch 671 loss: 0.00014074154198169707\n",
            "  batch 672 loss: 0.00015329809486865997\n",
            "  batch 673 loss: 9.289717674255371e-05\n",
            "  batch 674 loss: 0.00013424219191074372\n",
            "  batch 675 loss: 0.00010876742005348205\n",
            "  batch 676 loss: 0.00010759209096431732\n",
            "  batch 677 loss: 0.00011431025713682174\n",
            "  batch 678 loss: 0.00011220734566450119\n",
            "  batch 679 loss: 0.00013305865228176118\n",
            "  batch 680 loss: 0.00015567329525947572\n",
            "  batch 681 loss: 7.815396785736083e-05\n",
            "  batch 682 loss: 8.343198150396347e-05\n",
            "  batch 683 loss: 0.00012925347685813904\n",
            "  batch 684 loss: 9.833330661058426e-05\n",
            "  batch 685 loss: 0.00014741568267345428\n",
            "  batch 686 loss: 0.00013160455226898194\n",
            "  batch 687 loss: 0.0001340070366859436\n",
            "  batch 688 loss: 8.753000944852829e-05\n",
            "  batch 689 loss: 9.794650226831436e-05\n",
            "  batch 690 loss: 8.431621640920639e-05\n",
            "  batch 691 loss: 0.00015281982719898225\n",
            "  batch 692 loss: 8.177383244037628e-05\n",
            "  batch 693 loss: 0.00010504841804504395\n",
            "  batch 694 loss: 0.00020224764943122865\n",
            "  batch 695 loss: 8.336372673511505e-05\n",
            "  batch 696 loss: 0.0001323123574256897\n",
            "  batch 697 loss: 8.643820136785508e-05\n",
            "  batch 698 loss: 5.843357741832733e-05\n",
            "  batch 699 loss: 7.775094360113144e-05\n",
            "  batch 700 loss: 0.00012681743502616883\n",
            "  batch 701 loss: 0.00015777504444122313\n",
            "  batch 702 loss: 7.957132160663604e-05\n",
            "  batch 703 loss: 0.00013018304109573364\n",
            "  batch 704 loss: 9.216064214706421e-05\n",
            "  batch 705 loss: 0.00010326983779668808\n",
            "  batch 706 loss: 7.70239382982254e-05\n",
            "  batch 707 loss: 9.49249267578125e-05\n",
            "  batch 708 loss: 0.00011111313849687576\n",
            "  batch 709 loss: 7.555052638053894e-05\n",
            "  batch 710 loss: 9.940127283334732e-05\n",
            "  batch 711 loss: 8.729042112827301e-05\n",
            "  batch 712 loss: 9.46471318602562e-05\n",
            "  batch 713 loss: 0.00010711029171943665\n",
            "  batch 714 loss: 6.825999170541763e-05\n",
            "  batch 715 loss: 9.624414145946503e-05\n",
            "  batch 716 loss: 0.00012965330481529236\n",
            "  batch 717 loss: 8.154091984033584e-05\n",
            "  batch 718 loss: 8.009938895702362e-05\n",
            "  batch 719 loss: 9.968692809343338e-05\n",
            "  batch 720 loss: 0.00013239361345767975\n",
            "  batch 721 loss: 0.00014886245131492614\n",
            "  batch 722 loss: 7.44187980890274e-05\n",
            "  batch 723 loss: 9.914741665124893e-05\n",
            "  batch 724 loss: 9.09082368016243e-05\n",
            "  batch 725 loss: 9.752357751131058e-05\n",
            "  batch 726 loss: 8.524658530950546e-05\n",
            "  batch 727 loss: 0.00013563323020935057\n",
            "  batch 728 loss: 0.0001384240984916687\n",
            "  batch 729 loss: 8.484230190515518e-05\n",
            "  batch 730 loss: 0.00013536052405834197\n",
            "  batch 731 loss: 0.00011427812278270721\n",
            "  batch 732 loss: 0.00012408499419689178\n",
            "  batch 733 loss: 0.00010706017166376114\n",
            "  batch 734 loss: 9.373709559440613e-05\n",
            "  batch 735 loss: 0.00012538860738277435\n",
            "  batch 736 loss: 0.00011736554652452469\n",
            "  batch 737 loss: 8.637734502553939e-05\n",
            "  batch 738 loss: 0.00013361527025699616\n",
            "  batch 739 loss: 6.912823021411896e-05\n",
            "  batch 740 loss: 0.00010817395895719528\n",
            "  batch 741 loss: 0.0001905243545770645\n",
            "  batch 742 loss: 0.00013180670142173766\n",
            "  batch 743 loss: 8.405552059412003e-05\n",
            "  batch 744 loss: 8.68108943104744e-05\n",
            "  batch 745 loss: 0.0001459675282239914\n",
            "  batch 746 loss: 9.813744574785232e-05\n",
            "  batch 747 loss: 0.00012903799116611482\n",
            "  batch 748 loss: 7.926462590694428e-05\n",
            "  batch 749 loss: 0.00012786658108234405\n",
            "  batch 750 loss: 5.801187083125114e-05\n",
            "  batch 751 loss: 0.00012300295382738113\n",
            "  batch 752 loss: 0.00010445498675107955\n",
            "  batch 753 loss: 0.00014550110697746278\n",
            "  batch 754 loss: 0.00011852748692035675\n",
            "  batch 755 loss: 6.428210437297822e-05\n",
            "  batch 756 loss: 0.00016318483650684356\n",
            "  batch 757 loss: 0.00016465064883232117\n",
            "  batch 758 loss: 0.0001268174797296524\n",
            "  batch 759 loss: 6.287171691656113e-05\n",
            "  batch 760 loss: 8.163870126008988e-05\n",
            "  batch 761 loss: 0.00010227956622838974\n",
            "  batch 762 loss: 6.755270063877106e-05\n",
            "  batch 763 loss: 0.00015097030997276306\n",
            "  batch 764 loss: 8.702674508094787e-05\n",
            "  batch 765 loss: 8.178818225860596e-05\n",
            "  batch 766 loss: 5.3119339048862456e-05\n",
            "  batch 767 loss: 7.417277246713638e-05\n",
            "  batch 768 loss: 0.0001712220013141632\n",
            "  batch 769 loss: 9.617949277162552e-05\n",
            "  batch 770 loss: 9.279964119195938e-05\n",
            "  batch 771 loss: 9.511821717023849e-05\n",
            "  batch 772 loss: 0.00014749738574028015\n",
            "  batch 773 loss: 0.00010009895265102387\n",
            "  batch 774 loss: 9.540703147649765e-05\n",
            "  batch 775 loss: 0.0001157568022608757\n",
            "  batch 776 loss: 0.0001893802285194397\n",
            "  batch 777 loss: 8.914674818515778e-05\n",
            "  batch 778 loss: 0.00011272191256284714\n",
            "  batch 779 loss: 8.613988757133484e-05\n",
            "  batch 780 loss: 0.00015100380778312682\n",
            "  batch 781 loss: 0.00011034543812274933\n",
            "  batch 782 loss: 0.00012656714022159576\n",
            "  batch 783 loss: 0.00013479189574718475\n",
            "  batch 784 loss: 7.264817506074906e-05\n",
            "  batch 785 loss: 0.00014009496569633483\n",
            "  batch 786 loss: 0.00010073255747556686\n",
            "  batch 787 loss: 5.081032961606979e-05\n",
            "  batch 788 loss: 7.062938064336777e-05\n",
            "  batch 789 loss: 6.363607197999954e-05\n",
            "  batch 790 loss: 7.654007524251938e-05\n",
            "  batch 791 loss: 0.00014600014686584473\n",
            "  batch 792 loss: 8.881700038909913e-05\n",
            "  batch 793 loss: 0.00010315734893083572\n",
            "  batch 794 loss: 0.00014424011111259462\n",
            "  batch 795 loss: 6.660670042037963e-05\n",
            "  batch 796 loss: 8.621615916490554e-05\n",
            "  batch 797 loss: 5.104264989495277e-05\n",
            "  batch 798 loss: 0.00012889775633811952\n",
            "  batch 799 loss: 7.71515890955925e-05\n",
            "  batch 800 loss: 8.418604731559754e-05\n",
            "  batch 801 loss: 6.998517364263535e-05\n",
            "  batch 802 loss: 0.00010453418642282486\n",
            "  batch 803 loss: 0.00012185709923505783\n",
            "  batch 804 loss: 5.889569222927093e-05\n",
            "  batch 805 loss: 6.666986644268036e-05\n",
            "  batch 806 loss: 0.00012303272634744643\n",
            "  batch 807 loss: 0.00013558304309844971\n",
            "  batch 808 loss: 0.00011927685141563415\n",
            "  batch 809 loss: 0.0001433706432580948\n",
            "  batch 810 loss: 0.00011590709537267685\n",
            "  batch 811 loss: 0.0001465287208557129\n",
            "  batch 812 loss: 0.00011926852911710738\n",
            "  batch 813 loss: 0.00010958316177129746\n",
            "  batch 814 loss: 8.409774303436279e-05\n",
            "  batch 815 loss: 5.117010697722435e-05\n",
            "  batch 816 loss: 8.22635143995285e-05\n",
            "  batch 817 loss: 5.2463717758655546e-05\n",
            "  batch 818 loss: 0.00013109569251537324\n",
            "  batch 819 loss: 6.355950981378555e-05\n",
            "  batch 820 loss: 0.0001463332623243332\n",
            "  batch 821 loss: 0.0001004670411348343\n",
            "  batch 822 loss: 0.00013256366550922394\n",
            "  batch 823 loss: 0.00010340140759944916\n",
            "  batch 824 loss: 0.00015468141436576844\n",
            "  batch 825 loss: 0.00012858299911022185\n",
            "  batch 826 loss: 0.0001026499941945076\n",
            "  batch 827 loss: 7.2953961789608e-05\n",
            "  batch 828 loss: 6.230102479457855e-05\n",
            "  batch 829 loss: 0.00012775817513465882\n",
            "  batch 830 loss: 7.069991528987885e-05\n",
            "  batch 831 loss: 0.00016605339944362641\n",
            "  batch 832 loss: 0.00010285128653049469\n",
            "  batch 833 loss: 0.00011338800191879273\n",
            "  batch 834 loss: 0.0001129801720380783\n",
            "  batch 835 loss: 0.00010292840749025344\n",
            "  batch 836 loss: 7.41068422794342e-05\n",
            "  batch 837 loss: 0.00011264444142580032\n",
            "  batch 838 loss: 0.0001577853411436081\n",
            "  batch 839 loss: 9.095872938632965e-05\n",
            "  batch 840 loss: 0.0001381719261407852\n",
            "  batch 841 loss: 7.794490456581116e-05\n",
            "  batch 842 loss: 0.00011412055045366287\n",
            "  batch 843 loss: 5.071883276104927e-05\n",
            "  batch 844 loss: 0.00012693585455417634\n",
            "  batch 845 loss: 0.00010099540650844574\n",
            "  batch 846 loss: 8.372930437326432e-05\n",
            "  batch 847 loss: 7.869167625904083e-05\n",
            "  batch 848 loss: 9.850423783063888e-05\n",
            "  batch 849 loss: 0.00011053049564361572\n",
            "  batch 850 loss: 6.306946277618409e-05\n",
            "  batch 851 loss: 8.392682671546937e-05\n",
            "  batch 852 loss: 6.4175546169281e-05\n",
            "  batch 853 loss: 8.743582665920258e-05\n",
            "  batch 854 loss: 0.0001033938005566597\n",
            "  batch 855 loss: 9.974770992994309e-05\n",
            "  batch 856 loss: 3.992130234837532e-05\n",
            "  batch 857 loss: 0.00013708856701850891\n",
            "  batch 858 loss: 0.00011864611506462097\n",
            "  batch 859 loss: 0.00011010389775037766\n",
            "  batch 860 loss: 0.00012508830428123474\n",
            "  batch 861 loss: 9.147149324417114e-05\n",
            "  batch 862 loss: 8.857864141464234e-05\n",
            "  batch 863 loss: 9.136305004358291e-05\n",
            "  batch 864 loss: 0.00015881147980690002\n",
            "  batch 865 loss: 0.00010225868970155716\n",
            "  batch 866 loss: 9.36216339468956e-05\n",
            "  batch 867 loss: 0.0001140957921743393\n",
            "  batch 868 loss: 0.00011718717962503433\n",
            "  batch 869 loss: 0.0001064917892217636\n",
            "  batch 870 loss: 8.321624994277954e-05\n",
            "  batch 871 loss: 6.70614019036293e-05\n",
            "  batch 872 loss: 9.994285553693771e-05\n",
            "  batch 873 loss: 7.90765956044197e-05\n",
            "  batch 874 loss: 0.00010171041637659073\n",
            "  batch 875 loss: 6.862913817167282e-05\n",
            "  batch 876 loss: 0.0001573403775691986\n",
            "  batch 877 loss: 6.307831406593322e-05\n",
            "  batch 878 loss: 0.00016045401990413667\n",
            "  batch 879 loss: 7.295827567577362e-05\n",
            "  batch 880 loss: 0.00010304168611764908\n",
            "  batch 881 loss: 8.928671479225159e-05\n",
            "  batch 882 loss: 8.967600762844086e-05\n",
            "  batch 883 loss: 9.407626092433929e-05\n",
            "  batch 884 loss: 9.718367457389832e-05\n",
            "  batch 885 loss: 0.00010541143268346787\n",
            "  batch 886 loss: 4.5602750033140184e-05\n",
            "  batch 887 loss: 0.00010095015168190002\n",
            "  batch 888 loss: 3.508242219686508e-05\n",
            "  batch 889 loss: 8.14903974533081e-05\n",
            "  batch 890 loss: 0.00011443730443716049\n",
            "  batch 891 loss: 9.674379974603653e-05\n",
            "  batch 892 loss: 0.00011023826897144318\n",
            "  batch 893 loss: 7.656430453062058e-05\n",
            "  batch 894 loss: 5.4990358650684354e-05\n",
            "  batch 895 loss: 9.594777226448059e-05\n",
            "  batch 896 loss: 0.00011032527685165405\n",
            "  batch 897 loss: 0.00011426639556884765\n",
            "  batch 898 loss: 0.00011056172847747802\n",
            "  batch 899 loss: 0.00018600960075855255\n",
            "  batch 900 loss: 0.00011451020836830139\n",
            "  batch 901 loss: 0.0001234918013215065\n",
            "  batch 902 loss: 0.0001174248531460762\n",
            "  batch 903 loss: 8.310667425394058e-05\n",
            "  batch 904 loss: 9.353825449943543e-05\n",
            "  batch 905 loss: 0.00010482697933912277\n",
            "  batch 906 loss: 0.00012415263056755066\n",
            "  batch 907 loss: 0.0001460096389055252\n",
            "  batch 908 loss: 0.00010373745113611222\n",
            "  batch 909 loss: 6.772028654813767e-05\n",
            "  batch 910 loss: 6.577474623918534e-05\n",
            "  batch 911 loss: 0.00012642858922481536\n",
            "  batch 912 loss: 9.675994515419006e-05\n",
            "  batch 913 loss: 5.94637431204319e-05\n",
            "  batch 914 loss: 0.00011823596805334091\n",
            "  batch 915 loss: 0.00011106586456298828\n",
            "  batch 916 loss: 0.00010241976380348206\n",
            "  batch 917 loss: 0.00012447763979434967\n",
            "  batch 918 loss: 5.961200967431069e-05\n",
            "  batch 919 loss: 0.00010124710947275161\n",
            "  batch 920 loss: 0.00018717031180858612\n",
            "  batch 921 loss: 7.151040434837341e-05\n",
            "  batch 922 loss: 9.253395348787308e-05\n",
            "  batch 923 loss: 0.00013503649830818177\n",
            "  batch 924 loss: 0.00014837290346622468\n",
            "  batch 925 loss: 6.0626916587352755e-05\n",
            "  batch 926 loss: 7.916289567947388e-05\n",
            "  batch 927 loss: 0.00010977735370397567\n",
            "  batch 928 loss: 0.00015410570800304413\n",
            "  batch 929 loss: 7.609911262989044e-05\n",
            "  batch 930 loss: 6.936166435480118e-05\n",
            "  batch 931 loss: 5.794127285480499e-05\n",
            "  batch 932 loss: 0.00016880391538143157\n",
            "  batch 933 loss: 0.00011296815425157547\n",
            "  batch 934 loss: 7.390729337930679e-05\n",
            "  batch 935 loss: 7.777166366577148e-05\n",
            "  batch 936 loss: 9.352624416351318e-05\n",
            "  batch 937 loss: 0.00011362327635288238\n",
            "  batch 938 loss: 0.0001017933189868927\n",
            "  batch 939 loss: 7.943964749574662e-05\n",
            "  batch 940 loss: 0.00013754433393478393\n",
            "  batch 941 loss: 0.00011633753776550293\n",
            "  batch 942 loss: 0.00017377659678459169\n",
            "  batch 943 loss: 0.00011635354161262513\n",
            "  batch 944 loss: 9.797771275043487e-05\n",
            "  batch 945 loss: 0.00010039473325014115\n",
            "  batch 946 loss: 0.0001215011104941368\n",
            "  batch 947 loss: 0.00012184622138738633\n",
            "  batch 948 loss: 0.0001003151386976242\n",
            "  batch 949 loss: 9.343396872282029e-05\n",
            "  batch 950 loss: 8.576714247465133e-05\n",
            "  batch 951 loss: 6.762891262769699e-05\n",
            "  batch 952 loss: 0.0001265942007303238\n",
            "  batch 953 loss: 7.400386035442352e-05\n",
            "  batch 954 loss: 4.103666171431541e-05\n",
            "  batch 955 loss: 8.3221435546875e-05\n",
            "  batch 956 loss: 0.0001068577840924263\n",
            "  batch 957 loss: 7.998942583799362e-05\n",
            "  batch 958 loss: 4.586464911699295e-05\n",
            "  batch 959 loss: 0.00010196150839328765\n",
            "  batch 960 loss: 8.283375948667526e-05\n",
            "  batch 961 loss: 0.00013677628338336944\n",
            "  batch 962 loss: 0.00010328447073698044\n",
            "  batch 963 loss: 0.00011681650578975677\n",
            "  batch 964 loss: 8.186603337526321e-05\n",
            "  batch 965 loss: 5.710703507065773e-05\n",
            "  batch 966 loss: 0.00014378394186496734\n",
            "  batch 967 loss: 7.35773891210556e-05\n",
            "  batch 968 loss: 0.00011145775020122528\n",
            "  batch 969 loss: 8.971645683050156e-05\n",
            "  batch 970 loss: 0.00011167502403259277\n",
            "  batch 971 loss: 3.840877488255501e-05\n",
            "  batch 972 loss: 0.00011991094797849655\n",
            "  batch 973 loss: 0.00011195920407772064\n",
            "  batch 974 loss: 8.834957331418991e-05\n",
            "  batch 975 loss: 0.00011775704473257065\n",
            "  batch 976 loss: 9.682182222604752e-05\n",
            "  batch 977 loss: 9.553129225969315e-05\n",
            "  batch 978 loss: 7.480799406766891e-05\n",
            "  batch 979 loss: 5.119316652417183e-05\n",
            "  batch 980 loss: 0.00012420094758272172\n",
            "  batch 981 loss: 0.00011555132269859314\n",
            "  batch 982 loss: 7.212132960557937e-05\n",
            "  batch 983 loss: 0.00016514007747173308\n",
            "  batch 984 loss: 0.00011345818638801575\n",
            "  batch 985 loss: 9.448683261871339e-05\n",
            "  batch 986 loss: 0.0001234111115336418\n",
            "  batch 987 loss: 9.947200864553451e-05\n",
            "  batch 988 loss: 8.663929253816605e-05\n",
            "  batch 989 loss: 4.59088459610939e-05\n",
            "  batch 990 loss: 4.577767103910446e-05\n",
            "  batch 991 loss: 0.00013903594017028808\n",
            "  batch 992 loss: 7.002846151590347e-05\n",
            "  batch 993 loss: 0.00013846433162689208\n",
            "  batch 994 loss: 0.00012203066051006317\n",
            "  batch 995 loss: 9.054124355316162e-05\n",
            "  batch 996 loss: 0.00011887311190366746\n",
            "  batch 997 loss: 7.279530912637711e-05\n",
            "  batch 998 loss: 0.00012909919023513795\n",
            "  batch 999 loss: 9.901592135429382e-05\n",
            "  batch 1000 loss: 8.854703605175019e-05\n",
            "  batch 1001 loss: 7.394643127918244e-05\n",
            "  batch 1002 loss: 0.00014680804312229155\n",
            "  batch 1003 loss: 8.962080627679824e-05\n",
            "  batch 1004 loss: 0.0001775272935628891\n",
            "  batch 1005 loss: 0.00011242453753948212\n",
            "  batch 1006 loss: 0.0001054181158542633\n",
            "  batch 1007 loss: 0.00013727563619613648\n",
            "  batch 1008 loss: 0.00011759360134601593\n",
            "  batch 1009 loss: 6.742242723703384e-05\n",
            "  batch 1010 loss: 0.00010164454579353333\n",
            "  batch 1011 loss: 0.00011847773194313049\n",
            "  batch 1012 loss: 0.0001484752744436264\n",
            "  batch 1013 loss: 0.00011770398914813995\n",
            "  batch 1014 loss: 9.949716925621032e-05\n",
            "  batch 1015 loss: 8.385277539491654e-05\n",
            "  batch 1016 loss: 6.208929792046547e-05\n",
            "  batch 1017 loss: 9.402773529291153e-05\n",
            "  batch 1018 loss: 9.628040343523026e-05\n",
            "  batch 1019 loss: 9.76610779762268e-05\n",
            "  batch 1020 loss: 6.649179011583329e-05\n",
            "  batch 1021 loss: 0.00013037681579589845\n",
            "  batch 1022 loss: 0.00011181407421827317\n",
            "  batch 1023 loss: 0.0001651245206594467\n",
            "  batch 1024 loss: 4.015419632196426e-05\n",
            "  batch 1025 loss: 0.0001511623114347458\n",
            "  batch 1026 loss: 0.00011103450506925584\n",
            "  batch 1027 loss: 0.0001237923800945282\n",
            "  batch 1028 loss: 0.000156472846865654\n",
            "  batch 1029 loss: 9.574142098426819e-05\n",
            "  batch 1030 loss: 7.931381464004516e-05\n",
            "  batch 1031 loss: 0.00012252152711153032\n",
            "  batch 1032 loss: 0.00015253981947898864\n",
            "  batch 1033 loss: 5.4928693920373914e-05\n",
            "  batch 1034 loss: 5.742736160755158e-05\n",
            "  batch 1035 loss: 0.00014094848930835724\n",
            "  batch 1036 loss: 6.355416029691696e-05\n",
            "  batch 1037 loss: 9.994665533304214e-05\n",
            "  batch 1038 loss: 6.795811653137206e-05\n",
            "  batch 1039 loss: 5.9215083718299865e-05\n",
            "  batch 1040 loss: 5.820263549685478e-05\n",
            "  batch 1041 loss: 6.052413955330849e-05\n",
            "  batch 1042 loss: 0.0001597045660018921\n",
            "  batch 1043 loss: 9.870103746652603e-05\n",
            "  batch 1044 loss: 0.00012696418166160584\n",
            "  batch 1045 loss: 9.891034662723542e-05\n",
            "  batch 1046 loss: 0.00011346948146820068\n",
            "  batch 1047 loss: 7.898557931184768e-05\n",
            "  batch 1048 loss: 0.00010367779433727264\n",
            "  batch 1049 loss: 7.937446981668472e-05\n",
            "  batch 1050 loss: 8.431737869977952e-05\n",
            "  batch 1051 loss: 5.6705016642808917e-05\n",
            "  batch 1052 loss: 9.523352235555649e-05\n",
            "  batch 1053 loss: 7.879222184419632e-05\n",
            "  batch 1054 loss: 0.00011886152625083924\n",
            "  batch 1055 loss: 0.00014548304677009582\n",
            "  batch 1056 loss: 0.0001563175171613693\n",
            "  batch 1057 loss: 8.17975178360939e-05\n",
            "  batch 1058 loss: 0.00013338930904865264\n",
            "  batch 1059 loss: 8.782545477151871e-05\n",
            "  batch 1060 loss: 0.00015986981987953187\n",
            "  batch 1061 loss: 0.00011291328072547913\n",
            "  batch 1062 loss: 0.00011378853023052216\n",
            "  batch 1063 loss: 0.00012771640717983246\n",
            "  batch 1064 loss: 6.541734933853149e-05\n",
            "  batch 1065 loss: 8.356419950723648e-05\n",
            "  batch 1066 loss: 0.00010573574900627136\n",
            "  batch 1067 loss: 8.107291162014008e-05\n",
            "  batch 1068 loss: 0.00015745073556900025\n",
            "  batch 1069 loss: 7.704389840364456e-05\n",
            "  batch 1070 loss: 8.578912913799286e-05\n",
            "  batch 1071 loss: 9.232641756534577e-05\n",
            "  batch 1072 loss: 0.00019984832406044005\n",
            "  batch 1073 loss: 0.00011489959061145782\n",
            "  batch 1074 loss: 9.663705527782441e-05\n",
            "  batch 1075 loss: 7.836064696311951e-05\n",
            "  batch 1076 loss: 4.365793243050575e-05\n",
            "  batch 1077 loss: 8.404217660427094e-05\n",
            "  batch 1078 loss: 4.05958779156208e-05\n",
            "  batch 1079 loss: 0.00011491266638040542\n",
            "  batch 1080 loss: 9.145819395780563e-05\n",
            "  batch 1081 loss: 9.900432080030441e-05\n",
            "  batch 1082 loss: 9.229452162981033e-05\n",
            "  batch 1083 loss: 4.208711162209511e-05\n",
            "  batch 1084 loss: 0.00012931831181049348\n",
            "  batch 1085 loss: 0.00011443640291690827\n",
            "  batch 1086 loss: 7.681317627429963e-05\n",
            "  batch 1087 loss: 7.955466210842133e-05\n",
            "  batch 1088 loss: 0.00012991389632225036\n",
            "  batch 1089 loss: 8.287549763917924e-05\n",
            "  batch 1090 loss: 0.0001267445981502533\n",
            "  batch 1091 loss: 0.00010426647961139679\n",
            "  batch 1092 loss: 0.0001030837818980217\n",
            "  batch 1093 loss: 9.583942592144013e-05\n",
            "  batch 1094 loss: 6.620285660028457e-05\n",
            "  batch 1095 loss: 0.00010835230350494384\n",
            "  batch 1096 loss: 0.0001324125975370407\n",
            "  batch 1097 loss: 0.00013293509185314178\n",
            "  batch 1098 loss: 7.38416239619255e-05\n",
            "  batch 1099 loss: 8.16032513976097e-05\n",
            "  batch 1100 loss: 8.532974869012832e-05\n",
            "  batch 1101 loss: 7.533544301986695e-05\n",
            "  batch 1102 loss: 8.576958626508713e-05\n",
            "  batch 1103 loss: 7.700428366661072e-05\n",
            "  batch 1104 loss: 0.00012334466725587846\n",
            "  batch 1105 loss: 8.542058616876602e-05\n",
            "  batch 1106 loss: 0.00012044522911310196\n",
            "  batch 1107 loss: 8.365938067436218e-05\n",
            "  batch 1108 loss: 8.95460695028305e-05\n",
            "  batch 1109 loss: 8.368082344532013e-05\n",
            "  batch 1110 loss: 0.0001151285320520401\n",
            "  batch 1111 loss: 0.00018422405421733855\n",
            "  batch 1112 loss: 9.724689275026321e-05\n",
            "  batch 1113 loss: 7.91691243648529e-05\n",
            "  batch 1114 loss: 0.00012950071692466737\n",
            "  batch 1115 loss: 0.00011651396751403808\n",
            "  batch 1116 loss: 8.768244832754136e-05\n",
            "  batch 1117 loss: 4.3864600360393525e-05\n",
            "  batch 1118 loss: 0.00011010143160820008\n",
            "  batch 1119 loss: 0.00010048094391822815\n",
            "  batch 1120 loss: 8.656445890665055e-05\n",
            "  batch 1121 loss: 0.0001007062941789627\n",
            "  batch 1122 loss: 8.345871418714523e-05\n",
            "  batch 1123 loss: 0.00011266091465950013\n",
            "  batch 1124 loss: 8.913695067167282e-05\n",
            "  batch 1125 loss: 8.090253919363022e-05\n",
            "  batch 1126 loss: 7.138204574584962e-05\n",
            "  batch 1127 loss: 6.724366545677185e-05\n",
            "  batch 1128 loss: 0.00010974831879138947\n",
            "  batch 1129 loss: 6.354746967554092e-05\n",
            "  batch 1130 loss: 3.795681521296501e-05\n",
            "  batch 1131 loss: 0.00013051651418209077\n",
            "  batch 1132 loss: 0.0001708439439535141\n",
            "  batch 1133 loss: 4.660868272185326e-05\n",
            "  batch 1134 loss: 8.107569813728333e-05\n",
            "  batch 1135 loss: 9.687672555446625e-05\n",
            "  batch 1136 loss: 8.151187002658844e-05\n",
            "  batch 1137 loss: 0.00012507244944572448\n",
            "  batch 1138 loss: 0.00012539710104465484\n",
            "  batch 1139 loss: 0.00014095973968505858\n",
            "  batch 1140 loss: 7.581048458814622e-05\n",
            "  batch 1141 loss: 8.192995935678482e-05\n",
            "  batch 1142 loss: 6.553372740745545e-05\n",
            "  batch 1143 loss: 3.882621228694916e-05\n",
            "  batch 1144 loss: 0.00016562861204147338\n",
            "  batch 1145 loss: 5.0924208015203474e-05\n",
            "  batch 1146 loss: 0.00012895160913467408\n",
            "  batch 1147 loss: 0.00010214756429195403\n",
            "  batch 1148 loss: 0.00011725398153066635\n",
            "  batch 1149 loss: 0.00020370663702487947\n",
            "  batch 1150 loss: 0.00017128580808639527\n",
            "  batch 1151 loss: 0.00014955058693885803\n",
            "  batch 1152 loss: 6.284484267234802e-05\n",
            "  batch 1153 loss: 0.00011439335346221924\n",
            "  batch 1154 loss: 9.628532826900482e-05\n",
            "  batch 1155 loss: 8.762439340353012e-05\n",
            "  batch 1156 loss: 8.765772730112075e-05\n",
            "  batch 1157 loss: 0.00014953097701072692\n",
            "  batch 1158 loss: 9.983866661787034e-05\n",
            "  batch 1159 loss: 5.975344032049179e-05\n",
            "  batch 1160 loss: 5.775371566414833e-05\n",
            "  batch 1161 loss: 9.900657832622528e-05\n",
            "  batch 1162 loss: 9.817790985107422e-05\n",
            "  batch 1163 loss: 0.00017081215977668763\n",
            "  batch 1164 loss: 8.469107002019882e-05\n",
            "  batch 1165 loss: 8.33001732826233e-05\n",
            "  batch 1166 loss: 0.000135536789894104\n",
            "  batch 1167 loss: 8.910781145095825e-05\n",
            "  batch 1168 loss: 0.000130281463265419\n",
            "  batch 1169 loss: 5.728782713413239e-05\n",
            "  batch 1170 loss: 0.00013453423976898194\n",
            "  batch 1171 loss: 0.00012011776864528655\n",
            "  batch 1172 loss: 0.00012439554929733276\n",
            "  batch 1173 loss: 0.00011041539907455444\n",
            "  batch 1174 loss: 0.0001009531244635582\n",
            "  batch 1175 loss: 0.00011260780692100525\n",
            "  batch 1176 loss: 0.0001130601242184639\n",
            "  batch 1177 loss: 0.00013842912018299103\n",
            "  batch 1178 loss: 0.00012181596457958221\n",
            "  batch 1179 loss: 9.255421161651611e-05\n",
            "  batch 1180 loss: 5.219307169318199e-05\n",
            "  batch 1181 loss: 9.355033934116363e-05\n",
            "  batch 1182 loss: 0.00017525966465473176\n",
            "  batch 1183 loss: 6.867425888776779e-05\n",
            "  batch 1184 loss: 4.0422268211841585e-05\n",
            "  batch 1185 loss: 9.038840234279633e-05\n",
            "  batch 1186 loss: 0.0001455533653497696\n",
            "  batch 1187 loss: 6.481377035379409e-05\n",
            "  batch 1188 loss: 0.00010740945488214493\n",
            "  batch 1189 loss: 0.0001493745744228363\n",
            "  batch 1190 loss: 6.98067769408226e-05\n",
            "  batch 1191 loss: 0.00014236696064472197\n",
            "  batch 1192 loss: 0.00010424649715423583\n",
            "  batch 1193 loss: 8.755306899547578e-05\n",
            "  batch 1194 loss: 7.606480270624161e-05\n",
            "  batch 1195 loss: 5.985285341739655e-05\n",
            "  batch 1196 loss: 0.00013513506948947907\n",
            "  batch 1197 loss: 0.00019398126006126403\n",
            "  batch 1198 loss: 0.00010945627093315124\n",
            "  batch 1199 loss: 8.145006000995636e-05\n",
            "  batch 1200 loss: 9.135192632675171e-05\n",
            "  batch 1201 loss: 8.469603955745696e-05\n",
            "  batch 1202 loss: 9.16423723101616e-05\n",
            "  batch 1203 loss: 0.00011206777393817902\n",
            "  batch 1204 loss: 5.804922431707382e-05\n",
            "  batch 1205 loss: 2.5373255833983422e-05\n",
            "  batch 1206 loss: 0.00014674624800682068\n",
            "  batch 1207 loss: 0.0001836817115545273\n",
            "  batch 1208 loss: 8.547618240118027e-05\n",
            "  batch 1209 loss: 8.078442513942718e-05\n",
            "  batch 1210 loss: 9.331919997930527e-05\n",
            "  batch 1211 loss: 0.00011935202777385711\n",
            "  batch 1212 loss: 0.0001519700437784195\n",
            "  batch 1213 loss: 0.00010935433954000472\n",
            "  batch 1214 loss: 0.00010599064826965332\n",
            "  batch 1215 loss: 4.232252016663551e-05\n",
            "  batch 1216 loss: 0.0001480032056570053\n",
            "  batch 1217 loss: 9.273692220449448e-05\n",
            "  batch 1218 loss: 8.199093490839005e-05\n",
            "  batch 1219 loss: 7.05246776342392e-05\n",
            "  batch 1220 loss: 8.517025411128998e-05\n",
            "  batch 1221 loss: 9.254395216703415e-05\n",
            "  batch 1222 loss: 9.598156809806824e-05\n",
            "  batch 1223 loss: 6.328033655881882e-05\n",
            "  batch 1224 loss: 7.647794485092163e-05\n",
            "  batch 1225 loss: 7.775991410017014e-05\n",
            "  batch 1226 loss: 7.507053762674332e-05\n",
            "  batch 1227 loss: 0.00012850335240364075\n",
            "  batch 1228 loss: 7.48283788561821e-05\n",
            "  batch 1229 loss: 8.310312032699585e-05\n",
            "  batch 1230 loss: 0.00011963646113872529\n",
            "  batch 1231 loss: 7.038968801498413e-05\n",
            "  batch 1232 loss: 0.00014521071314811706\n",
            "  batch 1233 loss: 5.5610407143831254e-05\n",
            "  batch 1234 loss: 0.00011027153581380844\n",
            "  batch 1235 loss: 7.84710943698883e-05\n",
            "  batch 1236 loss: 0.0001697486639022827\n",
            "  batch 1237 loss: 7.520215213298797e-05\n",
            "  batch 1238 loss: 8.416383713483811e-05\n",
            "  batch 1239 loss: 6.185921281576156e-05\n",
            "  batch 1240 loss: 7.614416629076005e-05\n",
            "  batch 1241 loss: 8.640054613351822e-05\n",
            "  batch 1242 loss: 7.909250259399414e-05\n",
            "  batch 1243 loss: 0.00011857544630765915\n",
            "  batch 1244 loss: 9.710493683815002e-05\n",
            "  batch 1245 loss: 0.00014073054492473602\n",
            "  batch 1246 loss: 8.263736218214035e-05\n",
            "  batch 1247 loss: 0.000113890640437603\n",
            "  batch 1248 loss: 0.00010785790532827377\n",
            "  batch 1249 loss: 5.821725726127624e-05\n",
            "  batch 1250 loss: 9.77906659245491e-05\n",
            "  batch 1251 loss: 6.21676966547966e-05\n",
            "  batch 1252 loss: 0.00012149656563997269\n",
            "  batch 1253 loss: 6.70706182718277e-05\n",
            "  batch 1254 loss: 8.505649864673615e-05\n",
            "  batch 1255 loss: 0.00011605620384216308\n",
            "  batch 1256 loss: 5.7734616100788117e-05\n",
            "  batch 1257 loss: 7.069624960422516e-05\n",
            "  batch 1258 loss: 0.00011667870730161667\n",
            "  batch 1259 loss: 8.275780826807022e-05\n",
            "  batch 1260 loss: 0.00010592972487211228\n",
            "  batch 1261 loss: 0.0001391732096672058\n",
            "  batch 1262 loss: 5.779613927006721e-05\n",
            "  batch 1263 loss: 7.185201346874237e-05\n",
            "  batch 1264 loss: 6.111487001180649e-05\n",
            "  batch 1265 loss: 4.3566729873418806e-05\n",
            "  batch 1266 loss: 0.00010317637771368027\n",
            "  batch 1267 loss: 6.449925899505615e-05\n",
            "  batch 1268 loss: 0.00010372068732976914\n",
            "  batch 1269 loss: 3.8972210139036176e-05\n",
            "  batch 1270 loss: 0.00014679189026355744\n",
            "  batch 1271 loss: 5.888544768095016e-05\n",
            "  batch 1272 loss: 6.201623752713203e-05\n",
            "  batch 1273 loss: 4.022560641169548e-05\n",
            "  batch 1274 loss: 9.408096224069596e-05\n",
            "  batch 1275 loss: 7.446566969156265e-05\n",
            "  batch 1276 loss: 0.000116680808365345\n",
            "  batch 1277 loss: 7.289285212755204e-05\n",
            "  batch 1278 loss: 9.383166581392288e-05\n",
            "  batch 1279 loss: 0.0001202562227845192\n",
            "  batch 1280 loss: 0.00011403253674507141\n",
            "  batch 1281 loss: 6.195275485515594e-05\n",
            "  batch 1282 loss: 8.376640826463699e-05\n",
            "  batch 1283 loss: 6.524324417114257e-05\n",
            "  batch 1284 loss: 0.00013685348629951478\n",
            "  batch 1285 loss: 8.51980522274971e-05\n",
            "  batch 1286 loss: 5.247398838400841e-05\n",
            "  batch 1287 loss: 6.57549947500229e-05\n",
            "  batch 1288 loss: 0.00016003267467021943\n",
            "  batch 1289 loss: 7.547762989997864e-05\n",
            "  batch 1290 loss: 0.00010219845175743102\n",
            "  batch 1291 loss: 9.399580955505371e-05\n",
            "  batch 1292 loss: 0.00010579010099172592\n",
            "  batch 1293 loss: 0.00010054224729537964\n",
            "  batch 1294 loss: 0.0001032889038324356\n",
            "  batch 1295 loss: 0.00013989877700805665\n",
            "  batch 1296 loss: 0.00015174676477909088\n",
            "  batch 1297 loss: 7.571155577898026e-05\n",
            "  batch 1298 loss: 5.963125452399254e-05\n",
            "  batch 1299 loss: 6.615422666072845e-05\n",
            "  batch 1300 loss: 0.000138100802898407\n",
            "  batch 1301 loss: 9.360291808843613e-05\n",
            "  batch 1302 loss: 5.502790957689285e-05\n",
            "  batch 1303 loss: 0.00010747204720973969\n",
            "  batch 1304 loss: 9.482388198375702e-05\n",
            "  batch 1305 loss: 9.72154662013054e-05\n",
            "  batch 1306 loss: 0.00013873143494129181\n",
            "  batch 1307 loss: 7.43333175778389e-05\n",
            "  batch 1308 loss: 9.673025459051133e-05\n",
            "  batch 1309 loss: 6.131498888134956e-05\n",
            "  batch 1310 loss: 6.797275692224502e-05\n",
            "  batch 1311 loss: 5.741690471768379e-05\n",
            "  batch 1312 loss: 9.283909201622009e-05\n",
            "  batch 1313 loss: 5.255258455872536e-05\n",
            "  batch 1314 loss: 6.00108839571476e-05\n",
            "  batch 1315 loss: 0.00016819725930690765\n",
            "  batch 1316 loss: 8.837635070085526e-05\n",
            "  batch 1317 loss: 9.23871025443077e-05\n",
            "  batch 1318 loss: 6.807027757167816e-05\n",
            "  batch 1319 loss: 6.629346311092377e-05\n",
            "  batch 1320 loss: 0.0013700718879699706\n",
            "  batch 1321 loss: 0.00012065502256155014\n",
            "  batch 1322 loss: 8.846639841794968e-05\n",
            "  batch 1323 loss: 0.0001114855781197548\n",
            "  batch 1324 loss: 0.00013424792885780335\n",
            "  batch 1325 loss: 0.0001299201250076294\n",
            "  batch 1326 loss: 0.00012263135612010956\n",
            "  batch 1327 loss: 0.0001192997395992279\n",
            "  batch 1328 loss: 0.00011913184821605682\n",
            "  batch 1329 loss: 0.00010351385921239853\n",
            "  batch 1330 loss: 7.264317572116852e-05\n",
            "  batch 1331 loss: 0.00011129386723041535\n",
            "  batch 1332 loss: 8.027684688568115e-05\n",
            "  batch 1333 loss: 0.00011597418785095214\n",
            "  batch 1334 loss: 9.447239339351655e-05\n",
            "  batch 1335 loss: 0.00011241750419139862\n",
            "  batch 1336 loss: 9.918144345283508e-05\n",
            "  batch 1337 loss: 9.77955237030983e-05\n",
            "  batch 1338 loss: 8.4675133228302e-05\n",
            "  batch 1339 loss: 8.371032029390335e-05\n",
            "  batch 1340 loss: 0.0030508058071136474\n",
            "  batch 1341 loss: 8.34052711725235e-05\n",
            "  batch 1342 loss: 0.0008223819732666016\n",
            "  batch 1343 loss: 0.005159875392913818\n",
            "  batch 1344 loss: 0.007927260875701904\n",
            "  batch 1345 loss: 0.0008196394443511963\n",
            "  batch 1346 loss: 0.00011188782751560211\n",
            "  batch 1347 loss: 0.00011827404052019119\n",
            "  batch 1348 loss: 0.00018718104064464569\n",
            "  batch 1349 loss: 0.00020946070551872254\n",
            "  batch 1350 loss: 0.00024343109130859375\n",
            "  batch 1351 loss: 0.00012942123413085937\n",
            "  batch 1352 loss: 0.00021921028196811675\n",
            "  batch 1353 loss: 0.00023425188660621644\n",
            "  batch 1354 loss: 0.0001736437976360321\n",
            "  batch 1355 loss: 0.00011942576617002487\n",
            "  batch 1356 loss: 0.00017749816179275514\n",
            "  batch 1357 loss: 8.085881173610687e-05\n",
            "  batch 1358 loss: 9.254150837659835e-05\n",
            "  batch 1359 loss: 9.905929863452911e-05\n",
            "  batch 1360 loss: 7.348663359880448e-05\n",
            "  batch 1361 loss: 0.00012949267029762268\n",
            "  batch 1362 loss: 0.000102143794298172\n",
            "  batch 1363 loss: 0.00014689239859580993\n",
            "  batch 1364 loss: 0.00011380503326654434\n",
            "  batch 1365 loss: 0.00010100810974836349\n",
            "  batch 1366 loss: 0.00010495881736278534\n",
            "  batch 1367 loss: 9.496542066335678e-05\n",
            "  batch 1368 loss: 0.00012944269180297852\n",
            "  batch 1369 loss: 7.365195453166961e-05\n",
            "  batch 1370 loss: 9.463541209697723e-05\n",
            "  batch 1371 loss: 8.338245749473572e-05\n",
            "  batch 1372 loss: 7.3062002658844e-05\n",
            "  batch 1373 loss: 0.00017251430451869964\n",
            "  batch 1374 loss: 9.405659139156341e-05\n",
            "  batch 1375 loss: 7.759275287389755e-05\n",
            "  batch 1376 loss: 0.00010485439747571945\n",
            "  batch 1377 loss: 9.40251275897026e-05\n",
            "  batch 1378 loss: 8.353094756603242e-05\n",
            "  batch 1379 loss: 9.461846947669983e-05\n",
            "  batch 1380 loss: 8.783791959285737e-05\n",
            "  batch 1381 loss: 0.00013603414595127105\n",
            "  batch 1382 loss: 0.00018074215948581695\n",
            "  batch 1383 loss: 0.0001226007416844368\n",
            "  batch 1384 loss: 7.32368677854538e-05\n",
            "  batch 1385 loss: 0.00010606878250837326\n",
            "  batch 1386 loss: 0.00012907013297080994\n",
            "  batch 1387 loss: 0.00011071494221687316\n",
            "  batch 1388 loss: 0.00010945083945989609\n",
            "  batch 1389 loss: 0.00014314883947372437\n",
            "  batch 1390 loss: 0.00015454521775245666\n",
            "  batch 1391 loss: 9.240657836198807e-05\n",
            "  batch 1392 loss: 9.209538996219636e-05\n",
            "  batch 1393 loss: 9.470199793577194e-05\n",
            "  batch 1394 loss: 0.0001299155652523041\n",
            "  batch 1395 loss: 0.00015299995243549347\n",
            "  batch 1396 loss: 6.471320241689682e-05\n",
            "  batch 1397 loss: 0.00013166713714599608\n",
            "  batch 1398 loss: 4.728347063064575e-05\n",
            "  batch 1399 loss: 0.00015475337207317353\n",
            "  batch 1400 loss: 0.00013436290621757506\n",
            "  batch 1401 loss: 9.946427494287491e-05\n",
            "  batch 1402 loss: 6.108059361577034e-05\n",
            "  batch 1403 loss: 9.29076299071312e-05\n",
            "  batch 1404 loss: 0.00012106288969516754\n",
            "  batch 1405 loss: 0.00010265348106622696\n",
            "  batch 1406 loss: 9.344343096017837e-05\n",
            "  batch 1407 loss: 0.00010084987431764603\n",
            "  batch 1408 loss: 0.00011857572942972183\n",
            "  batch 1409 loss: 0.00011541663110256195\n",
            "  batch 1410 loss: 9.204371273517608e-05\n",
            "  batch 1411 loss: 0.0001274537295103073\n",
            "  batch 1412 loss: 0.00014529183506965638\n",
            "  batch 1413 loss: 0.00019403328001499175\n",
            "  batch 1414 loss: 9.821202605962753e-05\n",
            "  batch 1415 loss: 0.00014247201383113861\n",
            "  batch 1416 loss: 0.0001160535216331482\n",
            "  batch 1417 loss: 0.0001382139027118683\n",
            "  batch 1418 loss: 0.0001093071773648262\n",
            "  batch 1419 loss: 9.566210955381393e-05\n",
            "  batch 1420 loss: 7.390142232179641e-05\n",
            "  batch 1421 loss: 0.00012231528013944626\n",
            "  batch 1422 loss: 0.00016749097406864167\n",
            "  batch 1423 loss: 0.00013512247800827025\n",
            "  batch 1424 loss: 8.713941276073456e-05\n",
            "  batch 1425 loss: 0.00010639665275812148\n",
            "  batch 1426 loss: 9.847154468297959e-05\n",
            "  batch 1427 loss: 0.00014411920309066772\n",
            "  batch 1428 loss: 0.00011077792197465897\n",
            "  batch 1429 loss: 0.00010771164298057556\n",
            "  batch 1430 loss: 4.2706262320280075e-05\n",
            "  batch 1431 loss: 0.0002605384290218353\n",
            "  batch 1432 loss: 0.0001283775418996811\n",
            "  batch 1433 loss: 9.850059449672699e-05\n",
            "  batch 1434 loss: 0.0001275629699230194\n",
            "  batch 1435 loss: 0.0001168750524520874\n",
            "  batch 1436 loss: 8.332248777151107e-05\n",
            "  batch 1437 loss: 0.00015262506902217866\n",
            "  batch 1438 loss: 3.943083435297012e-05\n",
            "  batch 1439 loss: 0.00010096287727355957\n",
            "  batch 1440 loss: 0.00011289726942777634\n",
            "  batch 1441 loss: 0.00012946324050426482\n",
            "  batch 1442 loss: 0.00015025608241558074\n",
            "  batch 1443 loss: 0.00015075910091400148\n",
            "  batch 1444 loss: 0.00011222951859235764\n",
            "  batch 1445 loss: 0.00020715658366680145\n",
            "  batch 1446 loss: 0.00010875177383422851\n",
            "  batch 1447 loss: 8.14594179391861e-05\n",
            "  batch 1448 loss: 0.00026436448097229005\n",
            "  batch 1449 loss: 8.803046494722366e-05\n",
            "  batch 1450 loss: 8.107878267765045e-05\n",
            "  batch 1451 loss: 7.547792047262192e-05\n",
            "  batch 1452 loss: 0.0001119617223739624\n",
            "  batch 1453 loss: 0.00017200247943401336\n",
            "  batch 1454 loss: 0.0001266029328107834\n",
            "  batch 1455 loss: 0.00014357537031173706\n",
            "  batch 1456 loss: 0.00012327697873115538\n",
            "  batch 1457 loss: 0.0001186298280954361\n",
            "  batch 1458 loss: 9.182295948266983e-05\n",
            "  batch 1459 loss: 0.00011366170644760131\n",
            "  batch 1460 loss: 9.590022265911102e-05\n",
            "  batch 1461 loss: 0.00013095666468143462\n",
            "  batch 1462 loss: 0.00011895138770341873\n",
            "  batch 1463 loss: 0.00017056621611118315\n",
            "  batch 1464 loss: 0.00010115841031074524\n",
            "  batch 1465 loss: 4.858190566301346e-05\n",
            "  batch 1466 loss: 0.00023856364190578462\n",
            "  batch 1467 loss: 0.00016856755316257477\n",
            "  batch 1468 loss: 9.392943978309631e-05\n",
            "  batch 1469 loss: 0.00013475076854228973\n",
            "  batch 1470 loss: 0.00015101870894432067\n",
            "  batch 1471 loss: 0.00014228230714797974\n",
            "  batch 1472 loss: 0.00010826598107814789\n",
            "  batch 1473 loss: 0.00011549419164657593\n",
            "  batch 1474 loss: 0.00012836620211601258\n",
            "  batch 1475 loss: 0.0001281999498605728\n",
            "  batch 1476 loss: 0.00012263255566358566\n",
            "  batch 1477 loss: 9.912114590406418e-05\n",
            "  batch 1478 loss: 0.00013573665916919708\n",
            "  batch 1479 loss: 0.00013540542125701905\n",
            "  batch 1480 loss: 6.345842778682708e-05\n",
            "  batch 1481 loss: 0.00011985626071691512\n",
            "  batch 1482 loss: 0.0001050359383225441\n",
            "  batch 1483 loss: 6.783398240804672e-05\n",
            "  batch 1484 loss: 0.00011378829181194306\n",
            "  batch 1485 loss: 0.0001527227908372879\n",
            "  batch 1486 loss: 0.00011716721951961517\n",
            "  batch 1487 loss: 0.00010123973339796066\n",
            "  batch 1488 loss: 0.0001047351211309433\n",
            "  batch 1489 loss: 8.005093038082123e-05\n",
            "  batch 1490 loss: 9.67971533536911e-05\n",
            "  batch 1491 loss: 0.00011056313663721084\n",
            "  batch 1492 loss: 8.097347617149352e-05\n",
            "  batch 1493 loss: 0.00010078061372041702\n",
            "  batch 1494 loss: 0.00013524778187274934\n",
            "  batch 1495 loss: 7.488800585269928e-05\n",
            "  batch 1496 loss: 0.0001596343219280243\n",
            "  batch 1497 loss: 0.0001406501680612564\n",
            "  batch 1498 loss: 0.00012046703696250915\n",
            "  batch 1499 loss: 0.00010082119703292847\n",
            "  batch 1500 loss: 0.00010078386962413788\n",
            "  batch 1501 loss: 9.218994528055191e-05\n",
            "  batch 1502 loss: 0.00014736326038837432\n",
            "  batch 1503 loss: 0.00011572179198265075\n",
            "  batch 1504 loss: 9.506563097238541e-05\n",
            "  batch 1505 loss: 0.00015031151473522187\n",
            "  batch 1506 loss: 8.658543229103088e-05\n",
            "  batch 1507 loss: 0.00011025519669055938\n",
            "  batch 1508 loss: 0.00014686623215675354\n",
            "  batch 1509 loss: 0.00012851722538471223\n",
            "  batch 1510 loss: 9.03400033712387e-05\n",
            "  batch 1511 loss: 0.0001383182555437088\n",
            "  batch 1512 loss: 9.527172148227692e-05\n",
            "  batch 1513 loss: 7.720144838094711e-05\n",
            "  batch 1514 loss: 9.153685718774796e-05\n",
            "  batch 1515 loss: 0.0001291022300720215\n",
            "  batch 1516 loss: 0.00010647296905517578\n",
            "  batch 1517 loss: 6.889662891626358e-05\n",
            "  batch 1518 loss: 0.00011387086659669876\n",
            "  batch 1519 loss: 0.0001408393234014511\n",
            "  batch 1520 loss: 0.00010623587667942047\n",
            "  batch 1521 loss: 0.00010953018069267273\n",
            "  batch 1522 loss: 0.00015327560901641846\n",
            "  batch 1523 loss: 0.00013626414537429808\n",
            "  batch 1524 loss: 0.00017741559445858\n",
            "  batch 1525 loss: 0.00012998120486736297\n",
            "  batch 1526 loss: 0.00014112445712089538\n",
            "  batch 1527 loss: 0.00015836887061595917\n",
            "  batch 1528 loss: 4.9869272857904436e-05\n",
            "  batch 1529 loss: 0.00010132388025522232\n",
            "  batch 1530 loss: 7.330147176980972e-05\n",
            "  batch 1531 loss: 0.00012639413774013518\n",
            "  batch 1532 loss: 0.00014325165748596192\n",
            "  batch 1533 loss: 0.00010987092554569245\n",
            "  batch 1534 loss: 0.00017736537754535676\n",
            "  batch 1535 loss: 0.00015108297765254974\n",
            "  batch 1536 loss: 9.31667909026146e-05\n",
            "  batch 1537 loss: 0.00010513971000909805\n",
            "  batch 1538 loss: 0.00011137335747480393\n",
            "  batch 1539 loss: 9.979903697967529e-05\n",
            "  batch 1540 loss: 0.00011124615371227265\n",
            "  batch 1541 loss: 9.125319123268128e-05\n",
            "  batch 1542 loss: 7.455223798751831e-05\n",
            "  batch 1543 loss: 7.442194968461991e-05\n",
            "  batch 1544 loss: 0.00012584389746189117\n",
            "  batch 1545 loss: 0.00019816139340400696\n",
            "  batch 1546 loss: 0.0001178656741976738\n",
            "  batch 1547 loss: 0.0001033012792468071\n",
            "  batch 1548 loss: 8.27227532863617e-05\n",
            "  batch 1549 loss: 8.594285696744919e-05\n",
            "  batch 1550 loss: 0.0001277213841676712\n",
            "  batch 1551 loss: 0.00010917589813470841\n",
            "  batch 1552 loss: 0.00010322988033294678\n",
            "  batch 1553 loss: 0.00010132779181003571\n",
            "  batch 1554 loss: 0.00011791761219501495\n",
            "  batch 1555 loss: 9.13849025964737e-05\n",
            "  batch 1556 loss: 0.0001416989266872406\n",
            "  batch 1557 loss: 0.00010534919798374176\n",
            "  batch 1558 loss: 0.00013765867054462433\n",
            "  batch 1559 loss: 0.00013158062100410462\n",
            "  batch 1560 loss: 0.00013410872220993042\n",
            "  batch 1561 loss: 0.00015781620144844055\n",
            "  batch 1562 loss: 6.264340132474899e-05\n",
            "  batch 1563 loss: 8.49069207906723e-05\n",
            "  batch 1564 loss: 6.271746754646301e-05\n",
            "  batch 1565 loss: 0.00011641918122768402\n",
            "  batch 1566 loss: 0.0001295700818300247\n",
            "  batch 1567 loss: 7.99303725361824e-05\n",
            "  batch 1568 loss: 0.00012234511971473693\n",
            "  batch 1569 loss: 0.00011799589544534683\n",
            "  batch 1570 loss: 7.379713654518127e-05\n",
            "  batch 1571 loss: 9.455940127372742e-05\n",
            "  batch 1572 loss: 0.00011185640841722488\n",
            "  batch 1573 loss: 7.239610701799393e-05\n",
            "  batch 1574 loss: 0.000155251145362854\n",
            "  batch 1575 loss: 0.00010738833993673324\n",
            "  batch 1576 loss: 0.00015064483880996703\n",
            "  batch 1577 loss: 9.151915460824967e-05\n",
            "  batch 1578 loss: 0.0001582900881767273\n",
            "  batch 1579 loss: 6.45354613661766e-05\n",
            "  batch 1580 loss: 6.385841220617295e-05\n",
            "  batch 1581 loss: 9.12277027964592e-05\n",
            "  batch 1582 loss: 9.945426136255264e-05\n",
            "  batch 1583 loss: 0.00010104431957006455\n",
            "  batch 1584 loss: 9.249836206436157e-05\n",
            "  batch 1585 loss: 0.00011704491078853607\n",
            "  batch 1586 loss: 0.00012810558080673218\n",
            "  batch 1587 loss: 9.768915176391602e-05\n",
            "  batch 1588 loss: 8.386901766061783e-05\n",
            "  batch 1589 loss: 5.476576089859009e-05\n",
            "  batch 1590 loss: 4.446600750088692e-05\n",
            "  batch 1591 loss: 6.149397417902947e-05\n",
            "  batch 1592 loss: 0.00014327871799468995\n",
            "  batch 1593 loss: 0.00011820178478956222\n",
            "  batch 1594 loss: 8.153962343931198e-05\n",
            "  batch 1595 loss: 0.0001817484498023987\n",
            "  batch 1596 loss: 9.147443622350693e-05\n",
            "  batch 1597 loss: 0.00010988397151231766\n",
            "  batch 1598 loss: 0.00011602945625782012\n",
            "  batch 1599 loss: 0.00013467410206794737\n",
            "  batch 1600 loss: 5.9983775019645694e-05\n",
            "  batch 1601 loss: 0.00012979818880558014\n",
            "  batch 1602 loss: 0.00011439305543899535\n",
            "  batch 1603 loss: 5.500604212284088e-05\n",
            "  batch 1604 loss: 0.00010020813345909119\n",
            "  batch 1605 loss: 8.576371520757675e-05\n",
            "  batch 1606 loss: 9.909317642450332e-05\n",
            "  batch 1607 loss: 7.340233027935028e-05\n",
            "  batch 1608 loss: 0.00011572574824094772\n",
            "  batch 1609 loss: 7.24009871482849e-05\n",
            "  batch 1610 loss: 9.932121634483337e-05\n",
            "  batch 1611 loss: 0.0001344260722398758\n",
            "  batch 1612 loss: 8.335881680250168e-05\n",
            "  batch 1613 loss: 0.00024227957427501678\n",
            "  batch 1614 loss: 0.0001179015338420868\n",
            "  batch 1615 loss: 0.00012018270045518875\n",
            "  batch 1616 loss: 8.56131911277771e-05\n",
            "  batch 1617 loss: 7.383569329977035e-05\n",
            "  batch 1618 loss: 8.013107627630234e-05\n",
            "  batch 1619 loss: 0.0001286959946155548\n",
            "  batch 1620 loss: 7.936741411685943e-05\n",
            "  batch 1621 loss: 9.74995419383049e-05\n",
            "  batch 1622 loss: 0.0001352473944425583\n",
            "  batch 1623 loss: 0.0001142098531126976\n",
            "  batch 1624 loss: 9.62805598974228e-05\n",
            "  batch 1625 loss: 0.00012084224075078964\n",
            "  batch 1626 loss: 0.00015987762808799744\n",
            "  batch 1627 loss: 0.00012737679481506346\n",
            "  batch 1628 loss: 9.630575776100159e-05\n",
            "  batch 1629 loss: 6.577765196561813e-05\n",
            "  batch 1630 loss: 0.00011737219244241715\n",
            "  batch 1631 loss: 0.00011173824965953828\n",
            "  batch 1632 loss: 9.731604158878327e-05\n",
            "  batch 1633 loss: 0.000150912806391716\n",
            "  batch 1634 loss: 8.464854210615157e-05\n",
            "  batch 1635 loss: 0.000100218765437603\n",
            "  batch 1636 loss: 0.00014836566150188446\n",
            "  batch 1637 loss: 8.970045298337936e-05\n",
            "  batch 1638 loss: 0.0001694125384092331\n",
            "  batch 1639 loss: 0.00014137613773345948\n",
            "  batch 1640 loss: 0.00013934320211410523\n",
            "  batch 1641 loss: 0.0001079116016626358\n",
            "  batch 1642 loss: 0.00011718776077032089\n",
            "  batch 1643 loss: 0.00010701580345630646\n",
            "  batch 1644 loss: 0.00011592568457126617\n",
            "  batch 1645 loss: 9.738211333751679e-05\n",
            "  batch 1646 loss: 9.030065685510636e-05\n",
            "  batch 1647 loss: 0.0001260996162891388\n",
            "  batch 1648 loss: 0.00011144229024648667\n",
            "  batch 1649 loss: 5.9742927551269534e-05\n",
            "  batch 1650 loss: 8.452197164297104e-05\n",
            "  batch 1651 loss: 8.654914796352387e-05\n",
            "  batch 1652 loss: 0.00011542023718357086\n",
            "  batch 1653 loss: 0.0001818954199552536\n",
            "  batch 1654 loss: 0.00014111575484275818\n",
            "  batch 1655 loss: 0.00014926791191101075\n",
            "  batch 1656 loss: 0.00010815320163965225\n",
            "  batch 1657 loss: 0.00010012403875589371\n",
            "  batch 1658 loss: 0.0001080741211771965\n",
            "  batch 1659 loss: 6.107692793011665e-05\n",
            "  batch 1660 loss: 0.00013520295917987822\n",
            "  batch 1661 loss: 0.00015106041729450226\n",
            "  batch 1662 loss: 5.7482365518808366e-05\n",
            "  batch 1663 loss: 7.466793060302735e-05\n",
            "  batch 1664 loss: 9.816628694534302e-05\n",
            "  batch 1665 loss: 0.00010745981335639954\n",
            "  batch 1666 loss: 6.570937484502792e-05\n",
            "  batch 1667 loss: 8.594397455453873e-05\n",
            "  batch 1668 loss: 0.00012270081788301468\n",
            "  batch 1669 loss: 0.00010388457030057907\n",
            "  batch 1670 loss: 7.12929517030716e-05\n",
            "  batch 1671 loss: 5.6031506508588794e-05\n",
            "  batch 1672 loss: 8.222671598196029e-05\n",
            "  batch 1673 loss: 0.00010153330862522125\n",
            "  batch 1674 loss: 0.00019128820300102234\n",
            "  batch 1675 loss: 0.00011695106327533723\n",
            "  batch 1676 loss: 0.00011053156852722168\n",
            "  batch 1677 loss: 0.00011288880556821823\n",
            "  batch 1678 loss: 0.00014133673906326294\n",
            "  batch 1679 loss: 0.0001161915585398674\n",
            "  batch 1680 loss: 0.00012372198700904847\n",
            "  batch 1681 loss: 0.00013583476841449738\n",
            "  batch 1682 loss: 4.810715466737747e-05\n",
            "  batch 1683 loss: 0.00017799416184425355\n",
            "  batch 1684 loss: 0.00010526042431592942\n",
            "  batch 1685 loss: 0.00013207992911338807\n",
            "  batch 1686 loss: 0.00014452600479125977\n",
            "  batch 1687 loss: 0.0001223573237657547\n",
            "  batch 1688 loss: 0.00011037031561136246\n",
            "  batch 1689 loss: 0.00012527048587799073\n",
            "  batch 1690 loss: 0.0001170583963394165\n",
            "  batch 1691 loss: 0.00012310697138309478\n",
            "  batch 1692 loss: 0.00011930157244205475\n",
            "  batch 1693 loss: 6.719090044498443e-05\n",
            "  batch 1694 loss: 0.00010737201571464538\n",
            "  batch 1695 loss: 7.016771286725998e-05\n",
            "  batch 1696 loss: 0.00012901215255260468\n",
            "  batch 1697 loss: 0.00016408129036426543\n",
            "  batch 1698 loss: 0.00011998401582241058\n",
            "  batch 1699 loss: 0.00014037573337554932\n",
            "  batch 1700 loss: 0.00012026460468769073\n",
            "  batch 1701 loss: 0.00015556226670742035\n",
            "  batch 1702 loss: 5.379370599985123e-05\n",
            "  batch 1703 loss: 0.00012877684831619262\n",
            "  batch 1704 loss: 9.591589868068695e-05\n",
            "  batch 1705 loss: 0.00012328637391328812\n",
            "  batch 1706 loss: 0.00017817582190036773\n",
            "  batch 1707 loss: 9.293837100267411e-05\n",
            "  batch 1708 loss: 0.00010453805327415467\n",
            "  batch 1709 loss: 6.203298643231392e-05\n",
            "  batch 1710 loss: 8.969583362340928e-05\n",
            "  batch 1711 loss: 8.478837460279464e-05\n",
            "  batch 1712 loss: 8.343802392482757e-05\n",
            "  batch 1713 loss: 0.00010568220168352127\n",
            "  batch 1714 loss: 0.00013674065470695496\n",
            "  batch 1715 loss: 0.00011469025164842606\n",
            "  batch 1716 loss: 0.0001770927757024765\n",
            "  batch 1717 loss: 9.006060659885406e-05\n",
            "  batch 1718 loss: 4.3066173791885374e-05\n",
            "  batch 1719 loss: 7.262726128101349e-05\n",
            "  batch 1720 loss: 6.403685361146926e-05\n",
            "  batch 1721 loss: 0.00011473500728607178\n",
            "  batch 1722 loss: 0.00013379813730716704\n",
            "  batch 1723 loss: 9.534607082605362e-05\n",
            "  batch 1724 loss: 7.102692127227783e-05\n",
            "  batch 1725 loss: 0.0001490176171064377\n",
            "  batch 1726 loss: 9.203936904668808e-05\n",
            "  batch 1727 loss: 0.00010579310357570648\n",
            "  batch 1728 loss: 7.807543873786926e-05\n",
            "  batch 1729 loss: 7.797805219888687e-05\n",
            "  batch 1730 loss: 6.719730794429779e-05\n",
            "  batch 1731 loss: 0.00014441671967506408\n",
            "  batch 1732 loss: 0.00010500360280275344\n",
            "  batch 1733 loss: 0.00013881267607212066\n",
            "  batch 1734 loss: 0.0001221054792404175\n",
            "  batch 1735 loss: 0.0001251177042722702\n",
            "  batch 1736 loss: 0.00011075370013713837\n",
            "  batch 1737 loss: 0.00016037201881408691\n",
            "  batch 1738 loss: 0.00010290888696908951\n",
            "  batch 1739 loss: 0.00010933469235897065\n",
            "  batch 1740 loss: 0.00014036360383033752\n",
            "  batch 1741 loss: 0.00013878670334815978\n",
            "  batch 1742 loss: 0.0001397663652896881\n",
            "  batch 1743 loss: 9.479473531246185e-05\n",
            "  batch 1744 loss: 0.0001268543154001236\n",
            "  batch 1745 loss: 0.00012324460595846177\n",
            "  batch 1746 loss: 8.077429980039597e-05\n",
            "  batch 1747 loss: 5.4982941597700116e-05\n",
            "  batch 1748 loss: 0.00013308236002922057\n",
            "  batch 1749 loss: 0.0001394570916891098\n",
            "  batch 1750 loss: 0.00012512741982936859\n",
            "  batch 1751 loss: 0.00013097348809242248\n",
            "  batch 1752 loss: 0.00011603008210659027\n",
            "  batch 1753 loss: 0.00020168142020702363\n",
            "  batch 1754 loss: 6.46292045712471e-05\n",
            "  batch 1755 loss: 7.811041921377182e-05\n",
            "  batch 1756 loss: 6.353002041578293e-05\n",
            "  batch 1757 loss: 6.425514817237854e-05\n",
            "  batch 1758 loss: 0.00013068601489067078\n",
            "  batch 1759 loss: 8.304689824581146e-05\n",
            "  batch 1760 loss: 8.456894010305405e-05\n",
            "  batch 1761 loss: 0.0002197374254465103\n",
            "  batch 1762 loss: 9.857556968927383e-05\n",
            "  batch 1763 loss: 5.091113969683647e-05\n",
            "  batch 1764 loss: 0.00010148624330759049\n",
            "  batch 1765 loss: 9.201844781637191e-05\n",
            "  batch 1766 loss: 9.650048613548278e-05\n",
            "  batch 1767 loss: 0.00014240856468677521\n",
            "  batch 1768 loss: 0.00012374810129404067\n",
            "  batch 1769 loss: 7.863078266382218e-05\n",
            "  batch 1770 loss: 0.00011387508362531662\n",
            "  batch 1771 loss: 7.571250200271606e-05\n",
            "  batch 1772 loss: 5.732414871454239e-05\n",
            "  batch 1773 loss: 8.241359889507294e-05\n",
            "  batch 1774 loss: 0.00010452933609485627\n",
            "  batch 1775 loss: 0.00014350062608718873\n",
            "  batch 1776 loss: 9.50586274266243e-05\n",
            "  batch 1777 loss: 7.618842273950576e-05\n",
            "  batch 1778 loss: 0.00012075036764144898\n",
            "  batch 1779 loss: 0.00010018762946128845\n",
            "  batch 1780 loss: 4.822175204753876e-05\n",
            "  batch 1781 loss: 0.00011051469296216965\n",
            "  batch 1782 loss: 8.473579585552216e-05\n",
            "  batch 1783 loss: 5.391737818717956e-05\n",
            "  batch 1784 loss: 0.00011584419757127761\n",
            "  batch 1785 loss: 0.00011348581314086915\n",
            "  batch 1786 loss: 6.730016320943832e-05\n",
            "  batch 1787 loss: 9.268879890441894e-05\n",
            "  batch 1788 loss: 0.00011174386739730835\n",
            "  batch 1789 loss: 5.363911390304565e-05\n",
            "  batch 1790 loss: 9.087974578142167e-05\n",
            "  batch 1791 loss: 0.00012596772611141204\n",
            "  batch 1792 loss: 8.456498384475709e-05\n",
            "  batch 1793 loss: 0.000128886342048645\n",
            "  batch 1794 loss: 0.00012564367055892944\n",
            "  batch 1795 loss: 0.00014896795153617858\n",
            "  batch 1796 loss: 7.10374116897583e-05\n",
            "  batch 1797 loss: 9.16803851723671e-05\n",
            "  batch 1798 loss: 7.86336287856102e-05\n",
            "  batch 1799 loss: 0.00010705548524856568\n",
            "  batch 1800 loss: 9.58600714802742e-05\n",
            "  batch 1801 loss: 3.5354241728782655e-05\n",
            "  batch 1802 loss: 8.519778400659561e-05\n",
            "  batch 1803 loss: 9.324454516172409e-05\n",
            "  batch 1804 loss: 0.00011321786046028137\n",
            "  batch 1805 loss: 0.00011671267449855804\n",
            "  batch 1806 loss: 0.00016168077290058136\n",
            "  batch 1807 loss: 4.9722008407115935e-05\n",
            "  batch 1808 loss: 0.0001368744969367981\n",
            "  batch 1809 loss: 5.455200374126434e-05\n",
            "  batch 1810 loss: 0.00010099505633115768\n",
            "  batch 1811 loss: 0.00011805635690689086\n",
            "  batch 1812 loss: 6.251903623342514e-05\n",
            "  batch 1813 loss: 0.00019019633531570435\n",
            "  batch 1814 loss: 8.679225295782089e-05\n",
            "  batch 1815 loss: 8.114979416131973e-05\n",
            "  batch 1816 loss: 0.00010981179773807526\n",
            "  batch 1817 loss: 9.029389917850494e-05\n",
            "  batch 1818 loss: 0.00012274297326803207\n",
            "  batch 1819 loss: 0.0001661308705806732\n",
            "  batch 1820 loss: 9.017746895551682e-05\n",
            "  batch 1821 loss: 9.821067005395889e-05\n",
            "  batch 1822 loss: 8.542701601982116e-05\n",
            "  batch 1823 loss: 9.089363366365433e-05\n",
            "  batch 1824 loss: 0.0001044861525297165\n",
            "  batch 1825 loss: 4.2086642235517504e-05\n",
            "  batch 1826 loss: 0.00010153789073228836\n",
            "  batch 1827 loss: 9.678741544485092e-05\n",
            "  batch 1828 loss: 8.962810784578324e-05\n",
            "  batch 1829 loss: 8.396559953689576e-05\n",
            "  batch 1830 loss: 0.00011071199923753739\n",
            "  batch 1831 loss: 0.0001599973142147064\n",
            "  batch 1832 loss: 0.00013976821303367614\n",
            "  batch 1833 loss: 0.0001447158604860306\n",
            "  batch 1834 loss: 0.0001413327306509018\n",
            "  batch 1835 loss: 0.0001564999222755432\n",
            "  batch 1836 loss: 7.515059411525726e-05\n",
            "  batch 1837 loss: 0.00010869528353214264\n",
            "  batch 1838 loss: 0.0001261250078678131\n",
            "  batch 1839 loss: 0.00011252572387456894\n",
            "  batch 1840 loss: 0.00012015361338853837\n",
            "  batch 1841 loss: 0.00011492430418729783\n",
            "  batch 1842 loss: 6.824059784412384e-05\n",
            "  batch 1843 loss: 9.31006297469139e-05\n",
            "  batch 1844 loss: 8.373300731182098e-05\n",
            "  batch 1845 loss: 0.0001073840707540512\n",
            "  batch 1846 loss: 0.0001336098313331604\n",
            "  batch 1847 loss: 0.0001454332172870636\n",
            "  batch 1848 loss: 0.00010868934541940689\n",
            "  batch 1849 loss: 8.210863173007965e-05\n",
            "  batch 1850 loss: 0.0001365789920091629\n",
            "  batch 1851 loss: 0.00012143470346927643\n",
            "  batch 1852 loss: 5.34767284989357e-05\n",
            "  batch 1853 loss: 6.474971771240234e-05\n",
            "  batch 1854 loss: 0.00011060131341218948\n",
            "  batch 1855 loss: 0.00011378861963748932\n",
            "  batch 1856 loss: 6.74067735671997e-05\n",
            "  batch 1857 loss: 0.00015203359723091125\n",
            "  batch 1858 loss: 8.636345714330673e-05\n",
            "  batch 1859 loss: 0.00014814454317092895\n",
            "  batch 1860 loss: 9.899827092885971e-05\n",
            "  batch 1861 loss: 8.178894966840744e-05\n",
            "  batch 1862 loss: 0.00015937869250774383\n",
            "  batch 1863 loss: 0.0001175880953669548\n",
            "  batch 1864 loss: 8.826719224452973e-05\n",
            "  batch 1865 loss: 6.50491863489151e-05\n",
            "  batch 1866 loss: 0.00012623611092567444\n",
            "  batch 1867 loss: 0.00012678390741348267\n",
            "  batch 1868 loss: 0.00012155112624168397\n",
            "  batch 1869 loss: 8.326294273138046e-05\n",
            "  batch 1870 loss: 0.00013107874989509583\n",
            "  batch 1871 loss: 0.00016950570046901702\n",
            "  batch 1872 loss: 9.589516371488571e-05\n",
            "  batch 1873 loss: 0.00010735758394002914\n",
            "  batch 1874 loss: 9.024206548929214e-05\n",
            "  batch 1875 loss: 0.00011542986333370209\n",
            "  batch 1876 loss: 5.6237615644931793e-05\n",
            "  batch 1877 loss: 7.435519248247146e-05\n",
            "  batch 1878 loss: 0.00011311506479978562\n",
            "  batch 1879 loss: 9.919459372758866e-05\n",
            "  batch 1880 loss: 8.393748849630355e-05\n",
            "  batch 1881 loss: 7.374310493469238e-05\n",
            "  batch 1882 loss: 0.00011067251861095428\n",
            "  batch 1883 loss: 2.8007708489894866e-05\n",
            "  batch 1884 loss: 0.00015953168272972107\n",
            "  batch 1885 loss: 0.00011016055941581726\n",
            "  batch 1886 loss: 9.463296085596085e-05\n",
            "  batch 1887 loss: 4.300883784890175e-05\n",
            "  batch 1888 loss: 0.00013395750522613527\n",
            "  batch 1889 loss: 0.00010835611075162887\n",
            "  batch 1890 loss: 0.00012034247070550919\n",
            "  batch 1891 loss: 9.370657056570054e-05\n",
            "  batch 1892 loss: 0.00011223045736551284\n",
            "  batch 1893 loss: 5.7654786854982376e-05\n",
            "  batch 1894 loss: 0.0001136099398136139\n",
            "  batch 1895 loss: 0.0001471250355243683\n",
            "  batch 1896 loss: 0.00011528302729129791\n",
            "  batch 1897 loss: 9.07534658908844e-05\n",
            "  batch 1898 loss: 6.533917784690857e-05\n",
            "  batch 1899 loss: 0.0001509072631597519\n",
            "  batch 1900 loss: 9.957466274499893e-05\n",
            "  batch 1901 loss: 0.00012871041893959046\n",
            "  batch 1902 loss: 0.0001070374995470047\n",
            "  batch 1903 loss: 0.0001298159658908844\n",
            "  batch 1904 loss: 8.892181515693665e-05\n",
            "  batch 1905 loss: 6.937447190284729e-05\n",
            "  batch 1906 loss: 7.231308519840241e-05\n",
            "  batch 1907 loss: 0.00010596388578414917\n",
            "  batch 1908 loss: 0.000136415496468544\n",
            "  batch 1909 loss: 8.551798760890961e-05\n",
            "  batch 1910 loss: 4.519042372703552e-05\n",
            "  batch 1911 loss: 0.00013408038020133972\n",
            "  batch 1912 loss: 0.00015381023287773133\n",
            "  batch 1913 loss: 0.0001158139631152153\n",
            "  batch 1914 loss: 0.00017682956159114838\n",
            "  batch 1915 loss: 8.937518298625946e-05\n",
            "  batch 1916 loss: 0.00010812090337276459\n",
            "  batch 1917 loss: 9.799943864345551e-05\n",
            "  batch 1918 loss: 9.277066588401795e-05\n",
            "  batch 1919 loss: 0.00011416976153850555\n",
            "  batch 1920 loss: 8.454815298318864e-05\n",
            "  batch 1921 loss: 7.482939213514328e-05\n",
            "  batch 1922 loss: 9.780531376600266e-05\n",
            "  batch 1923 loss: 0.00011565648764371872\n",
            "  batch 1924 loss: 0.00010561832040548325\n",
            "  batch 1925 loss: 0.00010396149754524231\n",
            "  batch 1926 loss: 0.00010406109690666198\n",
            "  batch 1927 loss: 0.00010951454192399978\n",
            "  batch 1928 loss: 9.266138076782226e-05\n",
            "  batch 1929 loss: 0.0001442730575799942\n",
            "  batch 1930 loss: 9.468704462051392e-05\n",
            "  batch 1931 loss: 8.672916144132614e-05\n",
            "  batch 1932 loss: 8.79855528473854e-05\n",
            "  batch 1933 loss: 0.00013521625101566316\n",
            "  batch 1934 loss: 0.00011332659423351288\n",
            "  batch 1935 loss: 0.00010827540606260299\n",
            "  batch 1936 loss: 9.939832240343094e-05\n",
            "  batch 1937 loss: 7.434252649545669e-05\n",
            "  batch 1938 loss: 8.516572415828705e-05\n",
            "  batch 1939 loss: 6.906350702047348e-05\n",
            "  batch 1940 loss: 0.00010994237661361695\n",
            "  batch 1941 loss: 5.714758485555649e-05\n",
            "  batch 1942 loss: 0.00011131385713815689\n",
            "  batch 1943 loss: 0.00014096887409687043\n",
            "  batch 1944 loss: 0.00013808533549308777\n",
            "  batch 1945 loss: 0.00012245890498161315\n",
            "  batch 1946 loss: 0.00010346648097038269\n",
            "  batch 1947 loss: 0.0001562800407409668\n",
            "  batch 1948 loss: 0.0001492777317762375\n",
            "  batch 1949 loss: 0.00015486863255500795\n",
            "  batch 1950 loss: 7.098773121833801e-05\n",
            "  batch 1951 loss: 7.840066403150558e-05\n",
            "  batch 1952 loss: 8.473597466945648e-05\n",
            "  batch 1953 loss: 0.00012668547034263612\n",
            "  batch 1954 loss: 9.862332791090011e-05\n",
            "  batch 1955 loss: 8.092758059501647e-05\n",
            "  batch 1956 loss: 0.00016182062029838562\n",
            "  batch 1957 loss: 6.622729450464249e-05\n",
            "  batch 1958 loss: 0.00017933472990989686\n",
            "loss tensor(0.1793, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "avg loss in epoch 0.00017933472990989686\n",
            "LOSS train 0.00017933472990989686 valid 0.10996326059103012\n",
            "EPOCH 2:\n",
            "  batch 1 loss: 9.448954463005066e-05\n",
            "  batch 2 loss: 0.00012631560862064363\n",
            "  batch 3 loss: 0.00014446189999580385\n",
            "  batch 4 loss: 9.73157063126564e-05\n",
            "  batch 5 loss: 8.308999985456467e-05\n",
            "  batch 6 loss: 5.618615448474884e-05\n",
            "  batch 7 loss: 0.0001608753502368927\n",
            "  batch 8 loss: 6.858066469430923e-05\n",
            "  batch 9 loss: 6.087779253721237e-05\n",
            "  batch 10 loss: 0.00011200924217700958\n",
            "  batch 11 loss: 5.8760203421115876e-05\n",
            "  batch 12 loss: 0.00011920479685068131\n",
            "  batch 13 loss: 0.00011375688016414643\n",
            "  batch 14 loss: 0.00010606064647436141\n",
            "  batch 15 loss: 0.00011978280544281006\n",
            "  batch 16 loss: 8.680254966020585e-05\n",
            "  batch 17 loss: 0.00012599731981754303\n",
            "  batch 18 loss: 6.120407953858376e-05\n",
            "  batch 19 loss: 6.890596449375153e-05\n",
            "  batch 20 loss: 0.0001271555721759796\n",
            "  batch 21 loss: 9.862641245126725e-05\n",
            "  batch 22 loss: 5.8970138430595396e-05\n",
            "  batch 23 loss: 0.00012913666665554047\n",
            "  batch 24 loss: 5.8549925684928895e-05\n",
            "  batch 25 loss: 0.00015246805548667909\n",
            "  batch 26 loss: 0.00012082146853208542\n",
            "  batch 27 loss: 8.908242732286454e-05\n",
            "  batch 28 loss: 8.748889714479446e-05\n",
            "  batch 29 loss: 7.524074614048004e-05\n",
            "  batch 30 loss: 0.00010160250216722488\n",
            "  batch 31 loss: 0.00011352168768644333\n",
            "  batch 32 loss: 0.00011244336515665054\n",
            "  batch 33 loss: 0.00015459398925304413\n",
            "  batch 34 loss: 0.00011241131275892258\n",
            "  batch 35 loss: 9.072026610374451e-05\n",
            "  batch 36 loss: 0.00014135445654392243\n",
            "  batch 37 loss: 0.00013676998019218444\n",
            "  batch 38 loss: 0.00011663863062858581\n",
            "  batch 39 loss: 8.580207079648971e-05\n",
            "  batch 40 loss: 9.550115466117859e-05\n",
            "  batch 41 loss: 8.97890105843544e-05\n",
            "  batch 42 loss: 0.00012662357091903687\n",
            "  batch 43 loss: 0.00013705042004585267\n",
            "  batch 44 loss: 9.65355709195137e-05\n",
            "  batch 45 loss: 0.00010697634518146514\n",
            "  batch 46 loss: 8.011604845523835e-05\n",
            "  batch 47 loss: 0.00015436987578868865\n",
            "  batch 48 loss: 0.00011554311960935592\n",
            "  batch 49 loss: 9.359256178140641e-05\n",
            "  batch 50 loss: 8.763235062360764e-05\n",
            "  batch 51 loss: 4.99175526201725e-05\n",
            "  batch 52 loss: 7.563092559576035e-05\n",
            "  batch 53 loss: 9.08735692501068e-05\n",
            "  batch 54 loss: 0.00015652869641780853\n",
            "  batch 55 loss: 9.55481305718422e-05\n",
            "  batch 56 loss: 0.00012135744839906692\n",
            "  batch 57 loss: 0.00015379725396633148\n",
            "  batch 58 loss: 0.000123667374253273\n",
            "  batch 59 loss: 9.444425255060196e-05\n",
            "  batch 60 loss: 0.00014688584208488465\n",
            "  batch 61 loss: 0.00011523386836051941\n",
            "  batch 62 loss: 0.0001269732117652893\n",
            "  batch 63 loss: 9.50266420841217e-05\n",
            "  batch 64 loss: 6.545602530241013e-05\n",
            "  batch 65 loss: 8.330126106739044e-05\n",
            "  batch 66 loss: 0.00010885143280029296\n",
            "  batch 67 loss: 0.000172528475522995\n",
            "  batch 68 loss: 0.00013268904387950898\n",
            "  batch 69 loss: 0.00011958616971969605\n",
            "  batch 70 loss: 0.00011628358066082\n",
            "  batch 71 loss: 0.0001334400475025177\n",
            "  batch 72 loss: 0.00011132261902093887\n",
            "  batch 73 loss: 7.691771537065506e-05\n",
            "  batch 74 loss: 0.0001156536489725113\n",
            "  batch 75 loss: 0.00014376375079154968\n",
            "  batch 76 loss: 5.4957188665866854e-05\n",
            "  batch 77 loss: 0.00011789970844984054\n",
            "  batch 78 loss: 6.442099809646607e-05\n",
            "  batch 79 loss: 0.00010456483066082\n",
            "  batch 80 loss: 0.00010348010808229446\n",
            "  batch 81 loss: 6.549033522605896e-05\n",
            "  batch 82 loss: 7.578453421592713e-05\n",
            "  batch 83 loss: 7.993374764919281e-05\n",
            "  batch 84 loss: 0.00015798135101795197\n",
            "  batch 85 loss: 0.00013847482204437257\n",
            "  batch 86 loss: 0.00016832683980464935\n",
            "  batch 87 loss: 9.876431524753571e-05\n",
            "  batch 88 loss: 0.00013464318215847016\n",
            "  batch 89 loss: 0.00011988952755928039\n",
            "  batch 90 loss: 7.343683391809463e-05\n",
            "  batch 91 loss: 0.0001284482479095459\n",
            "  batch 92 loss: 7.452358305454254e-05\n",
            "  batch 93 loss: 6.403513252735138e-05\n",
            "  batch 94 loss: 9.882311522960663e-05\n",
            "  batch 95 loss: 9.957493096590042e-05\n",
            "  batch 96 loss: 0.00012115276604890823\n",
            "  batch 97 loss: 0.00012918028235435486\n",
            "  batch 98 loss: 0.00010855438560247422\n",
            "  batch 99 loss: 0.00012255418300628662\n",
            "  batch 100 loss: 0.0001504647731781006\n",
            "  batch 101 loss: 0.00010764746367931366\n",
            "  batch 102 loss: 0.00010139451920986175\n",
            "  batch 103 loss: 8.41209515929222e-05\n",
            "  batch 104 loss: 7.026330381631851e-05\n",
            "  batch 105 loss: 8.351334184408188e-05\n",
            "  batch 106 loss: 8.78608375787735e-05\n",
            "  batch 107 loss: 5.0231762230396274e-05\n",
            "  batch 108 loss: 7.292919605970382e-05\n",
            "  batch 109 loss: 0.00010353817045688629\n",
            "  batch 110 loss: 0.00011714776605367661\n",
            "  batch 111 loss: 0.00018988721072673797\n",
            "  batch 112 loss: 5.999112129211426e-05\n",
            "  batch 113 loss: 7.438798248767853e-05\n",
            "  batch 114 loss: 6.807825714349746e-05\n",
            "  batch 115 loss: 0.00013961157202720643\n",
            "  batch 116 loss: 6.0956545174121854e-05\n",
            "  batch 117 loss: 7.204072177410126e-05\n",
            "  batch 118 loss: 8.981144428253174e-05\n",
            "  batch 119 loss: 0.00015418894588947296\n",
            "  batch 120 loss: 0.00011688026785850525\n",
            "  batch 121 loss: 0.00012763252854347228\n",
            "  batch 122 loss: 0.0001508384793996811\n",
            "  batch 123 loss: 0.00014200319349765779\n",
            "  batch 124 loss: 6.0372989624738694e-05\n",
            "  batch 125 loss: 8.935368061065674e-05\n",
            "  batch 126 loss: 8.040032535791397e-05\n",
            "  batch 127 loss: 0.00011686253547668457\n",
            "  batch 128 loss: 5.7517502456903455e-05\n",
            "  batch 129 loss: 6.743362545967102e-05\n",
            "  batch 130 loss: 0.0001371518075466156\n",
            "  batch 131 loss: 0.00011212198436260223\n",
            "  batch 132 loss: 0.00010316994041204453\n",
            "  batch 133 loss: 8.324738591909408e-05\n",
            "  batch 134 loss: 0.0001275317668914795\n",
            "  batch 135 loss: 9.375249594449996e-05\n",
            "  batch 136 loss: 0.00014179515838623046\n",
            "  batch 137 loss: 0.00010603830218315124\n",
            "  batch 138 loss: 9.504355490207673e-05\n",
            "  batch 139 loss: 7.6507106423378e-05\n",
            "  batch 140 loss: 8.619832247495652e-05\n",
            "  batch 141 loss: 9.05599445104599e-05\n",
            "  batch 142 loss: 0.00015570898354053497\n",
            "  batch 143 loss: 6.601457297801971e-05\n",
            "  batch 144 loss: 0.00014257261157035828\n",
            "  batch 145 loss: 0.00014675799012184144\n",
            "  batch 146 loss: 0.00010502902418375015\n",
            "  batch 147 loss: 0.00012868362665176391\n",
            "  batch 148 loss: 0.00010247199982404708\n",
            "  batch 149 loss: 0.00013520266115665437\n",
            "  batch 150 loss: 8.923988044261933e-05\n",
            "  batch 151 loss: 9.88880842924118e-05\n",
            "  batch 152 loss: 0.00014718075096607208\n",
            "  batch 153 loss: 5.87032288312912e-05\n",
            "  batch 154 loss: 6.98382630944252e-05\n",
            "  batch 155 loss: 8.597033470869065e-05\n",
            "  batch 156 loss: 0.00010528066009283066\n",
            "  batch 157 loss: 0.00011216852068901063\n",
            "  batch 158 loss: 5.526478588581085e-05\n",
            "  batch 159 loss: 0.0001402088850736618\n",
            "  batch 160 loss: 7.389730215072631e-05\n",
            "  batch 161 loss: 0.00010041500627994537\n",
            "  batch 162 loss: 5.624504014849663e-05\n",
            "  batch 163 loss: 6.733762472867966e-05\n",
            "  batch 164 loss: 0.00014443077147006987\n",
            "  batch 165 loss: 0.00013047778606414795\n",
            "  batch 166 loss: 0.00010474496334791183\n",
            "  batch 167 loss: 0.00014433230459690095\n",
            "  batch 168 loss: 7.915562391281128e-05\n",
            "  batch 169 loss: 0.00015161609649658203\n",
            "  batch 170 loss: 0.00010271789133548736\n",
            "  batch 171 loss: 9.820356965065002e-05\n",
            "  batch 172 loss: 0.00012586379051208495\n",
            "  batch 173 loss: 4.7058075666427614e-05\n",
            "  batch 174 loss: 0.00012562914192676544\n",
            "  batch 175 loss: 7.484549283981324e-05\n",
            "  batch 176 loss: 7.168471813201905e-05\n",
            "  batch 177 loss: 0.00010799958556890488\n",
            "  batch 178 loss: 7.240066677331924e-05\n",
            "  batch 179 loss: 6.808633357286453e-05\n",
            "  batch 180 loss: 0.00013914410769939422\n",
            "  batch 181 loss: 6.72333985567093e-05\n",
            "  batch 182 loss: 0.00011835862696170806\n",
            "  batch 183 loss: 0.0001188802644610405\n",
            "  batch 184 loss: 9.295953065156936e-05\n",
            "  batch 185 loss: 0.00011450675874948501\n",
            "  batch 186 loss: 8.156555145978927e-05\n",
            "  batch 187 loss: 0.00013716210424900054\n",
            "  batch 188 loss: 9.495604038238525e-05\n",
            "  batch 189 loss: 0.00015255774557590486\n",
            "  batch 190 loss: 9.00108590722084e-05\n",
            "  batch 191 loss: 9.791368246078492e-05\n",
            "  batch 192 loss: 0.00010982136428356171\n",
            "  batch 193 loss: 8.256285637617111e-05\n",
            "  batch 194 loss: 0.00010254678130149841\n",
            "  batch 195 loss: 7.399088144302369e-05\n",
            "  batch 196 loss: 8.117765933275223e-05\n",
            "  batch 197 loss: 9.983948618173599e-05\n",
            "  batch 198 loss: 0.00010429542511701584\n",
            "  batch 199 loss: 0.00010094189643859863\n",
            "  batch 200 loss: 0.00012982338666915892\n",
            "  batch 201 loss: 7.716356217861176e-05\n",
            "  batch 202 loss: 7.147298753261565e-05\n",
            "  batch 203 loss: 9.302125871181489e-05\n",
            "  batch 204 loss: 0.00014343148469924926\n",
            "  batch 205 loss: 0.00010083867609500885\n",
            "  batch 206 loss: 8.784456551074982e-05\n",
            "  batch 207 loss: 0.00016063065826892853\n",
            "  batch 208 loss: 0.00011220061779022217\n",
            "  batch 209 loss: 7.184510678052903e-05\n",
            "  batch 210 loss: 0.00010893383622169495\n",
            "  batch 211 loss: 0.00012040035426616668\n",
            "  batch 212 loss: 0.00011258254200220108\n",
            "  batch 213 loss: 0.0001131972223520279\n",
            "  batch 214 loss: 9.328994154930115e-05\n",
            "  batch 215 loss: 0.00011408314108848571\n",
            "  batch 216 loss: 0.0001381750851869583\n",
            "  batch 217 loss: 5.212826654314995e-05\n",
            "  batch 218 loss: 0.00011060149222612381\n",
            "  batch 219 loss: 0.000124128557741642\n",
            "  batch 220 loss: 0.00016117189824581146\n",
            "  batch 221 loss: 0.00011409669369459153\n",
            "  batch 222 loss: 0.00012746165692806245\n",
            "  batch 223 loss: 0.00012206664681434631\n",
            "  batch 224 loss: 4.931189864873886e-05\n",
            "  batch 225 loss: 0.00012554650008678435\n",
            "  batch 226 loss: 8.167777955532074e-05\n",
            "  batch 227 loss: 8.73979926109314e-05\n",
            "  batch 228 loss: 0.00010878578573465347\n",
            "  batch 229 loss: 9.147540479898452e-05\n",
            "  batch 230 loss: 0.00012049681693315505\n",
            "  batch 231 loss: 0.0001600448489189148\n",
            "  batch 232 loss: 0.00017507246136665345\n",
            "  batch 233 loss: 0.00015217176079750062\n",
            "  batch 234 loss: 0.00010483709722757339\n",
            "  batch 235 loss: 0.0001089482307434082\n",
            "  batch 236 loss: 9.105750173330308e-05\n",
            "  batch 237 loss: 6.32084384560585e-05\n",
            "  batch 238 loss: 8.74277874827385e-05\n",
            "  batch 239 loss: 8.545216917991638e-05\n",
            "  batch 240 loss: 0.00014035609364509584\n",
            "  batch 241 loss: 9.016328305006028e-05\n",
            "  batch 242 loss: 0.0001766373962163925\n",
            "  batch 243 loss: 9.560267627239228e-05\n",
            "  batch 244 loss: 9.932325035333634e-05\n",
            "  batch 245 loss: 8.71891900897026e-05\n",
            "  batch 246 loss: 0.00013944613933563233\n",
            "  batch 247 loss: 7.349073886871338e-05\n",
            "  batch 248 loss: 0.00010479892790317535\n",
            "  batch 249 loss: 8.525164425373077e-05\n",
            "  batch 250 loss: 8.571315556764603e-05\n",
            "  batch 251 loss: 5.320351198315621e-05\n",
            "  batch 252 loss: 8.909858763217926e-05\n",
            "  batch 253 loss: 7.203718274831771e-05\n",
            "  batch 254 loss: 0.00010780371725559235\n",
            "  batch 255 loss: 7.988329231739045e-05\n",
            "  batch 256 loss: 7.203073054552078e-05\n",
            "  batch 257 loss: 0.00010378754138946534\n",
            "  batch 258 loss: 4.747078195214272e-05\n",
            "  batch 259 loss: 0.00012107744812965393\n",
            "  batch 260 loss: 0.00015309308469295502\n",
            "  batch 261 loss: 9.378840774297714e-05\n",
            "  batch 262 loss: 8.593039214611053e-05\n",
            "  batch 263 loss: 0.00010892783105373383\n",
            "  batch 264 loss: 0.00012282381951808928\n",
            "  batch 265 loss: 0.0001688395291566849\n",
            "  batch 266 loss: 9.125122427940368e-05\n",
            "  batch 267 loss: 7.818525284528732e-05\n",
            "  batch 268 loss: 8.002730458974839e-05\n",
            "  batch 269 loss: 0.00010066306591033936\n",
            "  batch 270 loss: 5.688219517469406e-05\n",
            "  batch 271 loss: 8.479907363653183e-05\n",
            "  batch 272 loss: 0.00011451710015535355\n",
            "  batch 273 loss: 0.00013890951871871947\n",
            "  batch 274 loss: 0.00012573061883449553\n",
            "  batch 275 loss: 0.00016500267386436462\n",
            "  batch 276 loss: 7.371573150157929e-05\n",
            "  batch 277 loss: 6.753292679786682e-05\n",
            "  batch 278 loss: 0.00011733198910951614\n",
            "  batch 279 loss: 0.0001534750610589981\n",
            "  batch 280 loss: 0.00010825531184673309\n",
            "  batch 281 loss: 9.416463226079941e-05\n",
            "  batch 282 loss: 9.478627890348434e-05\n",
            "  batch 283 loss: 5.614995211362839e-05\n",
            "  batch 284 loss: 0.00011004888266324997\n",
            "  batch 285 loss: 6.255868077278137e-05\n",
            "  batch 286 loss: 0.00013449572026729583\n",
            "  batch 287 loss: 9.622466564178467e-05\n",
            "  batch 288 loss: 6.763333827257157e-05\n",
            "  batch 289 loss: 8.240016549825668e-05\n",
            "  batch 290 loss: 7.887376844882965e-05\n",
            "  batch 291 loss: 9.872616827487945e-05\n",
            "  batch 292 loss: 5.652899295091629e-05\n",
            "  batch 293 loss: 0.00020520970225334167\n",
            "  batch 294 loss: 7.986461371183395e-05\n",
            "  batch 295 loss: 0.0001627453863620758\n",
            "  batch 296 loss: 7.627347856760025e-05\n",
            "  batch 297 loss: 0.0001386078745126724\n",
            "  batch 298 loss: 4.2771410197019575e-05\n",
            "  batch 299 loss: 7.76231437921524e-05\n",
            "  batch 300 loss: 0.00011325696855783463\n",
            "  batch 301 loss: 0.00010975982993841171\n",
            "  batch 302 loss: 6.362169981002808e-05\n",
            "  batch 303 loss: 0.00010864575207233429\n",
            "  batch 304 loss: 0.0001417803019285202\n",
            "  batch 305 loss: 8.595630526542664e-05\n",
            "  batch 306 loss: 0.0001568562090396881\n",
            "  batch 307 loss: 6.882470846176147e-05\n",
            "  batch 308 loss: 8.674896508455276e-05\n",
            "  batch 309 loss: 0.0001348564177751541\n",
            "  batch 310 loss: 0.00010886271297931672\n",
            "  batch 311 loss: 5.4263688623905184e-05\n",
            "  batch 312 loss: 0.0001065681129693985\n",
            "  batch 313 loss: 9.121186286211014e-05\n",
            "  batch 314 loss: 0.00014747348427772522\n",
            "  batch 315 loss: 9.923241287469864e-05\n",
            "  batch 316 loss: 7.274962216615677e-05\n",
            "  batch 317 loss: 8.96669402718544e-05\n",
            "  batch 318 loss: 9.125236421823501e-05\n",
            "  batch 319 loss: 8.114652335643768e-05\n",
            "  batch 320 loss: 9.917498379945754e-05\n",
            "  batch 321 loss: 0.00012190565466880798\n",
            "  batch 322 loss: 7.856949418783188e-05\n",
            "  batch 323 loss: 0.00010584469139575959\n",
            "  batch 324 loss: 0.00010266271233558655\n",
            "  batch 325 loss: 4.914651066064834e-05\n",
            "  batch 326 loss: 0.00010145962983369828\n",
            "  batch 327 loss: 0.00012166574597358704\n",
            "  batch 328 loss: 0.00010849171876907349\n",
            "  batch 329 loss: 0.00012069679051637649\n",
            "  batch 330 loss: 8.797844499349594e-05\n",
            "  batch 331 loss: 9.670319408178329e-05\n",
            "  batch 332 loss: 5.124238133430481e-05\n",
            "  batch 333 loss: 0.00011300212889909744\n",
            "  batch 334 loss: 0.00010167571157217026\n",
            "  batch 335 loss: 0.00012892821431159974\n",
            "  batch 336 loss: 0.0001246696785092354\n",
            "  batch 337 loss: 5.376550927758217e-05\n",
            "  batch 338 loss: 0.00016431289911270143\n",
            "  batch 339 loss: 0.00013839221000671385\n",
            "  batch 340 loss: 7.902026921510697e-05\n",
            "  batch 341 loss: 0.00011612706631422043\n",
            "  batch 342 loss: 0.0001066739484667778\n",
            "  batch 343 loss: 0.0001062374785542488\n",
            "  batch 344 loss: 8.437053859233857e-05\n",
            "  batch 345 loss: 0.00011285693198442459\n",
            "  batch 346 loss: 0.00012222052365541458\n",
            "  batch 347 loss: 7.560566067695618e-05\n",
            "  batch 348 loss: 8.79267230629921e-05\n",
            "  batch 349 loss: 6.839288026094437e-05\n",
            "  batch 350 loss: 6.833329051733017e-05\n",
            "  batch 351 loss: 7.80281126499176e-05\n",
            "  batch 352 loss: 0.00012196704745292663\n",
            "  batch 353 loss: 0.00011496428400278092\n",
            "  batch 354 loss: 0.0001005222126841545\n",
            "  batch 355 loss: 0.00011878924816846847\n",
            "  batch 356 loss: 8.530282974243164e-05\n",
            "  batch 357 loss: 0.00016954576969146728\n",
            "  batch 358 loss: 0.00010759712010622025\n",
            "  batch 359 loss: 0.0001193780079483986\n",
            "  batch 360 loss: 8.823302388191223e-05\n",
            "  batch 361 loss: 0.00010500707477331161\n",
            "  batch 362 loss: 0.0001624254286289215\n",
            "  batch 363 loss: 0.00011705272644758224\n",
            "  batch 364 loss: 0.0001337190419435501\n",
            "  batch 365 loss: 9.90750715136528e-05\n",
            "  batch 366 loss: 0.00011265913397073745\n",
            "  batch 367 loss: 8.52242037653923e-05\n",
            "  batch 368 loss: 8.363266289234162e-05\n",
            "  batch 369 loss: 7.439622282981872e-05\n",
            "  batch 370 loss: 7.406443357467652e-05\n",
            "  batch 371 loss: 0.0001080363392829895\n",
            "  batch 372 loss: 0.00014745020866394043\n",
            "  batch 373 loss: 8.330864459276199e-05\n",
            "  batch 374 loss: 6.0798447579145434e-05\n",
            "  batch 375 loss: 9.833449125289917e-05\n",
            "  batch 376 loss: 7.166952639818191e-05\n",
            "  batch 377 loss: 0.00012568800151348115\n",
            "  batch 378 loss: 0.00012164800614118576\n",
            "  batch 379 loss: 0.0001169336810708046\n",
            "  batch 380 loss: 0.0001262846738100052\n",
            "  batch 381 loss: 0.00012167520821094512\n",
            "  batch 382 loss: 0.00011445386707782745\n",
            "  batch 383 loss: 0.0001584867686033249\n",
            "  batch 384 loss: 0.000128644660115242\n",
            "  batch 385 loss: 0.00011085010319948197\n",
            "  batch 386 loss: 8.716855943202973e-05\n",
            "  batch 387 loss: 0.00014949868619441986\n",
            "  batch 388 loss: 0.00011615995317697525\n",
            "  batch 389 loss: 6.737422198057175e-05\n",
            "  batch 390 loss: 0.00011563920229673386\n",
            "  batch 391 loss: 0.0001303349584341049\n",
            "  batch 392 loss: 6.662476062774658e-05\n",
            "  batch 393 loss: 9.773749113082885e-05\n",
            "  batch 394 loss: 7.984982430934906e-05\n",
            "  batch 395 loss: 9.720482677221299e-05\n",
            "  batch 396 loss: 0.0001385403722524643\n",
            "  batch 397 loss: 0.0001058659479022026\n",
            "  batch 398 loss: 7.158129662275314e-05\n",
            "  batch 399 loss: 0.00011035415530204773\n",
            "  batch 400 loss: 0.00013738128542900086\n",
            "  batch 401 loss: 7.766066491603851e-05\n",
            "  batch 402 loss: 6.992422044277191e-05\n",
            "  batch 403 loss: 0.00011073169857263564\n",
            "  batch 404 loss: 0.0001150229126214981\n",
            "  batch 405 loss: 7.530298829078674e-05\n",
            "  batch 406 loss: 0.00011349081248044967\n",
            "  batch 407 loss: 9.820597618818283e-05\n",
            "  batch 408 loss: 9.667160362005234e-05\n",
            "  batch 409 loss: 0.00010213768482208252\n",
            "  batch 410 loss: 6.764917075634003e-05\n",
            "  batch 411 loss: 8.847201615571976e-05\n",
            "  batch 412 loss: 8.876533806324005e-05\n",
            "  batch 413 loss: 7.267803698778153e-05\n",
            "  batch 414 loss: 0.00012188258022069931\n",
            "  batch 415 loss: 6.715806573629379e-05\n",
            "  batch 416 loss: 8.872320502996445e-05\n",
            "  batch 417 loss: 8.430266380310059e-05\n",
            "  batch 418 loss: 3.839413076639175e-05\n",
            "  batch 419 loss: 0.00011057253181934357\n",
            "  batch 420 loss: 5.798646807670593e-05\n",
            "  batch 421 loss: 6.373956054449081e-05\n",
            "  batch 422 loss: 0.00017782802879810333\n",
            "  batch 423 loss: 8.640886098146439e-05\n",
            "  batch 424 loss: 0.00012300869077444077\n",
            "  batch 425 loss: 8.700619637966156e-05\n",
            "  batch 426 loss: 0.00012251994013786317\n",
            "  batch 427 loss: 0.0001483926475048065\n",
            "  batch 428 loss: 4.131175205111504e-05\n",
            "  batch 429 loss: 0.00012302396446466445\n",
            "  batch 430 loss: 9.61834341287613e-05\n",
            "  batch 431 loss: 9.171917289495468e-05\n",
            "  batch 432 loss: 6.0390625149011614e-05\n",
            "  batch 433 loss: 9.213070571422577e-05\n",
            "  batch 434 loss: 0.0001258849799633026\n",
            "  batch 435 loss: 9.772163629531861e-05\n",
            "  batch 436 loss: 5.5602658540010454e-05\n",
            "  batch 437 loss: 9.730068594217301e-05\n",
            "  batch 438 loss: 7.262542843818665e-05\n",
            "  batch 439 loss: 8.968329429626465e-05\n",
            "  batch 440 loss: 0.00013708843290805817\n",
            "  batch 441 loss: 9.661673754453659e-05\n",
            "  batch 442 loss: 5.081195011734962e-05\n",
            "  batch 443 loss: 7.117626070976257e-05\n",
            "  batch 444 loss: 8.367104083299637e-05\n",
            "  batch 445 loss: 8.897320926189422e-05\n",
            "  batch 446 loss: 0.00012223900109529495\n",
            "  batch 447 loss: 0.00016011802852153778\n",
            "  batch 448 loss: 0.00013217850029468537\n",
            "  batch 449 loss: 9.597297757863998e-05\n",
            "  batch 450 loss: 0.00011836589872837066\n",
            "  batch 451 loss: 0.00015182781219482422\n",
            "  batch 452 loss: 5.410664901137352e-05\n",
            "  batch 453 loss: 0.00010492388904094697\n",
            "  batch 454 loss: 8.665193617343903e-05\n",
            "  batch 455 loss: 8.315809071063996e-05\n",
            "  batch 456 loss: 0.0001672002077102661\n",
            "  batch 457 loss: 0.0001519351303577423\n",
            "  batch 458 loss: 0.00010697923600673676\n",
            "  batch 459 loss: 0.00011203087121248246\n",
            "  batch 460 loss: 0.00010505029559135436\n",
            "  batch 461 loss: 0.00011070422083139419\n",
            "  batch 462 loss: 6.0364346951246263e-05\n",
            "  batch 463 loss: 0.00011622126400470733\n",
            "  batch 464 loss: 0.00015113866329193116\n",
            "  batch 465 loss: 0.00011409030109643937\n",
            "  batch 466 loss: 0.00018069371581077575\n",
            "  batch 467 loss: 7.56835862994194e-05\n",
            "  batch 468 loss: 0.0001364191472530365\n",
            "  batch 469 loss: 8.47841277718544e-05\n",
            "  batch 470 loss: 6.698233634233475e-05\n",
            "  batch 471 loss: 0.00010374560207128525\n",
            "  batch 472 loss: 0.00012627363204956055\n",
            "  batch 473 loss: 7.594382762908935e-05\n",
            "  batch 474 loss: 8.691733330488205e-05\n",
            "  batch 475 loss: 0.00011298319697380066\n",
            "  batch 476 loss: 0.0001216784492135048\n",
            "  batch 477 loss: 0.00010548224300146103\n",
            "  batch 478 loss: 0.0001220836490392685\n",
            "  batch 479 loss: 8.443149924278259e-05\n",
            "  batch 480 loss: 0.00015081222355365752\n",
            "  batch 481 loss: 0.00015099136531352997\n",
            "  batch 482 loss: 0.00011483806371688843\n",
            "  batch 483 loss: 7.858290523290634e-05\n",
            "  batch 484 loss: 5.1245246082544324e-05\n",
            "  batch 485 loss: 4.91739995777607e-05\n",
            "  batch 486 loss: 0.00013954558968544005\n",
            "  batch 487 loss: 9.590787440538407e-05\n",
            "  batch 488 loss: 0.0001429336816072464\n",
            "  batch 489 loss: 9.57343652844429e-05\n",
            "  batch 490 loss: 0.00011425959318876267\n",
            "  batch 491 loss: 8.321090042591095e-05\n",
            "  batch 492 loss: 8.106189966201782e-05\n",
            "  batch 493 loss: 0.00012881667912006378\n",
            "  batch 494 loss: 8.049465715885162e-05\n",
            "  batch 495 loss: 0.0001290712058544159\n",
            "  batch 496 loss: 0.0001332540810108185\n",
            "  batch 497 loss: 6.057313457131386e-05\n",
            "  batch 498 loss: 5.317294970154762e-05\n",
            "  batch 499 loss: 5.82064762711525e-05\n",
            "  batch 500 loss: 8.44656080007553e-05\n",
            "  batch 501 loss: 8.742888271808624e-05\n",
            "  batch 502 loss: 4.2141143232584e-05\n",
            "  batch 503 loss: 0.00011410935968160629\n",
            "  batch 504 loss: 0.00010067079961299897\n",
            "  batch 505 loss: 0.00011261527240276337\n",
            "  batch 506 loss: 9.256292879581451e-05\n",
            "  batch 507 loss: 0.00013823093473911286\n",
            "  batch 508 loss: 5.6884951889514923e-05\n",
            "  batch 509 loss: 0.0001529976725578308\n",
            "  batch 510 loss: 0.00012085293233394623\n",
            "  batch 511 loss: 0.0001346866935491562\n",
            "  batch 512 loss: 0.00010427653044462204\n",
            "  batch 513 loss: 4.47070524096489e-05\n",
            "  batch 514 loss: 8.271067589521408e-05\n",
            "  batch 515 loss: 0.00011691565066576004\n",
            "  batch 516 loss: 7.700473070144653e-05\n",
            "  batch 517 loss: 0.0001345885843038559\n",
            "  batch 518 loss: 0.00011796110868453979\n",
            "  batch 519 loss: 9.938385337591172e-05\n",
            "  batch 520 loss: 5.566045269370079e-05\n",
            "  batch 521 loss: 0.00018278646469116212\n",
            "  batch 522 loss: 7.790592312812805e-05\n",
            "  batch 523 loss: 6.992653012275695e-05\n",
            "  batch 524 loss: 0.0001254308968782425\n",
            "  batch 525 loss: 0.00010067582875490188\n",
            "  batch 526 loss: 6.162279844284057e-05\n",
            "  batch 527 loss: 8.056296408176421e-05\n",
            "  batch 528 loss: 0.00010740332305431366\n",
            "  batch 529 loss: 0.00012603063881397246\n",
            "  batch 530 loss: 7.577062398195267e-05\n",
            "  batch 531 loss: 0.00011153142154216767\n",
            "  batch 532 loss: 0.00011496399343013764\n",
            "  batch 533 loss: 7.822783291339874e-05\n",
            "  batch 534 loss: 8.514165133237839e-05\n",
            "  batch 535 loss: 0.00014108632504940033\n",
            "  batch 536 loss: 9.253963083028794e-05\n",
            "  batch 537 loss: 0.00012798789143562316\n",
            "  batch 538 loss: 9.120187163352966e-05\n",
            "  batch 539 loss: 4.789569228887558e-05\n",
            "  batch 540 loss: 0.00010909242928028107\n",
            "  batch 541 loss: 0.00011187198758125305\n",
            "  batch 542 loss: 0.00010751575976610184\n",
            "  batch 543 loss: 7.310554385185241e-05\n",
            "  batch 544 loss: 0.00012349189817905426\n",
            "  batch 545 loss: 8.383511751890182e-05\n",
            "  batch 546 loss: 0.00010226356983184815\n",
            "  batch 547 loss: 5.78409917652607e-05\n",
            "  batch 548 loss: 7.36643522977829e-05\n",
            "  batch 549 loss: 5.7434182614088055e-05\n",
            "  batch 550 loss: 8.946380764245988e-05\n",
            "  batch 551 loss: 8.355911076068878e-05\n",
            "  batch 552 loss: 0.00010353410243988036\n",
            "  batch 553 loss: 6.0360062867403034e-05\n",
            "  batch 554 loss: 8.657589554786682e-05\n",
            "  batch 555 loss: 8.177652955055237e-05\n",
            "  batch 556 loss: 8.952806890010834e-05\n",
            "  batch 557 loss: 0.00010494694113731384\n",
            "  batch 558 loss: 5.1968485116958616e-05\n",
            "  batch 559 loss: 0.00010618893802165985\n",
            "  batch 560 loss: 8.475389331579209e-05\n",
            "  batch 561 loss: 0.00010030783712863922\n",
            "  batch 562 loss: 0.00010464056581258774\n",
            "  batch 563 loss: 5.28012216091156e-05\n",
            "  batch 564 loss: 0.00011699815094470978\n",
            "  batch 565 loss: 5.247456952929497e-05\n",
            "  batch 566 loss: 0.00013945585489273072\n",
            "  batch 567 loss: 0.00012307578325271607\n",
            "  batch 568 loss: 7.224438339471818e-05\n",
            "  batch 569 loss: 0.00010915815830230712\n",
            "  batch 570 loss: 8.806253224611282e-05\n",
            "  batch 571 loss: 0.0001137097179889679\n",
            "  batch 572 loss: 0.00011087910830974578\n",
            "  batch 573 loss: 8.632078766822815e-05\n",
            "  batch 574 loss: 8.580020815134048e-05\n",
            "  batch 575 loss: 6.989020109176636e-05\n",
            "  batch 576 loss: 0.00011707238107919693\n",
            "  batch 577 loss: 0.00011296585202217102\n",
            "  batch 578 loss: 9.702014178037644e-05\n",
            "  batch 579 loss: 7.49065801501274e-05\n",
            "  batch 580 loss: 9.45686548948288e-05\n",
            "  batch 581 loss: 6.345858424901962e-05\n",
            "  batch 582 loss: 0.00011209524422883987\n",
            "  batch 583 loss: 8.22223499417305e-05\n",
            "  batch 584 loss: 0.00014283932745456695\n",
            "  batch 585 loss: 9.107618033885956e-05\n",
            "  batch 586 loss: 6.323674321174622e-05\n",
            "  batch 587 loss: 0.00010587425529956817\n",
            "  batch 588 loss: 0.00015513801574707032\n",
            "  batch 589 loss: 0.00011463239043951035\n",
            "  batch 590 loss: 8.93532708287239e-05\n",
            "  batch 591 loss: 0.0001143438220024109\n",
            "  batch 592 loss: 0.00017235112190246583\n",
            "  batch 593 loss: 7.140746712684631e-05\n",
            "  batch 594 loss: 0.00017892281711101533\n",
            "  batch 595 loss: 0.00011533771455287933\n",
            "  batch 596 loss: 0.00013467006385326386\n",
            "  batch 597 loss: 9.542775154113769e-05\n",
            "  batch 598 loss: 0.00013460642099380493\n",
            "  batch 599 loss: 9.970363974571227e-05\n",
            "  batch 600 loss: 0.00012906254827976227\n",
            "  batch 601 loss: 9.397589415311813e-05\n",
            "  batch 602 loss: 0.00013922716677188874\n",
            "  batch 603 loss: 8.372433483600617e-05\n",
            "  batch 604 loss: 0.00016800680756568907\n",
            "  batch 605 loss: 0.0001253724545240402\n",
            "  batch 606 loss: 9.105612337589264e-05\n",
            "  batch 607 loss: 9.936287254095078e-05\n",
            "  batch 608 loss: 0.00018612290918827056\n",
            "  batch 609 loss: 0.00011739446967840195\n",
            "  batch 610 loss: 0.00010821221023797988\n",
            "  batch 611 loss: 6.284260004758835e-05\n",
            "  batch 612 loss: 8.598344773054123e-05\n",
            "  batch 613 loss: 6.957996636629104e-05\n",
            "  batch 614 loss: 8.905419707298279e-05\n",
            "  batch 615 loss: 8.385071158409119e-05\n",
            "  batch 616 loss: 9.996725618839264e-05\n",
            "  batch 617 loss: 8.834274113178253e-05\n",
            "  batch 618 loss: 0.00011166871339082718\n",
            "  batch 619 loss: 7.901984453201294e-05\n",
            "  batch 620 loss: 0.00013749802112579345\n",
            "  batch 621 loss: 7.912545651197434e-05\n",
            "  batch 622 loss: 7.773555815219879e-05\n",
            "  batch 623 loss: 4.817698895931244e-05\n",
            "  batch 624 loss: 9.348391741514206e-05\n",
            "  batch 625 loss: 0.00011370021849870681\n",
            "  batch 626 loss: 8.270140737295151e-05\n",
            "  batch 627 loss: 0.00011571171134710312\n",
            "  batch 628 loss: 0.00010655514150857925\n",
            "  batch 629 loss: 0.00012909919023513795\n",
            "  batch 630 loss: 7.624352723360062e-05\n",
            "  batch 631 loss: 6.624109297990798e-05\n",
            "  batch 632 loss: 0.00012921342253684998\n",
            "  batch 633 loss: 0.0001459757834672928\n",
            "  batch 634 loss: 7.152532041072846e-05\n",
            "  batch 635 loss: 6.847762316465378e-05\n",
            "  batch 636 loss: 8.922908455133438e-05\n",
            "  batch 637 loss: 6.829657405614853e-05\n",
            "  batch 638 loss: 0.00012752696871757508\n",
            "  batch 639 loss: 8.38409960269928e-05\n",
            "  batch 640 loss: 0.00011884032189846038\n",
            "  batch 641 loss: 0.00014255766570568084\n",
            "  batch 642 loss: 7.570675760507584e-05\n",
            "  batch 643 loss: 0.0001410827934741974\n",
            "  batch 644 loss: 0.00014223988354206086\n",
            "  batch 645 loss: 9.795831143856049e-05\n",
            "  batch 646 loss: 8.855140954256058e-05\n",
            "  batch 647 loss: 0.0001563057452440262\n",
            "  batch 648 loss: 0.00013594615459442137\n",
            "  batch 649 loss: 0.00010652860254049301\n",
            "  batch 650 loss: 8.658866584300995e-05\n",
            "  batch 651 loss: 6.281156092882156e-05\n",
            "  batch 652 loss: 5.6378830224275586e-05\n",
            "  batch 653 loss: 0.00010803350061178208\n",
            "  batch 654 loss: 8.196673542261123e-05\n",
            "  batch 655 loss: 5.840874463319778e-05\n",
            "  batch 656 loss: 9.637300670146942e-05\n",
            "  batch 657 loss: 8.367765694856643e-05\n",
            "  batch 658 loss: 6.007128581404686e-05\n",
            "  batch 659 loss: 0.0001658475548028946\n",
            "  batch 660 loss: 0.0001063123419880867\n",
            "  batch 661 loss: 6.741944700479508e-05\n",
            "  batch 662 loss: 0.00011911014467477798\n",
            "  batch 663 loss: 0.00011119793355464936\n",
            "  batch 664 loss: 9.488211572170257e-05\n",
            "  batch 665 loss: 7.673457264900207e-05\n",
            "  batch 666 loss: 0.00013708275556564332\n",
            "  batch 667 loss: 6.0170628130435945e-05\n",
            "  batch 668 loss: 0.00012984685599803925\n",
            "  batch 669 loss: 0.00013654693961143494\n",
            "  batch 670 loss: 7.292343676090241e-05\n",
            "  batch 671 loss: 0.0001293640285730362\n",
            "  batch 672 loss: 6.85284212231636e-05\n",
            "  batch 673 loss: 6.591422855854035e-05\n",
            "  batch 674 loss: 0.00013824090361595155\n",
            "  batch 675 loss: 0.00015668854117393494\n",
            "  batch 676 loss: 0.00010719459503889085\n",
            "  batch 677 loss: 0.00010765785723924637\n",
            "  batch 678 loss: 0.00013490496575832366\n",
            "  batch 679 loss: 0.00010949550569057464\n",
            "  batch 680 loss: 0.00014858175814151764\n",
            "  batch 681 loss: 9.297692775726318e-05\n",
            "  batch 682 loss: 0.00010743502527475357\n",
            "  batch 683 loss: 9.473954886198044e-05\n",
            "  batch 684 loss: 0.00011316727846860886\n",
            "  batch 685 loss: 9.637208282947541e-05\n",
            "  batch 686 loss: 9.679517894983292e-05\n",
            "  batch 687 loss: 0.00011157239228487015\n",
            "  batch 688 loss: 6.139707192778588e-05\n",
            "  batch 689 loss: 0.00016735416650772095\n",
            "  batch 690 loss: 7.135956734418868e-05\n",
            "  batch 691 loss: 0.00012370089441537856\n",
            "  batch 692 loss: 0.00010697311908006668\n",
            "  batch 693 loss: 0.00015884211659431457\n",
            "  batch 694 loss: 0.00012625107169151307\n",
            "  batch 695 loss: 7.27052316069603e-05\n",
            "  batch 696 loss: 0.0001183628886938095\n",
            "  batch 697 loss: 0.00011917702108621597\n",
            "  batch 698 loss: 5.582170560956001e-05\n",
            "  batch 699 loss: 5.360633134841919e-05\n",
            "  batch 700 loss: 0.00011185150593519211\n",
            "  batch 701 loss: 9.061497449874878e-05\n",
            "  batch 702 loss: 0.00011965896934270858\n",
            "  batch 703 loss: 0.00010298344492912292\n",
            "  batch 704 loss: 0.00011847870051860809\n",
            "  batch 705 loss: 0.00011717509478330612\n",
            "  batch 706 loss: 0.00013454489409923554\n",
            "  batch 707 loss: 0.0001547608971595764\n",
            "  batch 708 loss: 0.00012004690617322922\n",
            "  batch 709 loss: 0.0001333739161491394\n",
            "  batch 710 loss: 6.431053578853607e-05\n",
            "  batch 711 loss: 0.0001182682141661644\n",
            "  batch 712 loss: 7.286340743303299e-05\n",
            "  batch 713 loss: 0.00013826511800289155\n",
            "  batch 714 loss: 8.149474859237672e-05\n",
            "  batch 715 loss: 9.811581671237945e-05\n",
            "  batch 716 loss: 0.00016991905868053437\n",
            "  batch 717 loss: 0.0001124783381819725\n",
            "  batch 718 loss: 9.20964926481247e-05\n",
            "  batch 719 loss: 0.00011064785718917847\n",
            "  batch 720 loss: 8.045878261327744e-05\n",
            "  batch 721 loss: 7.260750979185104e-05\n",
            "  batch 722 loss: 8.483003079891204e-05\n",
            "  batch 723 loss: 9.583244472742081e-05\n",
            "  batch 724 loss: 7.487913966178894e-05\n",
            "  batch 725 loss: 7.572310417890549e-05\n",
            "  batch 726 loss: 0.00010186740756034851\n",
            "  batch 727 loss: 0.0001029929369688034\n",
            "  batch 728 loss: 4.560956358909607e-05\n",
            "  batch 729 loss: 8.809942752122878e-05\n",
            "  batch 730 loss: 0.00010062822699546814\n",
            "  batch 731 loss: 7.130298763513565e-05\n",
            "  batch 732 loss: 0.00012289754301309585\n",
            "  batch 733 loss: 7.664553821086884e-05\n",
            "  batch 734 loss: 0.00013182882964611053\n",
            "  batch 735 loss: 8.944675326347351e-05\n",
            "  batch 736 loss: 0.0001188298761844635\n",
            "  batch 737 loss: 8.728604018688202e-05\n",
            "  batch 738 loss: 0.00011481249332427978\n",
            "  batch 739 loss: 0.00010742785036563873\n",
            "  batch 740 loss: 0.000106577567756176\n",
            "  batch 741 loss: 0.00011615202575922012\n",
            "  batch 742 loss: 0.00012786753475666046\n",
            "  batch 743 loss: 6.567353010177612e-05\n",
            "  batch 744 loss: 8.555559813976287e-05\n",
            "  batch 745 loss: 6.926223635673522e-05\n",
            "  batch 746 loss: 0.00013235853612422944\n",
            "  batch 747 loss: 7.661550492048263e-05\n",
            "  batch 748 loss: 3.479545935988426e-05\n",
            "  batch 749 loss: 5.703197419643402e-05\n",
            "  batch 750 loss: 9.038261324167252e-05\n",
            "  batch 751 loss: 0.00015270721912384033\n",
            "  batch 752 loss: 0.00013690827786922455\n",
            "  batch 753 loss: 0.00011633376032114029\n",
            "  batch 754 loss: 0.0001255977600812912\n",
            "  batch 755 loss: 0.00014869871735572816\n",
            "  batch 756 loss: 0.00010500433295965194\n",
            "  batch 757 loss: 0.00010124725103378296\n",
            "  batch 758 loss: 8.267313241958619e-05\n",
            "  batch 759 loss: 0.00016595126688480376\n",
            "  batch 760 loss: 0.00014889352023601533\n",
            "  batch 761 loss: 0.00011095279455184937\n",
            "  batch 762 loss: 0.00010592978447675705\n",
            "  batch 763 loss: 7.825309783220291e-05\n",
            "  batch 764 loss: 8.260975778102874e-05\n",
            "  batch 765 loss: 9.022864699363708e-05\n",
            "  batch 766 loss: 0.00010767920315265656\n",
            "  batch 767 loss: 0.00010654535889625549\n",
            "  batch 768 loss: 0.0001020471230149269\n",
            "  batch 769 loss: 0.00012923131883144377\n",
            "  batch 770 loss: 7.749861478805542e-05\n",
            "  batch 771 loss: 9.424202889204025e-05\n",
            "  batch 772 loss: 0.0001208442598581314\n",
            "  batch 773 loss: 9.420859068632126e-05\n",
            "  batch 774 loss: 0.00017208343744277955\n",
            "  batch 775 loss: 6.151033937931061e-05\n",
            "  batch 776 loss: 8.126351982355117e-05\n",
            "  batch 777 loss: 6.076057255268097e-05\n",
            "  batch 778 loss: 7.508447021245956e-05\n",
            "  batch 779 loss: 0.00011045104265213012\n",
            "  batch 780 loss: 9.037493914365768e-05\n",
            "  batch 781 loss: 8.005347102880477e-05\n",
            "  batch 782 loss: 0.00010953398793935776\n",
            "  batch 783 loss: 0.00012724700570106506\n",
            "  batch 784 loss: 0.00019417308270931245\n",
            "  batch 785 loss: 9.164601564407349e-05\n",
            "  batch 786 loss: 7.26504847407341e-05\n",
            "  batch 787 loss: 0.00010227926820516587\n",
            "  batch 788 loss: 0.0001542709618806839\n",
            "  batch 789 loss: 0.00010515817999839783\n",
            "  batch 790 loss: 0.0002096710503101349\n",
            "  batch 791 loss: 0.00014127914607524873\n",
            "  batch 792 loss: 0.00011040976643562317\n",
            "  batch 793 loss: 7.293875515460969e-05\n",
            "  batch 794 loss: 0.0001439713090658188\n",
            "  batch 795 loss: 9.740100055932999e-05\n",
            "  batch 796 loss: 0.00010540930926799775\n",
            "  batch 797 loss: 0.00010899853706359863\n",
            "  batch 798 loss: 9.832047671079636e-05\n",
            "  batch 799 loss: 0.0001201736181974411\n",
            "  batch 800 loss: 6.482870876789093e-05\n",
            "  batch 801 loss: 0.00012291965633630752\n",
            "  batch 802 loss: 7.872344553470612e-05\n",
            "  batch 803 loss: 0.0001798976957798004\n",
            "  batch 804 loss: 5.240974947810173e-05\n",
            "  batch 805 loss: 0.00010081296414136887\n",
            "  batch 806 loss: 0.00015292170643806458\n",
            "  batch 807 loss: 0.00011203352361917496\n",
            "  batch 808 loss: 7.492986321449279e-05\n",
            "  batch 809 loss: 0.00012526245415210723\n",
            "  batch 810 loss: 6.355468183755875e-05\n",
            "  batch 811 loss: 8.348242193460464e-05\n",
            "  batch 812 loss: 5.8309085667133333e-05\n",
            "  batch 813 loss: 3.065612353384495e-05\n",
            "  batch 814 loss: 5.438569188117981e-05\n",
            "  batch 815 loss: 8.558695018291473e-05\n",
            "  batch 816 loss: 0.00014788411557674407\n",
            "  batch 817 loss: 0.00011358758807182313\n",
            "  batch 818 loss: 9.180329740047454e-05\n",
            "  batch 819 loss: 0.00012654724717140196\n",
            "  batch 820 loss: 0.000159878209233284\n",
            "  batch 821 loss: 8.88669416308403e-05\n",
            "  batch 822 loss: 7.56726711988449e-05\n",
            "  batch 823 loss: 0.00011211849004030228\n",
            "  batch 824 loss: 7.333308458328247e-05\n",
            "  batch 825 loss: 9.235592186450958e-05\n",
            "  batch 826 loss: 7.425897568464279e-05\n",
            "  batch 827 loss: 0.00013566285371780396\n",
            "  batch 828 loss: 0.00011054712533950806\n",
            "  batch 829 loss: 0.00011547525227069854\n",
            "  batch 830 loss: 6.679852306842804e-05\n",
            "  batch 831 loss: 4.477627202868462e-05\n",
            "  batch 832 loss: 4.6007271856069565e-05\n",
            "  batch 833 loss: 0.00014527252316474915\n",
            "  batch 834 loss: 7.511992752552032e-05\n",
            "  batch 835 loss: 0.00011328083276748657\n",
            "  batch 836 loss: 8.41137170791626e-05\n",
            "  batch 837 loss: 0.00012069641798734665\n",
            "  batch 838 loss: 6.752023100852967e-05\n",
            "  batch 839 loss: 8.433083444833755e-05\n",
            "  batch 840 loss: 6.165195256471634e-05\n",
            "  batch 841 loss: 0.00012513718008995057\n",
            "  batch 842 loss: 5.8194052428007125e-05\n",
            "  batch 843 loss: 0.00013858719170093537\n",
            "  batch 844 loss: 0.00011472267657518386\n",
            "  batch 845 loss: 6.137968972325325e-05\n",
            "  batch 846 loss: 5.5081211030483246e-05\n",
            "  batch 847 loss: 5.5602040141820905e-05\n",
            "  batch 848 loss: 8.003050833940507e-05\n",
            "  batch 849 loss: 0.00010779467225074768\n",
            "  batch 850 loss: 8.625814318656921e-05\n",
            "  batch 851 loss: 5.6661013513803485e-05\n",
            "  batch 852 loss: 0.00013424484431743622\n",
            "  batch 853 loss: 0.00012323916703462602\n",
            "  batch 854 loss: 0.00011398941278457641\n",
            "  batch 855 loss: 6.800486892461777e-05\n",
            "  batch 856 loss: 7.7412948012352e-05\n",
            "  batch 857 loss: 8.072797209024429e-05\n",
            "  batch 858 loss: 0.00011902881413698196\n",
            "  batch 859 loss: 7.367225736379623e-05\n",
            "  batch 860 loss: 0.00010620075464248657\n",
            "  batch 861 loss: 7.235368341207505e-05\n",
            "  batch 862 loss: 0.0001676054745912552\n",
            "  batch 863 loss: 0.00010119719058275223\n",
            "  batch 864 loss: 0.00013515815138816833\n",
            "  batch 865 loss: 7.671356201171875e-05\n",
            "  batch 866 loss: 8.407171815633774e-05\n",
            "  batch 867 loss: 0.00011256193369627\n",
            "  batch 868 loss: 9.007032960653305e-05\n",
            "  batch 869 loss: 9.359155595302582e-05\n",
            "  batch 870 loss: 8.688700944185257e-05\n",
            "  batch 871 loss: 6.574693322181702e-05\n",
            "  batch 872 loss: 0.00011492545902729034\n",
            "  batch 873 loss: 0.00012042438983917236\n",
            "  batch 874 loss: 6.597320735454559e-05\n",
            "  batch 875 loss: 6.598459184169769e-05\n",
            "  batch 876 loss: 7.072965055704117e-05\n",
            "  batch 877 loss: 0.00013696551322937012\n",
            "  batch 878 loss: 9.357175976037979e-05\n",
            "  batch 879 loss: 7.152792066335678e-05\n",
            "  batch 880 loss: 6.338347494602203e-05\n",
            "  batch 881 loss: 0.00010647112876176834\n",
            "  batch 882 loss: 9.86884981393814e-05\n",
            "  batch 883 loss: 9.137320518493653e-05\n",
            "  batch 884 loss: 0.00015903356671333313\n",
            "  batch 885 loss: 8.278853446245194e-05\n",
            "  batch 886 loss: 0.0001335066854953766\n",
            "  batch 887 loss: 6.220465153455735e-05\n",
            "  batch 888 loss: 9.742892533540726e-05\n",
            "  batch 889 loss: 6.428194791078568e-05\n",
            "  batch 890 loss: 4.4925734400749206e-05\n",
            "  batch 891 loss: 7.812846451997757e-05\n",
            "  batch 892 loss: 0.00010697956383228302\n",
            "  batch 893 loss: 0.0001375988870859146\n",
            "  batch 894 loss: 8.91679897904396e-05\n",
            "  batch 895 loss: 9.061155468225479e-05\n",
            "  batch 896 loss: 8.774732053279876e-05\n",
            "  batch 897 loss: 0.00012686674296855926\n",
            "  batch 898 loss: 0.00011538364738225938\n",
            "  batch 899 loss: 7.504810392856598e-05\n",
            "  batch 900 loss: 9.911362826824189e-05\n",
            "  batch 901 loss: 8.184831589460374e-05\n",
            "  batch 902 loss: 8.610037714242935e-05\n",
            "  batch 903 loss: 8.643803000450134e-05\n",
            "  batch 904 loss: 0.00012031016498804092\n",
            "  batch 905 loss: 9.252707660198211e-05\n",
            "  batch 906 loss: 9.009787440299988e-05\n",
            "  batch 907 loss: 6.882677972316742e-05\n",
            "  batch 908 loss: 8.387642353773117e-05\n",
            "  batch 909 loss: 7.327289879322053e-05\n",
            "  batch 910 loss: 0.00012832428514957428\n",
            "  batch 911 loss: 0.000108814537525177\n",
            "  batch 912 loss: 0.00010972414910793304\n",
            "  batch 913 loss: 7.741831988096237e-05\n",
            "  batch 914 loss: 0.00010137049853801728\n",
            "  batch 915 loss: 0.0001054425910115242\n",
            "  batch 916 loss: 0.00010513629019260407\n",
            "  batch 917 loss: 8.681365847587586e-05\n",
            "  batch 918 loss: 0.0001509614735841751\n",
            "  batch 919 loss: 9.508880972862243e-05\n",
            "  batch 920 loss: 8.671429753303527e-05\n",
            "  batch 921 loss: 7.570169866085053e-05\n",
            "  batch 922 loss: 4.7350689768791196e-05\n",
            "  batch 923 loss: 6.183408945798874e-05\n",
            "  batch 924 loss: 9.13088396191597e-05\n",
            "  batch 925 loss: 9.395006299018859e-05\n",
            "  batch 926 loss: 0.00010365905612707138\n",
            "  batch 927 loss: 8.203232288360595e-05\n",
            "  batch 928 loss: 0.0001339408904314041\n",
            "  batch 929 loss: 6.16089440882206e-05\n",
            "  batch 930 loss: 7.600355148315429e-05\n",
            "  batch 931 loss: 0.00022073031961917876\n",
            "  batch 932 loss: 0.0001281813383102417\n",
            "  batch 933 loss: 0.00013929033279418945\n",
            "  batch 934 loss: 0.00010374580323696136\n",
            "  batch 935 loss: 9.055148065090179e-05\n",
            "  batch 936 loss: 0.00010942129790782929\n",
            "  batch 937 loss: 0.00010039683431386948\n",
            "  batch 938 loss: 7.708876579999924e-05\n",
            "  batch 939 loss: 0.00010088028013706208\n",
            "  batch 940 loss: 0.00013019506633281708\n",
            "  batch 941 loss: 7.689948379993438e-05\n",
            "  batch 942 loss: 7.851897180080414e-05\n",
            "  batch 943 loss: 0.00015189874172210694\n",
            "  batch 944 loss: 5.31521774828434e-05\n",
            "  batch 945 loss: 0.00011458615958690644\n",
            "  batch 946 loss: 0.00010230356454849243\n",
            "  batch 947 loss: 4.774264246225357e-05\n",
            "  batch 948 loss: 7.765863835811614e-05\n",
            "  batch 949 loss: 0.00015370011329650878\n",
            "  batch 950 loss: 9.637026488780975e-05\n",
            "  batch 951 loss: 8.995933830738068e-05\n",
            "  batch 952 loss: 0.00010220475494861603\n",
            "  batch 953 loss: 7.165977358818055e-05\n",
            "  batch 954 loss: 7.438156008720398e-05\n",
            "  batch 955 loss: 0.00013120855391025543\n",
            "  batch 956 loss: 5.673405155539513e-05\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg loss in epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_loss)\n\u001b[0;32m     19\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
            "Cell \u001b[1;32mIn[11], line 10\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m      3\u001b[0m last_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Here, we use enumerate(training_loader) instead of\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# iter(training_loader) so that we can track the batch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# index and do some intra-epoch reporting\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#swap train dataloader for dset\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdummy_train_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Every data instance is an input + label pair\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mSpiderDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     14\u001b[0m label_dir_list \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_dir)\n\u001b[0;32m     15\u001b[0m image_dir_list \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir)\n\u001b[1;32m---> 17\u001b[0m image_dir_list \u001b[38;5;241m=\u001b[39m \u001b[43mnatsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m label_dir_list \u001b[38;5;241m=\u001b[39m natsorted(label_dir_list)\n\u001b[0;32m     20\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir\u001b[38;5;241m.\u001b[39mjoinpath(image_dir_list[idx])\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\natsort\\natsort.py:293\u001b[0m, in \u001b[0;36mnatsorted\u001b[1;34m(seq, key, reverse, alg)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alg \u001b[38;5;241m&\u001b[39m ns\u001b[38;5;241m.\u001b[39mPRESORT:\n\u001b[0;32m    292\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(seq, reverse\u001b[38;5;241m=\u001b[39mreverse, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(seq, reverse\u001b[38;5;241m=\u001b[39mreverse, key\u001b[38;5;241m=\u001b[39mnatsort_keygen(key, alg))\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\natsort\\utils.py:341\u001b[0m, in \u001b[0;36mnatsort_key\u001b[1;34m(val, key, string_func, bytes_func, num_func)\u001b[0m\n\u001b[0;32m    338\u001b[0m     val \u001b[38;5;241m=\u001b[39m key(val)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mstr\u001b[39m, PurePath)):\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstring_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bytes_func(val)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\natsort\\utils.py:523\u001b[0m, in \u001b[0;36mparse_string_factory.<locals>.func\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    521\u001b[0m f \u001b[38;5;241m=\u001b[39m component_transform(e)  \u001b[38;5;66;03m# Apply transform on components.\u001b[39;00m\n\u001b[0;32m    522\u001b[0m g \u001b[38;5;241m=\u001b[39m sep_inserter(f, sep)  \u001b[38;5;66;03m# Insert '' between numbers.\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfinal_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\natsort\\utils.py:776\u001b[0m, in \u001b[0;36mfinal_data_transform_factory.<locals>.func\u001b[1;34m(split_val, val, _transform, _sep, _pre_sep)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\n\u001b[0;32m    770\u001b[0m     split_val: Iterable[NatsortInType],\n\u001b[0;32m    771\u001b[0m     val: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    774\u001b[0m     _pre_sep: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m pre_sep,\n\u001b[0;32m    775\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FinalTransform:\n\u001b[1;32m--> 776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(split_val)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\natsort\\utils.py:593\u001b[0m, in \u001b[0;36msep_inserter\u001b[1;34m(iterator, sep)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(first) \u001b[38;5;129;01min\u001b[39;00m types \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(second) \u001b[38;5;129;01min\u001b[39;00m types:\n\u001b[0;32m    592\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m sep\n\u001b[1;32m--> 593\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m second\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Catch StopIteration per deprecation in PEP 479:\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# \"Change StopIteration handling inside generators\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/spider_seg_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "    print(\"avg loss in epoch\", avg_loss)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(dummy_test_dataloader):\n",
        "            vinputs, vlabels = vdata\n",
        "\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = loss_func(voutputs, vlabels)\n",
        "            running_vloss += vloss\n",
        "            \n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    \n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        #commented out saving the model for now to debug loss being 0 \n",
        "        '''\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        '''\n",
        "    epoch_number += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
