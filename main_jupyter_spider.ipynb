{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFL78HZcbmSP"
      },
      "source": [
        "\n",
        "Connect to Google Drive for datasets (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcBj2OvobfY3",
        "outputId": "28f8c7e7-eb9b-490c-bd3c-674444fbe332"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1Wa9Ngfw6QC"
      },
      "source": [
        "Install dependencies (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOCzMWFLw53J",
        "outputId": "1a06cc2b-3e31-4c79-8917-9c8f90e50b52"
      },
      "outputs": [],
      "source": [
        "#!pip install simpleitk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVzzg3MJnhFU"
      },
      "source": [
        "Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QMgkyFD0Hm3",
        "outputId": "50d2df41-f013-4d5e-8c9a-b2d356fb8a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.widgets import Slider, Button, RadioButtons\n",
        "\n",
        "import scipy\n",
        "\n",
        "import SimpleITK as sitk\n",
        "reader = sitk.ImageFileReader()\n",
        "reader.SetImageIO(\"MetaImageIO\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import pathlib\n",
        "\n",
        "from natsort import natsorted\n",
        "\n",
        "#Set GPU/Cuda Device to run model on\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "np.random.seed(46)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H48GvrcPnhFh"
      },
      "source": [
        "Dataset Directories <Br>\n",
        "Comment out directory not in use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "35vekrfdnhFi"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Toy Dataset Slices \n",
        "dummy_train_img_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_train_img_slices\")\n",
        "dummy_train_label_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_train_label_slices\")\n",
        "dummy_test_img_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_test_img_slices\")\n",
        "dummy_test_label_slice_dir= pathlib.Path(r\"spider_toy_dset_slices/dummy_test_label_slices\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMoYlYbYnhFl"
      },
      "source": [
        "Image Slice Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mR75OjC82JOO"
      },
      "outputs": [],
      "source": [
        "from transforms import mri_transforms\n",
        "\n",
        "\n",
        "#TODO add bool image label for interp \n",
        "class Mri_Slice:\n",
        "    def __init__(self, path):\n",
        "        mri_mha = sitk.ReadImage(path, imageIO = \"MetaImageIO\") #explicitly setting ioreader just in case\n",
        "        #resample images to 64x64\n",
        "        #slice = mri_transforms.resample_img_to_res(itk_slice=slice, out_res= [64, 64], is_label= False, smoothing=False)\n",
        "\n",
        "        #get 2d array from mri slice\n",
        "        mri_a = np.array(sitk.GetArrayFromImage(mri_mha)) #mri_array\n",
        "        \n",
        "        mri_a_float32 = mri_a.astype(dtype = np.float32)\n",
        "        #TODO: set bounds to [-1000, 2000] https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        self.hu_a = mri_a_float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX_8rD_AyhSQ"
      },
      "source": [
        "Sort directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMiefYLdyRbn",
        "outputId": "9df4b03e-3cff-4f1a-ec74-55a8cc1ee98b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spider_toy_dset_slices\\dummy_train_img_slices\n",
            "spider_toy_dset_slices\\dummy_train_label_slices\n",
            "924\n"
          ]
        }
      ],
      "source": [
        "#get lists from directories\n",
        "\n",
        "#toy dset slices\n",
        "image_path = dummy_train_img_slice_dir\n",
        "label_path = dummy_train_label_slice_dir\n",
        "\n",
        "image_dir_list = os.listdir(image_path)\n",
        "label_dir_list = os.listdir(label_path)\n",
        "\n",
        "print(image_path) \n",
        "print(label_path)\n",
        "\n",
        "#local dset\n",
        "'''\n",
        "image_dir_list = os.listdir(local_img_idr)\n",
        "label_dir_list = os.listdir(local_label_dir)\n",
        "'''\n",
        "#sort lists\n",
        "image_dir_list = natsorted(image_dir_list)\n",
        "label_dir_list = natsorted(label_dir_list)\n",
        "#empty lists to hold x and y dimensions of images\n",
        "row_list = []\n",
        "col_list = []\n",
        "\n",
        "#dirlen = len(os.listdir(dummy_train_label_dir))\n",
        "dirlen = len(os.listdir(label_path))\n",
        "\n",
        "print(dirlen)\n",
        "\n",
        "#print(local_label_idr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get max dimension of slice in dset for x y padding <br>\n",
        "Images have slices with 0 label info removed and are cropped using zero crop "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m lbl_path \u001b[38;5;241m=\u001b[39m label_path\u001b[38;5;241m.\u001b[39mjoinpath(label_dir_list[idx])\u001b[38;5;66;03m#first part before joinpath is pathlib.Path, second part is the directory of hte file \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mimg_path = local_img_idr.joinpath(image_dir_list[idx])\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mlabel_path = local_label_dir.joinpath(label_dir_list[idx]) #first part before joinpath is pathlib.Path, second part is the directory of hte file \u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m image \u001b[38;5;241m=\u001b[39m Mri_Slice(img_path)\n\u001b[0;32m     15\u001b[0m label \u001b[38;5;241m=\u001b[39m Mri_Slice(lbl_path)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#if(image.hu_a.shape[0] > 600): #if image way too high res\u001b[39;00m\n\u001b[0;32m     19\u001b[0m   \u001b[38;5;66;03m#print(\"high res image in directory\", img_path)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#print(\"label after: \", label_a.shape)\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mMri_Slice.__init__\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m----> 7\u001b[0m     mri_mha \u001b[38;5;241m=\u001b[39m sitk\u001b[38;5;241m.\u001b[39mReadImage(path, imageIO \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetaImageIO\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#explicitly setting ioreader just in case\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#resample images to 64x64\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#slice = mri_transforms.resample_img_to_res(itk_slice=slice, out_res= [64, 64], is_label= False, smoothing=False)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#get 2d array from mri slice\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     mri_a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sitk\u001b[38;5;241m.\u001b[39mGetArrayFromImage(mri_mha)) \u001b[38;5;66;03m#mri_array\u001b[39;00m\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\SimpleITK\\extra.py:375\u001b[0m, in \u001b[0;36mReadImage\u001b[1;34m(fileName, outputPixelType, imageIO)\u001b[0m\n\u001b[0;32m    373\u001b[0m reader\u001b[38;5;241m.\u001b[39mSetImageIO(imageIO)\n\u001b[0;32m    374\u001b[0m reader\u001b[38;5;241m.\u001b[39mSetOutputPixelType(outputPixelType)\n\u001b[1;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mExecute()\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\SimpleITK\\SimpleITK.py:8430\u001b[0m, in \u001b[0;36mImageFileReader.Execute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   8417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mExecute\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   8418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   8419\u001b[0m \u001b[38;5;124;03m    Execute(ImageFileReader self) -> Image\u001b[39;00m\n\u001b[0;32m   8420\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8428\u001b[0m \n\u001b[0;32m   8429\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 8430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _SimpleITK\u001b[38;5;241m.\u001b[39mImageFileReader_Execute(\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transforms import array_transforms\n",
        "\n",
        "for idx in range(0, dirlen):\n",
        "  #print(\"dirlen\", dirlen)\n",
        "  \n",
        " #toy dset \n",
        "  \n",
        "  img_path = image_path.joinpath(image_dir_list[idx])\n",
        "  lbl_path = label_path.joinpath(label_dir_list[idx])#first part before joinpath is pathlib.Path, second part is the directory of hte file \n",
        "  '''\n",
        "  img_path = local_img_idr.joinpath(image_dir_list[idx])\n",
        "  label_path = local_label_dir.joinpath(label_dir_list[idx]) #first part before joinpath is pathlib.Path, second part is the directory of hte file \n",
        "  '''\n",
        "  image = Mri_Slice(img_path)\n",
        "  label = Mri_Slice(lbl_path)\n",
        "\n",
        "\n",
        "  #if(image.hu_a.shape[0] > 600): #if image way too high res\n",
        "    #print(\"high res image in directory\", img_path)\n",
        "\n",
        "  #print(\"label after: \", label_a.shape)\n",
        "  \n",
        "  row_list.append(image.hu_a.shape[0]) #add row value to list\n",
        "  col_list.append(image.hu_a.shape[1]) #add col value to list \n",
        "\n",
        "  \n",
        "#calculate max \n",
        "row_dim_max = max(row_list)\n",
        "col_dim_max = max(col_list)\n",
        "\n",
        "row_dim_max = ((row_dim_max + 15) // 16) * 16 #nearest multiple of 16\n",
        "col_dim_max = ((col_dim_max + 15) // 16) * 16 #nearest multiple of 16\n",
        "\n",
        "print(\"row max:\", max(row_list))\n",
        "print(\"col max:\", max(col_list))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVwddxJgnhFn"
      },
      "source": [
        "Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9X6rHdnnhFo",
        "outputId": "5e9fab66-6c87-4081-eaee-cd059d8a4445"
      },
      "outputs": [],
      "source": [
        "from transforms import tensor_transforms\n",
        "\n",
        "class SpiderDataset(Dataset):\n",
        "    def __init__(self, labels_dir, img_dir, transform=None, target_transform=None):\n",
        "        self.labels_dir = labels_dir\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.labels_dir))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label_dir_list = os.listdir(self.labels_dir)\n",
        "        image_dir_list = os.listdir(self.img_dir)\n",
        "\n",
        "        image_dir_list = natsorted(image_dir_list)\n",
        "        label_dir_list = natsorted(label_dir_list)\n",
        "\n",
        "        img_path = self.img_dir.joinpath(image_dir_list[idx])\n",
        "        label_path = self.labels_dir.joinpath(label_dir_list[idx])\n",
        "\n",
        "        image = Mri_Slice(img_path)\n",
        "        label = Mri_Slice(label_path)\n",
        "\n",
        "        image_a = image.hu_a\n",
        "        label_a = label.hu_a\n",
        "\n",
        "        #comment out the part not being used whether for 3d or 2d model \n",
        "    \n",
        "        '''\n",
        "        #3d tensor for 3D CNN\n",
        "        image_tensor = torch.from_numpy(image.hu_a)\n",
        "        label_tensor = torch.from_numpy(label.hu_a)\n",
        "        '''\n",
        "\n",
        "        image_tensor = torch.from_numpy(image_a)\n",
        "        label_tensor = torch.from_numpy(label_a)\n",
        "        \n",
        "        image_tensor = image_tensor.to(torch.float32)\n",
        "        label_tensor = label_tensor.to(torch.float32)\n",
        "        \n",
        "        #pad to max resolution of slice in dset \n",
        "        image_tensor = tensor_transforms.pad_to_resolution(image_tensor, [row_dim_max, col_dim_max])\n",
        "        label_tensor = tensor_transforms.pad_to_resolution(label_tensor, [row_dim_max, col_dim_max])\n",
        "\n",
        "        image_tensor = image_tensor.unsqueeze(0)\n",
        "        label_tensor = label_tensor.unsqueeze(0)\n",
        "\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        label_tensor = label_tensor.to(device)\n",
        "\n",
        "        #print(image_tensor.shape)\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train dataset len 924\n",
            "test dataset len 200\n"
          ]
        }
      ],
      "source": [
        "#toy train test dataset to test network running\n",
        "#local_train_set = SpiderDataset(local_img_idr, local_label_idr)\n",
        "dummy_train_set = SpiderDataset(dummy_train_label_slice_dir, dummy_train_img_slice_dir)\n",
        "\n",
        "dummy_test_set = SpiderDataset(dummy_test_label_slice_dir, dummy_test_img_slice_dir)\n",
        "\n",
        "print(\"train dataset len\",dummy_train_set.__len__())\n",
        "print(\"test dataset len\",dummy_test_set.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zsRlBNG-B6Q"
      },
      "source": [
        "Create Unet Instance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CT7aKn7S-F6c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;66;03m#one for every part of the spine\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m unet\u001b[38;5;241m.\u001b[39mUNet(in_channels \u001b[38;5;241m=\u001b[39m input_channels, out_channels \u001b[38;5;241m=\u001b[39m output_channels)\n\u001b[1;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ],
      "source": [
        "from models import unet \n",
        "\n",
        "input_channels = 1 #Hounsfield scale\n",
        "output_channels = 3 #Vertebra, disc and spinal canal masks SHOULD BE 3 FOR 3 MASKS\n",
        "output_channels = 16 #one for every part of the spine\n",
        "model = unet.UNet(in_channels = input_channels, out_channels = output_channels)\n",
        "model.to(device)\n",
        "model.to(torch.float32)\n",
        "#for param in model.parameters():\n",
        " #   print(param.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcuWKZAD9pHD"
      },
      "source": [
        "Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4lnCiKNW9sJN"
      },
      "outputs": [],
      "source": [
        "epochs = 5 #testing\n",
        "lr = 0.001 #testing\n",
        "batchsize = 2 #testing\n",
        "loss_func = nn.MSELoss()\n",
        "loss_func.to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWO0s80znhFr"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UQL_nxpCnhFs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor batch in dummy_train_dataloader:\\n    for tensor in batch:\\n        print(\"min\", torch.min(tensor))\\n        print(\"max\", torch.max(tensor))\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dummy_train_dataloader = DataLoader(dummy_train_set, batch_size = batchsize, shuffle=True)\n",
        "\n",
        "dummy_test_dataloader = DataLoader(dummy_test_set, batch_size = batchsize, shuffle=True)\n",
        "\n",
        "'''\n",
        "for batch in dummy_train_dataloader:\n",
        "    for tensor in batch:\n",
        "        print(\"min\", torch.min(tensor))\n",
        "        print(\"max\", torch.max(tensor))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensor Dimensions Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 899, 514])\n",
            "torch.Size([2, 1, 899, 514])\n",
            "torch.Size([2, 1, 899, 514])\n",
            "torch.Size([2, 1, 899, 514])\n",
            "torch.Size([2, 1, 899, 514])\n",
            "torch.Size([2, 1, 899, 514])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_slice \u001b[38;5;129;01min\u001b[39;00m dummy_train_dataloader:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m idx_slice:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(tensor\u001b[38;5;241m.\u001b[39mshape)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[10], line 18\u001b[0m, in \u001b[0;36mSpiderDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m image_dir_list \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir)\n\u001b[0;32m     17\u001b[0m image_dir_list \u001b[38;5;241m=\u001b[39m natsorted(image_dir_list)\n\u001b[1;32m---> 18\u001b[0m label_dir_list \u001b[38;5;241m=\u001b[39m natsorted(label_dir_list)\n\u001b[0;32m     20\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir\u001b[38;5;241m.\u001b[39mjoinpath(image_dir_list[idx])\n\u001b[0;32m     21\u001b[0m label_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_dir\u001b[38;5;241m.\u001b[39mjoinpath(label_dir_list[idx])\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\natsort\\natsort.py:293\u001b[0m, in \u001b[0;36mnatsorted\u001b[1;34m(seq, key, reverse, alg)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alg \u001b[38;5;241m&\u001b[39m ns\u001b[38;5;241m.\u001b[39mPRESORT:\n\u001b[0;32m    292\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(seq, reverse\u001b[38;5;241m=\u001b[39mreverse, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(seq, reverse\u001b[38;5;241m=\u001b[39mreverse, key\u001b[38;5;241m=\u001b[39mnatsort_keygen(key, alg))\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\natsort\\utils.py:341\u001b[0m, in \u001b[0;36mnatsort_key\u001b[1;34m(val, key, string_func, bytes_func, num_func)\u001b[0m\n\u001b[0;32m    338\u001b[0m     val \u001b[38;5;241m=\u001b[39m key(val)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mstr\u001b[39m, PurePath)):\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m string_func(val)\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bytes_func(val)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\natsort\\utils.py:523\u001b[0m, in \u001b[0;36mparse_string_factory.<locals>.func\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    521\u001b[0m f \u001b[38;5;241m=\u001b[39m component_transform(e)  \u001b[38;5;66;03m# Apply transform on components.\u001b[39;00m\n\u001b[0;32m    522\u001b[0m g \u001b[38;5;241m=\u001b[39m sep_inserter(f, sep)  \u001b[38;5;66;03m# Insert '' between numbers.\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_transform(g, original)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\natsort\\utils.py:776\u001b[0m, in \u001b[0;36mfinal_data_transform_factory.<locals>.func\u001b[1;34m(split_val, val, _transform, _sep, _pre_sep)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\n\u001b[0;32m    770\u001b[0m     split_val: Iterable[NatsortInType],\n\u001b[0;32m    771\u001b[0m     val: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    774\u001b[0m     _pre_sep: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m pre_sep,\n\u001b[0;32m    775\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FinalTransform:\n\u001b[1;32m--> 776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(split_val)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\natsort\\utils.py:593\u001b[0m, in \u001b[0;36msep_inserter\u001b[1;34m(iterator, sep)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(first) \u001b[38;5;129;01min\u001b[39;00m types \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(second) \u001b[38;5;129;01min\u001b[39;00m types:\n\u001b[0;32m    592\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m sep\n\u001b[1;32m--> 593\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m second\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Catch StopIteration per deprecation in PEP 479:\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# \"Change StopIteration handling inside generators\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for idx_slice in dummy_train_dataloader:\n",
        "    for tensor in idx_slice:\n",
        "        print(tensor.shape)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxAIK14tWJbp"
      },
      "source": [
        "One Epoch <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jxCZ41T_WOBF"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "\n",
        "    #swap train dataloader for dset\n",
        "    for i, data in enumerate(dummy_train_dataloader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        #print(\"labels\", labels.shape)\n",
        "        #print(inputs.shape)\n",
        "        #print(device)\n",
        "\n",
        "        #inputs = inputs.transpose(-1,0)\n",
        "        #labels = labels.transpose(-1,0)\n",
        "        #inputs = inputs.reshape(inputs.shape(1), inputs.shape(0), inputs.shape(3), inputs.shape(4))\n",
        "        #labels = labels.reshape(labels.shape(1), labels.shape(0), labels.shape(3), labels.shape(4))\n",
        "\n",
        "      \n",
        "        # Zero your gradients for every batch!\n",
        "        optim.zero_grad()\n",
        "\n",
        "        #2d\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        #print(\"outputs\", outputs.shape)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optim.step()\n",
        "\n",
        "        #3d tensor, probably won't use keeping here just in case  \n",
        "        '''\n",
        "        #run everything while indexing through z axis\n",
        "        for axis in images, masks:\n",
        "          for idx in range(0, axis.size(1)):\n",
        "            # Zero your gradients for every batch!\n",
        "            optim.zero_grad()\n",
        "            print(inputs[:, idx, : ,:])\n",
        "            # Make predictions for this batch\n",
        "            outputs = model(inputs[:, idx, : ,:])\n",
        "\n",
        "            # Compute the loss and its gradients\n",
        "            loss = loss_func(outputs[:, idx, : ,:], labels[:, idx, : ,:])\n",
        "            loss.backward()\n",
        "\n",
        "            # Adjust learning weights\n",
        "            optim.step()\n",
        "        '''\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        #if i % 1000 == 999:\n",
        "            #print(\"goes in\")\n",
        "        last_loss = running_loss / 1000 # loss per batch\n",
        "        print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "        tb_x = epoch_index * len(dummy_train_dataloader) + i + 1\n",
        "        tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "        running_loss = 0.\n",
        "        #if ends here\n",
        "\n",
        "    print(\"loss\", loss)\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyaaV1lYYiFc"
      },
      "source": [
        "Train Loop <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7i7TB_oOYhxQ",
        "outputId": "bfdaa28e-8cbc-4f46-be69-0979a6d3ceff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 448.00 MiB (GPU 0; 6.00 GiB total capacity; 5.15 GiB already allocated; 0 bytes free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(epoch_number, writer)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg loss in epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_loss)\n\u001b[0;32m     19\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
            "Cell \u001b[1;32mIn[16], line 31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m     27\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#2d\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#print(\"outputs\", outputs.shape)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(outputs, labels)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32md:\\Github Repos\\Spider Seg e19005\\spider-seg-e19005\\models\\unet.py:181\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    179\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_sample[i](x)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# Concatenate the output of the contracting path\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat[i](x, pass_through\u001b[38;5;241m.\u001b[39mpop())\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Two $3 \\times 3$ convolutional layers\u001b[39;00m\n\u001b[0;32m    183\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_conv[i](x)\n",
            "File \u001b[1;32md:\\Software\\Anaconda\\envs\\pytorch-medical\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32md:\\Github Repos\\Spider Seg e19005\\spider-seg-e19005\\models\\unet.py:117\u001b[0m, in \u001b[0;36mCropAndConcat.forward\u001b[1;34m(self, x, contracting_x)\u001b[0m\n\u001b[0;32m    115\u001b[0m contracting_x \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcenter_crop(contracting_x, [x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]])\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Concatenate the feature maps\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, contracting_x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 448.00 MiB (GPU 0; 6.00 GiB total capacity; 5.15 GiB already allocated; 0 bytes free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/spider_seg_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "    print(\"avg loss in epoch\", avg_loss)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(dummy_test_dataloader):\n",
        "            vinputs, vlabels = vdata\n",
        "\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = loss_func(voutputs, vlabels)\n",
        "            running_vloss += vloss\n",
        "            \n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    \n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        #commented out saving the model for now to debug loss being 0 \n",
        "        '''\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        '''\n",
        "    epoch_number += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
