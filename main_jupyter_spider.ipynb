{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFL78HZcbmSP"
      },
      "source": [
        "\n",
        "Connect to Google Drive for datasets (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcBj2OvobfY3",
        "outputId": "28f8c7e7-eb9b-490c-bd3c-674444fbe332"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1Wa9Ngfw6QC"
      },
      "source": [
        "Install dependencies (colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOCzMWFLw53J",
        "outputId": "1a06cc2b-3e31-4c79-8917-9c8f90e50b52"
      },
      "outputs": [],
      "source": [
        "#!pip install simpleitk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVzzg3MJnhFU"
      },
      "source": [
        "Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QMgkyFD0Hm3",
        "outputId": "50d2df41-f013-4d5e-8c9a-b2d356fb8a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.widgets import Slider, Button, RadioButtons\n",
        "\n",
        "import SimpleITK as sitk\n",
        "reader = sitk.ImageFileReader()\n",
        "reader.SetImageIO(\"MetaImageIO\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import pathlib\n",
        "\n",
        "from natsort import natsorted\n",
        "\n",
        "#Set GPU/Cuda Device to run model on\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "np.random.seed(46)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H48GvrcPnhFh"
      },
      "source": [
        "Dataset Directories <Br>\n",
        "Comment out directory not in use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "35vekrfdnhFi"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Toy Dataset Slices \n",
        "dummy_train_img_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_train_img_slices\")\n",
        "dummy_train_label_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_train_label_slices\")\n",
        "dummy_test_img_slice_dir = pathlib.Path(r\"spider_toy_dset_slices/dummy_test_img_slices\")\n",
        "dummy_test_label_slice_dir= pathlib.Path(r\"spider_toy_dset_slices/dummy_test_label_slices\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMoYlYbYnhFl"
      },
      "source": [
        "Image Slice Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mR75OjC82JOO"
      },
      "outputs": [],
      "source": [
        "from transforms import mri_transforms\n",
        "\n",
        "\n",
        "#TODO add bool image label for interp \n",
        "class Mri_Slice:\n",
        "    def __init__(self, path):\n",
        "        mri_mha = sitk.ReadImage(path, imageIO = \"MetaImageIO\") #explicitly setting ioreader just in case\n",
        "\n",
        "        #grabbing the image as well for downsampling testing\n",
        "        self.mri_mha = mri_mha\n",
        "        #resample images to 64x64\n",
        "       \n",
        "\n",
        "        #get 2d array from mri slice\n",
        "        mri_a = np.array(sitk.GetArrayFromImage(mri_mha)) #mri_array\n",
        "        \n",
        "        mri_a_float32 = mri_a.astype(dtype = np.float32)\n",
        "        #TODO: set bounds to [-1000, 2000] https://en.wikipedia.org/wiki/Hounsfield_scale\n",
        "        self.hu_a = mri_a_float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zero crop sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image (0000019E9F52C300)\n",
            "  RTTI typeinfo:   class itk::Image<double,2>\n",
            "  Reference Count: 1\n",
            "  Modified Time: 2067\n",
            "  Debug: Off\n",
            "  Object Name: \n",
            "  Observers: \n",
            "    none\n",
            "  Source: (none)\n",
            "  Source output name: (none)\n",
            "  Release Data: Off\n",
            "  Data Released: False\n",
            "  Global Release Data: Off\n",
            "  PipelineMTime: 2044\n",
            "  UpdateMTime: 2063\n",
            "  RealTimeStamp: 0 seconds \n",
            "  LargestPossibleRegion: \n",
            "    Dimension: 2\n",
            "    Index: [0, 0]\n",
            "    Size: [467, 482]\n",
            "  BufferedRegion: \n",
            "    Dimension: 2\n",
            "    Index: [0, 0]\n",
            "    Size: [467, 482]\n",
            "  RequestedRegion: \n",
            "    Dimension: 2\n",
            "    Index: [0, 0]\n",
            "    Size: [467, 482]\n",
            "  Spacing: [1, 1]\n",
            "  Origin: [0, 0]\n",
            "  Direction: \n",
            "1 0\n",
            "0 1\n",
            "\n",
            "  IndexToPointMatrix: \n",
            "1 0\n",
            "0 1\n",
            "\n",
            "  PointToIndexMatrix: \n",
            "1 0\n",
            "0 1\n",
            "\n",
            "  Inverse Direction: \n",
            "1 0\n",
            "0 1\n",
            "\n",
            "  PixelContainer: \n",
            "    ImportImageContainer (0000019EA6881260)\n",
            "      RTTI typeinfo:   class itk::ImportImageContainer<unsigned __int64,double>\n",
            "      Reference Count: 1\n",
            "      Modified Time: 2060\n",
            "      Debug: Off\n",
            "      Object Name: \n",
            "      Observers: \n",
            "        none\n",
            "      Pointer: 0000019EADF64040\n",
            "      Container manages memory: true\n",
            "      Size: 225094\n",
            "      Capacity: 225094\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transforms import array_transforms\n",
        "\n",
        "test_image_slice = Mri_Slice(r\"spider_toy_dset_slices/dummy_train_img_slices/1_t1_30.mha\")\n",
        "test_label_slice = Mri_Slice(r\"spider_toy_dset_slices/dummy_train_label_slices/1_t1_30.mha\")\n",
        "\n",
        "print(test_label_slice.mri_mha)\n",
        "\n",
        "test_image_a = test_image_slice.hu_a\n",
        "test_label_a = test_label_slice.hu_a\n",
        "\n",
        "test_image_a, test_label_a = array_transforms.crop_zero(test_image_a, test_label_a)\n",
        "\n",
        "test_image_crop_mha = sitk.GetImageFromArray(test_image_a)\n",
        "\n",
        "sitk.WriteImage(test_image_crop_mha, \"D:/sitk dump/1_t1_30_cropzero.mha\")\n",
        "\n",
        "#TODO continue here make images 1:1 scale and downsample to 64x64\n",
        "#if(sitk_slice.GetSize()[0] != sitk_slice.GetSize()[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX_8rD_AyhSQ"
      },
      "source": [
        "Sort directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMiefYLdyRbn",
        "outputId": "9df4b03e-3cff-4f1a-ec74-55a8cc1ee98b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spider_toy_dset_slices\\dummy_train_img_slices\n",
            "spider_toy_dset_slices\\dummy_train_label_slices\n",
            "2100\n"
          ]
        }
      ],
      "source": [
        "#get lists from directories\n",
        "\n",
        "#toy dset slices\n",
        "image_path = dummy_train_img_slice_dir\n",
        "label_path = dummy_train_label_slice_dir\n",
        "\n",
        "image_dir_list = os.listdir(image_path)\n",
        "label_dir_list = os.listdir(label_path)\n",
        "\n",
        "print(image_path) \n",
        "print(label_path)\n",
        "\n",
        "#local dset\n",
        "'''\n",
        "image_dir_list = os.listdir(local_img_idr)\n",
        "label_dir_list = os.listdir(local_label_dir)\n",
        "'''\n",
        "#sort lists\n",
        "image_dir_list = natsorted(image_dir_list)\n",
        "label_dir_list = natsorted(label_dir_list)\n",
        "#empty lists to hold x and y dimensions of images\n",
        "row_list = []\n",
        "col_list = []\n",
        "\n",
        "#dirlen = len(os.listdir(dummy_train_label_dir))\n",
        "dirlen = len(os.listdir(label_path))\n",
        "\n",
        "print(dirlen)\n",
        "\n",
        "#print(local_label_idr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get max dimension of slice in dset for x y padding <br>\n",
        "Images have slices with 0 label info removed and are cropped using zero crop <br>\n",
        "Also get min max values of dset for tensor normalization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "row max: 448\n",
            "col max: 224\n",
            "image tensor min -2057.0\n",
            "image tensor max 4292.0\n",
            "label tensor min 0.0\n",
            "label tensor max 208.0\n",
            "amount of masks [2.]\n"
          ]
        }
      ],
      "source": [
        "from transforms import array_transforms\n",
        "\n",
        "\n",
        "\n",
        "row_list = []\n",
        "col_list = []\n",
        "\n",
        "for idx in range(0, dirlen):\n",
        "  #print(\"dirlen\", dirlen)\n",
        "  \n",
        " #toy dset \n",
        "  \n",
        "  img_path = image_path.joinpath(image_dir_list[idx])\n",
        "  lbl_path = label_path.joinpath(label_dir_list[idx])#first part before joinpath is pathlib.Path, second part is the directory of hte file \n",
        "  '''\n",
        "  img_path = local_img_idr.joinpath(image_dir_list[idx])\n",
        "  label_path = local_label_dir.joinpath(label_dir_list[idx]) #first part before joinpath is pathlib.Path, second part is the directory of hte file \n",
        "  '''\n",
        "  image = Mri_Slice(img_path)\n",
        "  label = Mri_Slice(lbl_path)\n",
        "\n",
        "  image_a = image.hu_a\n",
        "  label_a = label.hu_a\n",
        "\n",
        "  #crop zero\n",
        "  image_a_cropzero, label_a_cropzero = array_transforms.crop_zero(image_a, label_a)\n",
        "\n",
        "  #resampling code will go here instead of crop-zero\n",
        "\n",
        "\n",
        "  if(idx ==0):\n",
        "    image_tensor_min = np.min(image_a_cropzero)\n",
        "    image_tensor_max = np.max(label_a_cropzero)\n",
        "    label_tensor_min = np.min(label_a_cropzero)\n",
        "    label_tensor_max = np.max(label_a_cropzero)\n",
        "\n",
        "    unique_masks, masks_counts = np.unique(label_a, return_counts=True)\n",
        "  else:\n",
        "    if(np.min(image_a_cropzero) < image_tensor_min):\n",
        "      image_tensor_min = np.min(image_a_cropzero)\n",
        "      image_tensor_min_dir = img_path\n",
        "    if(np.min(label_a_cropzero) < label_tensor_min):\n",
        "      label_tensor_min = np.min(label_a_cropzero)\n",
        "    if(np.max(image_a_cropzero) > image_tensor_max):\n",
        "      image_tensor_max = np.max(image_a_cropzero)\n",
        "    if(np.max(label_a_cropzero) > label_tensor_max):\n",
        "      label_tensor_max = np.max(label_a_cropzero)\n",
        "\n",
        "    current_masks, current_mask_counts = np.unique(label_a, return_counts=True)\n",
        "    if(len(current_masks > len(unique_masks))):\n",
        "      unique_masks = current_masks\n",
        "  #print(\"image res\", image_a_cropzero.shape)\n",
        "  \n",
        "  \n",
        "  row_list.append(image_a_cropzero.shape[0]) #add row value to list\n",
        "  #print(image_a_cropzero.shape[0])\n",
        "  col_list.append(image_a_cropzero.shape[1]) #add col value to list \n",
        "  \n",
        "\n",
        "#calculate max \n",
        "row_dim_max = max(row_list)\n",
        "col_dim_max = max(col_list)\n",
        "\n",
        "row_dim_max = ((row_dim_max + 15) // 16) * 16 #nearest multiple of 16\n",
        "col_dim_max = ((col_dim_max + 15) // 16) * 16 #nearest multiple of 16\n",
        "\n",
        "print(\"row max:\", max(row_list))\n",
        "print(\"col max:\", max(col_list))\n",
        "\n",
        "print(\"image tensor min\", image_tensor_min)\n",
        "print(\"image tensor max\", image_tensor_max)\n",
        "print(\"label tensor min\", label_tensor_min)\n",
        "print(\"label tensor max\", label_tensor_max)\n",
        "\n",
        "print(\"amount of masks\", unique_masks)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVwddxJgnhFn"
      },
      "source": [
        "Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9X6rHdnnhFo",
        "outputId": "5e9fab66-6c87-4081-eaee-cd059d8a4445"
      },
      "outputs": [],
      "source": [
        "from transforms import tensor_transforms\n",
        "\n",
        "class SpiderDataset(Dataset):\n",
        "    def __init__(self, labels_dir, img_dir, transform=None, target_transform=None):\n",
        "        self.labels_dir = labels_dir\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.labels_dir))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label_dir_list = os.listdir(self.labels_dir)\n",
        "        image_dir_list = os.listdir(self.img_dir)\n",
        "\n",
        "        image_dir_list = natsorted(image_dir_list)\n",
        "        label_dir_list = natsorted(label_dir_list)\n",
        "\n",
        "        img_path = self.img_dir.joinpath(image_dir_list[idx])\n",
        "        label_path = self.labels_dir.joinpath(label_dir_list[idx])\n",
        "\n",
        "        image = Mri_Slice(img_path)\n",
        "        label = Mri_Slice(label_path)\n",
        "\n",
        "        image_a = image.hu_a\n",
        "        label_a = label.hu_a\n",
        "\n",
        "        #comment out the part not being used whether for 3d or 2d model \n",
        "    \n",
        "        '''\n",
        "        #3d tensor for 3D CNN\n",
        "        image_tensor = torch.from_numpy(image.hu_a)\n",
        "        label_tensor = torch.from_numpy(label.hu_a)\n",
        "        '''\n",
        "\n",
        "        image_tensor = torch.from_numpy(image_a)\n",
        "        label_tensor = torch.from_numpy(label_a)\n",
        "        \n",
        "        image_tensor = image_tensor.to(torch.float32)\n",
        "        label_tensor = label_tensor.to(torch.float32)\n",
        "        \n",
        "        #pad to max resolution of slice in dset \n",
        "        image_tensor = tensor_transforms.pad_to_resolution(image_tensor, [row_dim_max, col_dim_max])\n",
        "        label_tensor = tensor_transforms.pad_to_resolution(label_tensor, [row_dim_max, col_dim_max])\n",
        "\n",
        "        \n",
        "         #normalise\n",
        "        image_tensor = (image_tensor - image_tensor_min) / (image_tensor_max - image_tensor_min)\n",
        "        label_tensor = (label_tensor - label_tensor_min) / (label_tensor_max - label_tensor_min)\n",
        "\n",
        "        #comment out unsqueeze testing\n",
        "        \n",
        "        image_tensor = image_tensor.unsqueeze(0)\n",
        "        label_tensor = label_tensor.unsqueeze(0)\n",
        "        \n",
        "\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        label_tensor = label_tensor.to(device)\n",
        "\n",
        "        #print(image_tensor.shape)\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train dataset len 2100\n",
            "test dataset len 215\n"
          ]
        }
      ],
      "source": [
        "#toy train test dataset to test network running\n",
        "#local_train_set = SpiderDataset(local_img_idr, local_label_idr)\n",
        "dummy_train_set = SpiderDataset(dummy_train_label_slice_dir, dummy_train_img_slice_dir)\n",
        "\n",
        "dummy_test_set = SpiderDataset(dummy_test_label_slice_dir, dummy_test_img_slice_dir)\n",
        "\n",
        "print(\"train dataset len\",dummy_train_set.__len__())\n",
        "print(\"test dataset len\",dummy_test_set.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zsRlBNG-B6Q"
      },
      "source": [
        "Create Unet Instance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CT7aKn7S-F6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (down_conv): ModuleList(\n",
              "    (0): DoubleConvolution(\n",
              "      (first): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (1): DoubleConvolution(\n",
              "      (first): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (2): DoubleConvolution(\n",
              "      (first): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (3): DoubleConvolution(\n",
              "      (first): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (down_sample): ModuleList(\n",
              "    (0-3): 4 x DownSample(\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (middle_conv): DoubleConvolution(\n",
              "    (first): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act1): ReLU()\n",
              "    (second): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (act2): ReLU()\n",
              "  )\n",
              "  (up_sample): ModuleList(\n",
              "    (0): UpSample(\n",
              "      (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (1): UpSample(\n",
              "      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (2): UpSample(\n",
              "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (3): UpSample(\n",
              "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "  )\n",
              "  (up_conv): ModuleList(\n",
              "    (0): DoubleConvolution(\n",
              "      (first): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (1): DoubleConvolution(\n",
              "      (first): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (2): DoubleConvolution(\n",
              "      (first): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "    (3): DoubleConvolution(\n",
              "      (first): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act1): ReLU()\n",
              "      (second): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (act2): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (concat): ModuleList(\n",
              "    (0-3): 4 x CropAndConcat()\n",
              "  )\n",
              "  (final_conv): Conv2d(64, 18, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from models import unet \n",
        "\n",
        "input_channels = 1 #Hounsfield scale\n",
        "output_channels = 3 #Vertebra, disc and spinal canal masks SHOULD BE 3 FOR 3 MASKS\n",
        "output_channels = 18 #one for every mask\n",
        "model = unet.UNet(in_channels = input_channels, out_channels = output_channels)\n",
        "model.to(device)\n",
        "model.to(torch.float32)\n",
        "#for param in model.parameters():\n",
        " #   print(param.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Faster-RCNN Instance trained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\\nfrom torchvision.utils import draw_bounding_boxes\\n\\nweights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\\nmodel = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Mask RCNN Instance Trained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights\\n\\nmodel = maskrcnn_resnet50_fpn_v2(\\n    weights = MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,\\n    num_classes = 19\\n    #weights_backbone = \\n)\\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights\n",
        "\n",
        "model = maskrcnn_resnet50_fpn_v2(\n",
        "    weights = MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,\n",
        "    num_classes = 19\n",
        "    #weights_backbone = \n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcuWKZAD9pHD"
      },
      "source": [
        "Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4lnCiKNW9sJN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n",
            "0.001\n",
            "8\n",
            "L1Loss()\n"
          ]
        }
      ],
      "source": [
        "epochs = 25 #testing\n",
        "lr = 0.001 #testing\n",
        "batchsize = 8 #testing\n",
        "loss_func = nn.L1Loss() #testing\n",
        "loss_func.to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "print(epochs)\n",
        "print(lr)\n",
        "print(batchsize)\n",
        "print(loss_func)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWO0s80znhFr"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UQL_nxpCnhFs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor batch in dummy_train_dataloader:\\n    for tensor in batch:\\n        print(\"min\", torch.min(tensor))\\n        print(\"max\", torch.max(tensor))\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dummy_train_dataloader = DataLoader(dummy_train_set, batch_size = batchsize, shuffle=True)\n",
        "\n",
        "dummy_test_dataloader = DataLoader(dummy_test_set, batch_size = batchsize, shuffle=True)\n",
        "\n",
        "'''\n",
        "for batch in dummy_train_dataloader:\n",
        "    for tensor in batch:\n",
        "        print(\"min\", torch.min(tensor))\n",
        "        print(\"max\", torch.max(tensor))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensor Dimensions Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor idx_slice in dummy_train_dataloader:\\n    for tensor in idx_slice:\\n        print(tensor.shape)\\n        break\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "for idx_slice in dummy_train_dataloader:\n",
        "    for tensor in idx_slice:\n",
        "        print(tensor.shape)\n",
        "        break\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxAIK14tWJbp"
      },
      "source": [
        "One Epoch <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jxCZ41T_WOBF"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "\n",
        "    #swap train dataloader for dset\n",
        "    for i, data in enumerate(dummy_train_dataloader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        #print(\"labels\", labels.shape)\n",
        "        #print(inputs.shape)\n",
        "        #print(device)\n",
        "\n",
        "        #inputs = inputs.transpose(-1,0)\n",
        "        #labels = labels.transpose(-1,0)\n",
        "        #inputs = inputs.reshape(inputs.shape(1), inputs.shape(0), inputs.shape(3), inputs.shape(4))\n",
        "        #labels = labels.reshape(labels.shape(1), labels.shape(0), labels.shape(3), labels.shape(4))\n",
        "\n",
        "      \n",
        "        # Zero your gradients for every batch!\n",
        "        optim.zero_grad()\n",
        "\n",
        "        #2d\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        #print(\"outputs\", outputs.shape)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optim.step()\n",
        "\n",
        "        #3d tensor, probably won't use keeping here just in case  \n",
        "        '''\n",
        "        #run everything while indexing through z axis\n",
        "        for axis in images, masks:\n",
        "          for idx in range(0, axis.size(1)):\n",
        "            # Zero your gradients for every batch!\n",
        "            optim.zero_grad()\n",
        "            print(inputs[:, idx, : ,:])\n",
        "            # Make predictions for this batch\n",
        "            outputs = model(inputs[:, idx, : ,:])\n",
        "\n",
        "            # Compute the loss and its gradients\n",
        "            loss = loss_func(outputs[:, idx, : ,:], labels[:, idx, : ,:])\n",
        "            loss.backward()\n",
        "\n",
        "            # Adjust learning weights\n",
        "            optim.step()\n",
        "        '''\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        #if i % 1000 == 999:\n",
        "            #print(\"goes in\")\n",
        "        last_loss = running_loss / 1000 # loss per batch\n",
        "        print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "        tb_x = epoch_index * len(dummy_train_dataloader) + i + 1\n",
        "        tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "        running_loss = 0.\n",
        "        #if ends here\n",
        "\n",
        "    print(\"loss\", loss)\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyaaV1lYYiFc"
      },
      "source": [
        "Train Loop <br>\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7i7TB_oOYhxQ",
        "outputId": "bfdaa28e-8cbc-4f46-be69-0979a6d3ceff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([8, 1, 448, 224])) that is different to the input size (torch.Size([8, 18, 448, 224])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 1 loss: 7.606413960456849e-05\n",
            "  batch 2 loss: 6.122398003935815e-05\n",
            "  batch 3 loss: 5.328477174043656e-05\n",
            "  batch 4 loss: 5.190099403262138e-05\n",
            "  batch 5 loss: 4.8544581979513167e-05\n",
            "  batch 6 loss: 3.0157944187521936e-05\n",
            "  batch 7 loss: 2.191663905978203e-05\n",
            "  batch 8 loss: 2.0613335072994234e-05\n",
            "  batch 9 loss: 2.3140767589211463e-05\n",
            "  batch 10 loss: 2.2484250366687774e-05\n",
            "  batch 11 loss: 1.965569518506527e-05\n",
            "  batch 12 loss: 3.5984288901090625e-05\n",
            "  batch 13 loss: 4.1607625782489775e-05\n",
            "  batch 14 loss: 1.797587238252163e-05\n",
            "  batch 15 loss: 4.251671209931374e-05\n",
            "  batch 16 loss: 1.777546666562557e-05\n",
            "  batch 17 loss: 2.282278425991535e-05\n",
            "  batch 18 loss: 2.7176205068826675e-05\n",
            "  batch 19 loss: 2.1548749879002573e-05\n",
            "  batch 20 loss: 2.2702788934111596e-05\n",
            "  batch 21 loss: 1.6977107152342795e-05\n",
            "  batch 22 loss: 2.595103532075882e-05\n",
            "  batch 23 loss: 3.4466136246919635e-05\n",
            "  batch 24 loss: 2.676677145063877e-05\n",
            "  batch 25 loss: 1.4788048341870308e-05\n",
            "  batch 26 loss: 2.755852974951267e-05\n",
            "  batch 27 loss: 3.2892584800720215e-05\n",
            "  batch 28 loss: 4.350343719124794e-05\n",
            "  batch 29 loss: 2.0596059039235114e-05\n",
            "  batch 30 loss: 1.8236281350255013e-05\n",
            "  batch 31 loss: 3.955768048763275e-05\n",
            "  batch 32 loss: 3.221383690834045e-05\n",
            "  batch 33 loss: 2.236447110772133e-05\n",
            "  batch 34 loss: 1.9792921841144563e-05\n",
            "  batch 35 loss: 1.1711550876498223e-05\n",
            "  batch 36 loss: 1.8176846206188203e-05\n",
            "  batch 37 loss: 2.4602876976132392e-05\n",
            "  batch 38 loss: 2.4642162024974823e-05\n",
            "  batch 39 loss: 2.7073167264461518e-05\n",
            "  batch 40 loss: 3.1464513391256335e-05\n",
            "  batch 41 loss: 2.6763830333948135e-05\n",
            "  batch 42 loss: 2.6005202904343605e-05\n",
            "  batch 43 loss: 2.0768782123923303e-05\n",
            "  batch 44 loss: 1.785360462963581e-05\n",
            "  batch 45 loss: 2.0789265632629395e-05\n",
            "  batch 46 loss: 2.0239904522895814e-05\n",
            "  batch 47 loss: 2.7716068550944327e-05\n",
            "  batch 48 loss: 1.697368733584881e-05\n",
            "  batch 49 loss: 2.6396246626973153e-05\n",
            "  batch 50 loss: 2.873034402728081e-05\n",
            "  batch 51 loss: 3.2095100730657577e-05\n",
            "  batch 52 loss: 3.501802310347557e-05\n",
            "  batch 53 loss: 3.3610764890909195e-05\n",
            "  batch 54 loss: 1.460883766412735e-05\n",
            "  batch 55 loss: 3.108712285757065e-05\n",
            "  batch 56 loss: 1.8947960808873177e-05\n",
            "  batch 57 loss: 1.5880895778536797e-05\n",
            "  batch 58 loss: 1.7831770703196526e-05\n",
            "  batch 59 loss: 3.39258499443531e-05\n",
            "  batch 60 loss: 4.1519045829772947e-05\n",
            "  batch 61 loss: 1.5230023302137851e-05\n",
            "  batch 62 loss: 2.5734532624483107e-05\n",
            "  batch 63 loss: 2.3096354678273202e-05\n",
            "  batch 64 loss: 2.1269109100103378e-05\n",
            "  batch 65 loss: 2.654606103897095e-05\n",
            "  batch 66 loss: 1.1996440589427948e-05\n",
            "  batch 67 loss: 1.8793679773807527e-05\n",
            "  batch 68 loss: 4.6063326299190524e-05\n",
            "  batch 69 loss: 1.7238175496459006e-05\n",
            "  batch 70 loss: 1.3560515828430653e-05\n",
            "  batch 71 loss: 3.2097682356834415e-05\n",
            "  batch 72 loss: 2.1490996703505515e-05\n",
            "  batch 73 loss: 2.6667943224310876e-05\n",
            "  batch 74 loss: 2.681785635650158e-05\n",
            "  batch 75 loss: 1.765407808125019e-05\n",
            "  batch 76 loss: 3.3253733068704604e-05\n",
            "  batch 77 loss: 1.044066995382309e-05\n",
            "  batch 78 loss: 2.3408817127346994e-05\n",
            "  batch 79 loss: 1.8640901893377304e-05\n",
            "  batch 80 loss: 3.466067835688591e-05\n",
            "  batch 81 loss: 2.1770551800727845e-05\n",
            "  batch 82 loss: 1.714427210390568e-05\n",
            "  batch 83 loss: 1.7106520012021065e-05\n",
            "  batch 84 loss: 1.808745227754116e-05\n",
            "  batch 85 loss: 2.1737698465585708e-05\n",
            "  batch 86 loss: 2.270948514342308e-05\n",
            "  batch 87 loss: 1.7789663746953012e-05\n",
            "  batch 88 loss: 2.8273502364754676e-05\n",
            "  batch 89 loss: 1.6005678102374077e-05\n",
            "  batch 90 loss: 2.732193470001221e-05\n",
            "  batch 91 loss: 3.519969433546066e-05\n",
            "  batch 92 loss: 2.5600727647542954e-05\n",
            "  batch 93 loss: 2.8009533882141115e-05\n",
            "  batch 94 loss: 3.551236540079117e-05\n",
            "  batch 95 loss: 2.3841556161642074e-05\n",
            "  batch 96 loss: 1.9145717844367028e-05\n",
            "  batch 97 loss: 3.120101988315582e-05\n",
            "  batch 98 loss: 1.8086904659867288e-05\n",
            "  batch 99 loss: 2.0163800567388535e-05\n",
            "  batch 100 loss: 3.3555071800947186e-05\n",
            "  batch 101 loss: 1.805279590189457e-05\n",
            "  batch 102 loss: 2.3313000798225403e-05\n",
            "  batch 103 loss: 1.4176287688314915e-05\n",
            "  batch 104 loss: 2.3064719513058662e-05\n",
            "  batch 105 loss: 1.7302261665463448e-05\n",
            "  batch 106 loss: 3.216155618429184e-05\n",
            "  batch 107 loss: 5.366785451769829e-06\n",
            "  batch 108 loss: 1.5413456596434117e-05\n",
            "  batch 109 loss: 2.7621814981102944e-05\n",
            "  batch 110 loss: 3.8239873945713044e-05\n",
            "  batch 111 loss: 3.21902334690094e-05\n",
            "  batch 112 loss: 2.251659706234932e-05\n",
            "  batch 113 loss: 1.7145533114671707e-05\n",
            "  batch 114 loss: 2.1329814568161964e-05\n",
            "  batch 115 loss: 2.6375262066721918e-05\n",
            "  batch 116 loss: 2.3467959836125373e-05\n",
            "  batch 117 loss: 2.372392266988754e-05\n",
            "  batch 118 loss: 2.2048773244023322e-05\n",
            "  batch 119 loss: 2.754756435751915e-05\n",
            "  batch 120 loss: 3.0263828113675118e-05\n",
            "  batch 121 loss: 2.800571545958519e-05\n",
            "  batch 122 loss: 2.092885598540306e-05\n",
            "  batch 123 loss: 2.5789355859160424e-05\n",
            "  batch 124 loss: 3.195134550333023e-05\n",
            "  batch 125 loss: 2.4453930556774138e-05\n",
            "  batch 126 loss: 2.1259598433971405e-05\n",
            "  batch 127 loss: 1.5582789666950703e-05\n",
            "  batch 128 loss: 3.2338362187147144e-05\n",
            "  batch 129 loss: 3.1594887375831604e-05\n",
            "  batch 130 loss: 1.6006961464881898e-05\n",
            "  batch 131 loss: 1.4496854506433011e-05\n",
            "  batch 132 loss: 2.5798011571168898e-05\n",
            "  batch 133 loss: 1.5335611999034883e-05\n",
            "  batch 134 loss: 1.2721494771540165e-05\n",
            "  batch 135 loss: 3.712008520960808e-05\n",
            "  batch 136 loss: 2.0920738577842714e-05\n",
            "  batch 137 loss: 1.468042191118002e-05\n",
            "  batch 138 loss: 1.5716305002570153e-05\n",
            "  batch 139 loss: 4.448438435792923e-05\n",
            "  batch 140 loss: 1.6213612630963326e-05\n",
            "  batch 141 loss: 1.1796090751886368e-05\n",
            "  batch 142 loss: 1.754785142838955e-05\n",
            "  batch 143 loss: 2.5024885311722755e-05\n",
            "  batch 144 loss: 9.620035998523235e-06\n",
            "  batch 145 loss: 2.6771849021315575e-05\n",
            "  batch 146 loss: 3.973125293850899e-05\n",
            "  batch 147 loss: 2.23588477820158e-05\n",
            "  batch 148 loss: 2.5629248470067978e-05\n",
            "  batch 149 loss: 4.413454234600067e-05\n",
            "  batch 150 loss: 3.6759134382009506e-05\n",
            "  batch 151 loss: 2.2276651114225387e-05\n",
            "  batch 152 loss: 1.8043139949440955e-05\n",
            "  batch 153 loss: 5.050092935562134e-05\n",
            "  batch 154 loss: 2.3407801985740663e-05\n",
            "  batch 155 loss: 1.8439939245581627e-05\n",
            "  batch 156 loss: 3.533191606402397e-05\n",
            "  batch 157 loss: 2.7726169675588607e-05\n",
            "  batch 158 loss: 2.239062450826168e-05\n",
            "  batch 159 loss: 3.265555202960968e-05\n",
            "  batch 160 loss: 2.133590541779995e-05\n",
            "  batch 161 loss: 3.4329093992710116e-05\n",
            "  batch 162 loss: 2.5583185255527497e-05\n",
            "  batch 163 loss: 2.18382403254509e-05\n",
            "  batch 164 loss: 2.163819968700409e-05\n",
            "  batch 165 loss: 2.1552624180912973e-05\n",
            "  batch 166 loss: 1.2102345004677772e-05\n",
            "  batch 167 loss: 4.244426265358925e-05\n",
            "  batch 168 loss: 1.9597042351961136e-05\n",
            "  batch 169 loss: 1.2332088313996791e-05\n",
            "  batch 170 loss: 1.052877213805914e-05\n",
            "  batch 171 loss: 2.587260492146015e-05\n",
            "  batch 172 loss: 2.0263075828552248e-05\n",
            "  batch 173 loss: 1.9140824675559998e-05\n",
            "  batch 174 loss: 1.3186473399400711e-05\n",
            "  batch 175 loss: 1.2889421544969082e-05\n",
            "  batch 176 loss: 1.657214015722275e-05\n",
            "  batch 177 loss: 2.6617141440510748e-05\n",
            "  batch 178 loss: 2.183457463979721e-05\n",
            "  batch 179 loss: 2.540718950331211e-05\n",
            "  batch 180 loss: 2.925936132669449e-05\n",
            "  batch 181 loss: 1.7525870352983473e-05\n",
            "  batch 182 loss: 1.3659940101206302e-05\n",
            "  batch 183 loss: 2.433437295258045e-05\n",
            "  batch 184 loss: 2.057456970214844e-05\n",
            "  batch 185 loss: 2.942034974694252e-05\n",
            "  batch 186 loss: 2.53383032977581e-05\n",
            "  batch 187 loss: 1.1922561563551427e-05\n",
            "  batch 188 loss: 2.2300828248262404e-05\n",
            "  batch 189 loss: 2.2838367149233817e-05\n",
            "  batch 190 loss: 3.5294387489557264e-05\n",
            "  batch 191 loss: 9.572953917086124e-06\n",
            "  batch 192 loss: 1.648814231157303e-05\n",
            "  batch 193 loss: 2.766207046806812e-05\n",
            "  batch 194 loss: 2.9892396181821823e-05\n",
            "  batch 195 loss: 1.093856245279312e-05\n",
            "  batch 196 loss: 3.143781796097755e-05\n",
            "  batch 197 loss: 3.055421635508537e-05\n",
            "  batch 198 loss: 2.1686851978302004e-05\n",
            "  batch 199 loss: 1.6816236078739165e-05\n",
            "  batch 200 loss: 5.225055664777756e-05\n",
            "  batch 201 loss: 1.537382509559393e-05\n",
            "  batch 202 loss: 2.2311139851808547e-05\n",
            "  batch 203 loss: 4.293091967701912e-05\n",
            "  batch 204 loss: 2.614942193031311e-05\n",
            "  batch 205 loss: 3.878169134259224e-05\n",
            "  batch 206 loss: 1.714429073035717e-05\n",
            "  batch 207 loss: 3.0323097482323646e-05\n",
            "  batch 208 loss: 1.3249466195702552e-05\n",
            "  batch 209 loss: 2.075611427426338e-05\n",
            "  batch 210 loss: 5.151799414306879e-06\n",
            "  batch 211 loss: 3.298502042889595e-05\n",
            "  batch 212 loss: 2.0958034321665764e-05\n",
            "  batch 213 loss: 2.5211861357092856e-05\n",
            "  batch 214 loss: 1.3628434389829636e-05\n",
            "  batch 215 loss: 2.4160848930478095e-05\n",
            "  batch 216 loss: 1.7148207873106002e-05\n",
            "  batch 217 loss: 2.094130776822567e-05\n",
            "  batch 218 loss: 2.81694196164608e-05\n",
            "  batch 219 loss: 1.554151065647602e-05\n",
            "  batch 220 loss: 2.379445545375347e-05\n",
            "  batch 221 loss: 3.0115922912955283e-05\n",
            "  batch 222 loss: 1.6378976404666902e-05\n",
            "  batch 223 loss: 3.376403823494911e-05\n",
            "  batch 224 loss: 1.8641922622919084e-05\n",
            "  batch 225 loss: 2.6135290041565895e-05\n",
            "  batch 226 loss: 1.663512922823429e-05\n",
            "  batch 227 loss: 2.6197927072644235e-05\n",
            "  batch 228 loss: 3.0147254467010497e-05\n",
            "  batch 229 loss: 2.7403581887483596e-05\n",
            "  batch 230 loss: 2.943546697497368e-05\n",
            "  batch 231 loss: 1.3472059741616248e-05\n",
            "  batch 232 loss: 2.6854444295167924e-05\n",
            "  batch 233 loss: 5.2648868411779405e-05\n",
            "  batch 234 loss: 9.781629778444767e-06\n",
            "  batch 235 loss: 2.110004611313343e-05\n",
            "  batch 236 loss: 6.472310516983271e-06\n",
            "  batch 237 loss: 1.65422223508358e-05\n",
            "  batch 238 loss: 2.3541465401649474e-05\n",
            "  batch 239 loss: 1.1102124117314816e-05\n",
            "  batch 240 loss: 2.051416411995888e-05\n",
            "  batch 241 loss: 1.0360782034695149e-05\n",
            "  batch 242 loss: 2.2431107237935067e-05\n",
            "  batch 243 loss: 2.8953958302736283e-05\n",
            "  batch 244 loss: 2.1696984767913818e-05\n",
            "  batch 245 loss: 1.2910901568830014e-05\n",
            "  batch 246 loss: 1.831805892288685e-05\n",
            "  batch 247 loss: 8.041487075388431e-06\n",
            "  batch 248 loss: 3.7428677082061765e-05\n",
            "  batch 249 loss: 7.971780374646187e-06\n",
            "  batch 250 loss: 3.407880663871765e-05\n",
            "  batch 251 loss: 2.4119047448039056e-05\n",
            "  batch 252 loss: 1.4711923897266389e-05\n",
            "  batch 253 loss: 2.887016162276268e-05\n",
            "  batch 254 loss: 1.0420527309179306e-05\n",
            "  batch 255 loss: 2.5980060920119286e-05\n",
            "  batch 256 loss: 1.293263304978609e-05\n",
            "  batch 257 loss: 1.4405113644897937e-05\n",
            "  batch 258 loss: 1.3958866707980632e-05\n",
            "  batch 259 loss: 3.471648693084717e-05\n",
            "  batch 260 loss: 3.672459349036217e-05\n",
            "  batch 261 loss: 2.1523622795939444e-05\n",
            "  batch 262 loss: 1.6377495601773263e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([4, 1, 448, 224])) that is different to the input size (torch.Size([4, 18, 448, 224])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  batch 263 loss: 2.352108433842659e-05\n",
            "loss tensor(0.0235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "avg loss in epoch 2.352108433842659e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Software\\Anaconda\\envs\\spider-torch-tb\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([7, 1, 448, 224])) that is different to the input size (torch.Size([7, 18, 448, 224])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOSS train 2.352108433842659e-05 valid 0.02323986031115055\n",
            "EPOCH 2:\n",
            "  batch 1 loss: 1.9189661368727683e-05\n",
            "  batch 2 loss: 3.749420493841171e-05\n",
            "  batch 3 loss: 2.191607467830181e-05\n",
            "  batch 4 loss: 1.083849836140871e-05\n",
            "  batch 5 loss: 1.2684209272265435e-05\n",
            "  batch 6 loss: 2.4678556248545648e-05\n",
            "  batch 7 loss: 1.5305672772228718e-05\n",
            "  batch 8 loss: 2.6101356372237204e-05\n",
            "  batch 9 loss: 3.0072689056396484e-05\n",
            "  batch 10 loss: 2.0354285836219787e-05\n",
            "  batch 11 loss: 2.116159349679947e-05\n",
            "  batch 12 loss: 1.707117073237896e-05\n",
            "  batch 13 loss: 3.0890600755810737e-05\n",
            "  batch 14 loss: 3.146391734480858e-05\n",
            "  batch 15 loss: 2.7164658531546594e-05\n",
            "  batch 16 loss: 2.104489505290985e-05\n",
            "  batch 17 loss: 2.2458039224147795e-05\n",
            "  batch 18 loss: 1.6835279762744902e-05\n",
            "  batch 19 loss: 2.5320740416646002e-05\n",
            "  batch 20 loss: 1.8854426220059395e-05\n",
            "  batch 21 loss: 3.887506201863289e-05\n",
            "  batch 22 loss: 1.2591764330863953e-05\n",
            "  batch 23 loss: 4.2703133076429367e-05\n",
            "  batch 24 loss: 1.4114093966782093e-05\n",
            "  batch 25 loss: 2.1209539845585823e-05\n",
            "  batch 26 loss: 2.511099725961685e-05\n",
            "  batch 27 loss: 2.023961581289768e-05\n",
            "  batch 28 loss: 1.3766433112323284e-05\n",
            "  batch 29 loss: 2.5005830451846123e-05\n",
            "  batch 30 loss: 1.1148881167173386e-05\n",
            "  batch 31 loss: 1.2934583239257336e-05\n",
            "  batch 32 loss: 1.55295766890049e-05\n",
            "  batch 33 loss: 2.162371203303337e-05\n",
            "  batch 34 loss: 1.2075067497789859e-05\n",
            "  batch 35 loss: 3.41583788394928e-05\n",
            "  batch 36 loss: 2.260901965200901e-05\n",
            "  batch 37 loss: 1.6510166227817535e-05\n",
            "  batch 38 loss: 2.3419110104441644e-05\n",
            "  batch 39 loss: 2.1362004801630972e-05\n",
            "  batch 40 loss: 3.174900636076927e-05\n",
            "  batch 41 loss: 1.5252718701958656e-05\n",
            "  batch 42 loss: 2.4569593369960786e-05\n",
            "  batch 43 loss: 1.590435579419136e-05\n",
            "  batch 44 loss: 1.9373970106244087e-05\n",
            "  batch 45 loss: 3.244123235344887e-05\n",
            "  batch 46 loss: 1.7051199451088905e-05\n",
            "  batch 47 loss: 2.8213484212756157e-05\n",
            "  batch 48 loss: 2.0600570365786552e-05\n",
            "  batch 49 loss: 3.4852489829063416e-05\n",
            "  batch 50 loss: 2.521597407758236e-05\n",
            "  batch 51 loss: 1.7617523670196532e-05\n",
            "  batch 52 loss: 2.0035041496157646e-05\n",
            "  batch 53 loss: 1.6811437904834748e-05\n",
            "  batch 54 loss: 1.6170978546142577e-05\n",
            "  batch 55 loss: 1.8553415313363077e-05\n",
            "  batch 56 loss: 1.3557890430092812e-05\n",
            "  batch 57 loss: 3.250693902373314e-05\n",
            "  batch 58 loss: 2.594275213778019e-05\n",
            "  batch 59 loss: 2.4010822176933288e-05\n",
            "  batch 60 loss: 2.4799536913633346e-05\n",
            "  batch 61 loss: 3.489181399345398e-05\n",
            "  batch 62 loss: 3.365997597575188e-05\n",
            "  batch 63 loss: 1.851092465221882e-05\n",
            "  batch 64 loss: 1.4761235564947128e-05\n",
            "  batch 65 loss: 1.4028999954462051e-05\n",
            "  batch 66 loss: 2.4713492020964622e-05\n",
            "  batch 67 loss: 2.0159970968961715e-05\n",
            "  batch 68 loss: 1.0058137588202954e-05\n",
            "  batch 69 loss: 2.3425905033946037e-05\n",
            "  batch 70 loss: 2.2811979055404663e-05\n",
            "  batch 71 loss: 3.161595016717911e-05\n",
            "  batch 72 loss: 1.9797319546341895e-05\n",
            "  batch 73 loss: 3.545087575912476e-05\n",
            "  batch 74 loss: 2.17124093323946e-05\n",
            "  batch 75 loss: 2.398235723376274e-05\n",
            "  batch 76 loss: 2.273181639611721e-05\n",
            "  batch 77 loss: 3.041377291083336e-05\n",
            "  batch 78 loss: 1.7511222511529922e-05\n",
            "  batch 79 loss: 1.1889832094311715e-05\n",
            "  batch 80 loss: 4.140373691916466e-05\n",
            "  batch 81 loss: 2.6140542700886726e-05\n",
            "  batch 82 loss: 2.5650154799222946e-05\n",
            "  batch 83 loss: 3.4969575703144075e-05\n",
            "  batch 84 loss: 2.3566687479615212e-05\n",
            "  batch 85 loss: 1.4233219437301159e-05\n",
            "  batch 86 loss: 1.99285838752985e-05\n",
            "  batch 87 loss: 1.163958851248026e-05\n",
            "  batch 88 loss: 2.0054031163454055e-05\n",
            "  batch 89 loss: 2.8320301324129104e-05\n",
            "  batch 90 loss: 1.8230672925710678e-05\n",
            "  batch 91 loss: 2.7725538238883018e-05\n",
            "  batch 92 loss: 1.980390027165413e-05\n",
            "  batch 93 loss: 1.3568182475864888e-05\n",
            "  batch 94 loss: 1.1626329272985458e-05\n",
            "  batch 95 loss: 2.1190645173192024e-05\n",
            "  batch 96 loss: 2.6657309383153915e-05\n",
            "  batch 97 loss: 1.7239831387996674e-05\n",
            "  batch 98 loss: 1.5422157011926175e-05\n",
            "  batch 99 loss: 3.1503479927778243e-05\n",
            "  batch 100 loss: 4.119274765253067e-05\n",
            "  batch 101 loss: 1.5572299249470235e-05\n",
            "  batch 102 loss: 1.900789327919483e-05\n",
            "  batch 103 loss: 2.141270413994789e-05\n",
            "  batch 104 loss: 1.0452170856297017e-05\n",
            "  batch 105 loss: 2.3576093837618827e-05\n",
            "  batch 106 loss: 2.23232451826334e-05\n",
            "  batch 107 loss: 1.832207106053829e-05\n",
            "  batch 108 loss: 4.3134849518537524e-05\n",
            "  batch 109 loss: 3.00309956073761e-05\n",
            "  batch 110 loss: 3.2335173338651654e-05\n",
            "  batch 111 loss: 3.791343048214912e-05\n",
            "  batch 112 loss: 2.8040338307619094e-05\n",
            "  batch 113 loss: 2.680128440260887e-05\n",
            "  batch 114 loss: 2.9663605615496636e-05\n",
            "  batch 115 loss: 1.2437771074473858e-05\n",
            "  batch 116 loss: 1.8969010561704635e-05\n",
            "  batch 117 loss: 2.7904605492949486e-05\n",
            "  batch 118 loss: 1.4044247567653657e-05\n",
            "  batch 119 loss: 1.5046089887619019e-05\n",
            "  batch 120 loss: 2.8422914445400237e-05\n",
            "  batch 121 loss: 3.041720762848854e-05\n",
            "  batch 122 loss: 1.9143575802445413e-05\n",
            "  batch 123 loss: 2.1390400826931e-05\n",
            "  batch 124 loss: 2.0976189523935317e-05\n",
            "  batch 125 loss: 3.0607802793383596e-05\n",
            "  batch 126 loss: 1.575888879597187e-05\n",
            "  batch 127 loss: 3.443761542439461e-05\n",
            "  batch 128 loss: 2.319556474685669e-05\n",
            "  batch 129 loss: 3.017291985452175e-05\n",
            "  batch 130 loss: 2.3000115528702737e-05\n",
            "  batch 131 loss: 3.756373003125191e-05\n",
            "  batch 132 loss: 1.942864991724491e-05\n",
            "  batch 133 loss: 2.5753108784556388e-05\n",
            "  batch 134 loss: 2.2249417379498483e-05\n",
            "  batch 135 loss: 1.3063471764326096e-05\n",
            "  batch 136 loss: 2.7978366240859033e-05\n",
            "  batch 137 loss: 2.059016190469265e-05\n",
            "  batch 138 loss: 3.354791924357414e-05\n",
            "  batch 139 loss: 2.0737072452902793e-05\n",
            "  batch 140 loss: 1.4074464328587056e-05\n",
            "  batch 141 loss: 2.186613529920578e-05\n",
            "  batch 142 loss: 1.4003872871398926e-05\n",
            "  batch 143 loss: 2.465977519750595e-05\n",
            "  batch 144 loss: 3.120605833828449e-05\n",
            "  batch 145 loss: 2.9588861390948295e-05\n",
            "  batch 146 loss: 2.5332432240247725e-05\n",
            "  batch 147 loss: 3.12981903553009e-05\n",
            "  batch 148 loss: 3.377784788608551e-05\n",
            "  batch 149 loss: 3.506796807050705e-05\n",
            "  batch 150 loss: 4.198737069964409e-05\n",
            "  batch 151 loss: 2.8974466025829316e-05\n",
            "  batch 152 loss: 2.9457008466124536e-05\n",
            "  batch 153 loss: 2.305534854531288e-05\n",
            "  batch 154 loss: 4.17424812912941e-05\n",
            "  batch 155 loss: 3.714023157954216e-05\n",
            "  batch 156 loss: 1.9349168986082076e-05\n",
            "  batch 157 loss: 1.5478456392884256e-05\n",
            "  batch 158 loss: 7.805167138576507e-05\n",
            "  batch 159 loss: 1.9538573920726774e-05\n",
            "  batch 160 loss: 2.7201516553759574e-05\n",
            "  batch 161 loss: 1.5109664760529996e-05\n",
            "  batch 162 loss: 5.793076381087303e-05\n",
            "  batch 163 loss: 2.3948356509208678e-05\n",
            "  batch 164 loss: 2.867215685546398e-05\n",
            "  batch 165 loss: 3.252029046416283e-05\n",
            "  batch 166 loss: 1.683829538524151e-05\n",
            "  batch 167 loss: 2.2296447306871415e-05\n",
            "  batch 168 loss: 2.1622952073812485e-05\n",
            "  batch 169 loss: 9.230086952447892e-05\n",
            "  batch 170 loss: 0.0002019956409931183\n",
            "  batch 171 loss: 3.4530498087406155e-05\n",
            "  batch 172 loss: 1.5024439431726933e-05\n",
            "  batch 173 loss: 2.9444048181176186e-05\n",
            "  batch 174 loss: 2.2909581661224364e-05\n",
            "  batch 175 loss: 2.6236459612846376e-05\n",
            "  batch 176 loss: 1.1708701029419898e-05\n",
            "  batch 177 loss: 1.7735300585627554e-05\n",
            "  batch 178 loss: 2.1029820665717127e-05\n",
            "  batch 179 loss: 1.906122453510761e-05\n",
            "  batch 180 loss: 2.1568013355135917e-05\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg loss in epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_loss)\n\u001b[0;32m     19\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
            "Cell \u001b[1;32mIn[16], line 61\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m#run everything while indexing through z axis\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03mfor axis in images, masks:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    optim.step()\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Gather data and report\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m#if i % 1000 == 999:\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m#print(\"goes in\")\u001b[39;00m\n\u001b[0;32m     64\u001b[0m last_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;66;03m# loss per batch\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/spider_seg_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "    print(\"avg loss in epoch\", avg_loss)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(dummy_test_dataloader):\n",
        "            vinputs, vlabels = vdata\n",
        "\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = loss_func(voutputs, vlabels)\n",
        "            running_vloss += vloss\n",
        "            \n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    \n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        #commented out saving the model for now to debug loss being 0 \n",
        "        '''\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        '''\n",
        "    epoch_number += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
