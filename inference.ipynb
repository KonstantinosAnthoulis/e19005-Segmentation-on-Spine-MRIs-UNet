{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import torch \n",
    "import numpy as np\n",
    "from models import unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Github Repositories\\LabML Unet\\spider-seg-e19005\\models\\unet.py:213: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(m.weight)\n",
      "c:\\Github Repositories\\LabML Unet\\spider-seg-e19005\\models\\unet.py:214: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(m.bias, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (conv_final): Conv2d(32, 19, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (down_convs): ModuleList(\n",
       "    (0): DownConv(\n",
       "      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): DownConv(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): DownConv(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): DownConv(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): DownConv(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (up_convs): ModuleList(\n",
       "    (0): UpConv(\n",
       "      (upconv): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): UpConv(\n",
       "      (upconv): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "        (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (2): UpConv(\n",
       "      (upconv): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "        (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (3): UpConv(\n",
       "      (upconv): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "        (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = unet.UNet(in_channels=1,num_classes=19, start_filts= 32, up_mode='upsample')\n",
    "#setting cpu for laptop inf testing\n",
    "model.load_state_dict(torch.load(\"C:/Users/Konstantinos/Desktop/Spider Inference Models Test/model_20240408_174231_7\", map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Import\" unique masks and one-hot encoding from training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9. 100. 201. 202. 203.\n",
      " 204. 205. 206. 207. 208. 209.]\n"
     ]
    }
   ],
   "source": [
    "unique_masks_a = np.array([0. ,   1. ,  2.   ,3.  , 4. ,  5. ,  6. ,  7. ,  8.,   9. ,100. ,201. ,202. ,203. ,204., 205. ,206. ,207. ,208. ,209.])\n",
    "\n",
    "val_range = np.arange(20)\n",
    "print(val_range)\n",
    "key_range = unique_masks_a #no of unique values in labels, including BG \n",
    "print(key_range)\n",
    "\n",
    "mapping_dict = dict(zip(key_range, val_range))\n",
    "\n",
    "# Define a function to perform the mapping using the dictionary\n",
    "def map_to_specified_set(value):\n",
    "    return mapping_dict.get(value, np.nan)  # Return np.nan for values not found in the mapping_dict\n",
    "\n",
    "# Create a vectorized version of the mapping function\n",
    "value_map = np.vectorize(map_to_specified_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test In MRI from images removed from training sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 320, 320)\n",
      "max 3096 min -1000\n",
      "torch.Size([1, 1, 320, 320])\n",
      "out tensor dims torch.Size([1, 19, 320, 320])\n",
      "out indices arr shape (1, 320, 320)\n",
      "out values array unique [  0.   1.   2.   3.   4.   5.   6.   9. 100. 201. 202. 203.]\n",
      "array going into sitk (320, 320)\n"
     ]
    }
   ],
   "source": [
    "in_mha = sitk.ReadImage(\"out_inference/132_t1.mha\")\n",
    "\n",
    "in_arr = sitk.GetArrayFromImage(in_mha) #3d arr\n",
    "\n",
    "if(in_arr.shape[0] > in_arr.shape[1] or  in_arr.shape[0] > in_arr.shape[2]):\n",
    "    in_arr = np.transpose(in_arr, (2, 0, 1))\n",
    "\n",
    "print(in_arr.shape)\n",
    "\n",
    "#image normalisation\n",
    "for idx in range (in_arr.shape[0]):\n",
    "    #print(in_arr[idx, :, :].shape)\n",
    "    idx_slice = in_arr[idx, : , :]\n",
    "    if (idx == 0):\n",
    "        image_max = np.max(idx_slice)\n",
    "        image_min = np.min(idx_slice)\n",
    "    else:\n",
    "        if(np.max(idx_slice) > image_max):\n",
    "            image_max = np.max(idx_slice)\n",
    "        if(np.min(idx_slice) < image_min):\n",
    "            image_min = np.min(idx_slice)\n",
    "\n",
    "print(\"max\", image_max, \"min\", image_min)\n",
    "\n",
    "out_arr = np.empty_like(in_arr)\n",
    "\n",
    "#convert each slice to tensor run inf on each slice and save in new np arr\n",
    "for idx in range(in_arr.shape[0]):\n",
    "    in_slice = in_arr[idx, :, :]\n",
    "    in_tensor = torch.from_numpy(in_slice)\n",
    "    in_tensor = (in_tensor - image_min) / (image_max - image_min)\n",
    "    in_tensor = in_tensor.to(torch.float32)\n",
    "    in_tensor = in_tensor.unsqueeze(0)\n",
    "    in_tensor = in_tensor.unsqueeze(0)\n",
    "    in_tensor.to('cpu')\n",
    "    print(in_tensor.shape) \n",
    "    out_tensor = model(in_tensor) #OK \n",
    "    print(\"out tensor dims\", out_tensor.shape)\n",
    "\n",
    "     #TODO parse one hot dict here to undo one hot encoding \n",
    "    #out_tensor_indices = torch.argmax(out_tensor, dim = 1)\n",
    "\n",
    "    #print(\"out tensor undo one hot shape\", out_tensor_indices.shape)\n",
    "\n",
    "    out_tensor_arr = out_tensor.detach().numpy()\n",
    "\n",
    "    out_indices_arr = np.argmax(out_tensor_arr, axis = 1)\n",
    "    out_values_arr = np.vectorize(lambda index: unique_masks_a[index])(out_indices_arr)\n",
    "    print(\"out indices arr shape\", out_indices_arr.shape)\n",
    "\n",
    "\n",
    "\n",
    "    out_arr = out_values_arr[0, :, :]\n",
    "\n",
    "    print(\"out values array unique\", np.unique(out_arr))\n",
    "    print(\"array going into sitk\", out_arr.shape)\n",
    "\n",
    "    out_image = sitk.GetImageFromArray(out_arr)\n",
    "    \n",
    "    sitk.WriteImage(out_image, \"C:/Users/Konstantinos/Desktop/sitk inference out/{}.mha\".format(idx))\n",
    "    \n",
    "    break #testing for one image \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the image into slices in loop and run inference model eval "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-medical-tb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
