{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add training py scripts to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "path_prefix = \"/home/kanthoulis/spider/\"\n",
    "\n",
    "training_scripts_dir = path_prefix + \"training\"\n",
    "transforms_dir = path_prefix + \"transforms\"\n",
    "image_dir = path_prefix + \"image\"\n",
    "\n",
    "\n",
    "sys.path.append(training_scripts_dir)\n",
    "sys.path.append(transforms_dir)\n",
    "sys.path.append(image_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanthoulis/jupyter-venv/lib/python3.12/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.23). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "#UNet \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import json \n",
    "import numpy as np\n",
    "import re \n",
    "import os\n",
    "import pathlib\n",
    "from natsort import natsorted\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "#model imports \n",
    "import unet\n",
    "\n",
    "import dataset_torch_numpy as dataset\n",
    "import dataset_torch_numpy_aug as dataset_aug\n",
    "import metric\n",
    "#import epoch as ep #ep not to conlfict with var name in loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "NVIDIA GeForce RTX 4090\n",
      "Using cuda:1 device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7e3c105e4d30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.device_count())  # Ensure you have more than 1 GPU\n",
    "print(torch.cuda.get_device_name(1))  # Name of the 2nd GPU (index 1)\n",
    "\n",
    "\"\"\"\n",
    "#Set GPU/Cuda Device to run model on\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "#use 2nd gpu \n",
    "device = torch.device(\"cuda:1\")\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using {device} device\")\n",
    "torch.manual_seed(46)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tensor data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "json_path = \"/home/kanthoulis/spider/tensor_data/tensor_data.json\" \n",
    "\n",
    "#Load tensor parameters from .json\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "#Assign each value to a variable\n",
    "row_max = data[\"row_max\"]\n",
    "col_max = data[\"col_max\"]\n",
    "image_tensor_min = data[\"image_tensor_min\"]\n",
    "image_tensor_max = data[\"image_tensor_max\"]\n",
    "label_tensor_min = data[\"label_tensor_min\"]\n",
    "label_tensor_max = data[\"label_tensor_max\"]\n",
    "masks_no = data[\"masks_no\"]\n",
    "masks_array = data[\"masks_array\"]\n",
    "\n",
    "print(masks_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset len 11484\n",
      "test dataset len 3197\n"
     ]
    }
   ],
   "source": [
    "path_prefix = \"/home/kanthoulis/spider/dataset/\"\n",
    "\n",
    "train_img_slice_dir = pathlib.Path(path_prefix + \"train_image_numpy\") #TODO SET PATH IN JHUB\n",
    "train_label_slice_dir = pathlib.Path(path_prefix + \"train_label_numpy\") #TODO SET PATH IN JHUB \n",
    "test_img_slice_dir = pathlib.Path(path_prefix + \"test_image_numpy\") #TODO SET PATH IN JHUB\n",
    "test_label_slice_dir= pathlib.Path(path_prefix + \"test_label_numpy\") \n",
    "\n",
    "#Create lists of filenames in directories (str)\n",
    "image_path = train_img_slice_dir\n",
    "label_path = test_label_slice_dir\n",
    "\n",
    "image_dir_list = os.listdir(image_path)\n",
    "label_dir_list = os.listdir(label_path)\n",
    "\n",
    "#Sort lists\n",
    "image_dir_list = natsorted(image_dir_list)\n",
    "label_dir_list = natsorted(label_dir_list)\n",
    "\n",
    "print(\"train dataset len\",image_dir_list.__len__())\n",
    "print(\"test dataset len\",label_dir_list.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dice_loss(pred, target, num_classes=20, smooth=1, epsilon=1e-6):\n",
    "    # Ensure target shape is [batch_size, H, W]\n",
    "    target = target.squeeze(1)  # Remove unnecessary channel dimension if present\n",
    "\n",
    "    # Apply softmax to get probabilities from logits\n",
    "    pred = F.softmax(pred, dim=1)  # Shape: [batch_size, num_classes, H, W]\n",
    "\n",
    "    # Convert target to one-hot encoding\n",
    "    target = F.one_hot(target.long(), num_classes=num_classes)  # Shape: [batch_size, H, W, num_classes]\n",
    "    target = target.permute(0, 3, 1, 2).float()  # Shape: [batch_size, num_classes, H, W]\n",
    "\n",
    "    # Compute intersection and union\n",
    "    intersection = (pred * target).sum(dim=(2, 3))  # Shape: [batch_size, num_classes]\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))  # Shape: [batch_size, num_classes]\n",
    "\n",
    "    # Prevent division by zero by adding epsilon\n",
    "    union = union + epsilon\n",
    "\n",
    "    # Compute per-class Dice score\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "    # Define class weights (0.1 for background, 1.0 for others)\n",
    "    class_weights = torch.ones(num_classes, device=pred.device)\n",
    "    class_weights[0] = 0.1  # Background class weight\n",
    "\n",
    "    # Compute weighted Dice loss\n",
    "    weighted_dice = class_weights * (1 - dice)  # Shape: [batch_size, num_classes]\n",
    "    loss = weighted_dice.mean()  # Average over batch and classes\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CrossEntropyLoss\n",
    "\n",
    "class CombinedLoss(torch.nn.Module):\n",
    "    def __init__(self, ce_weight=0.5, dice_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        weights = torch.cat([torch.tensor([0.1]), torch.ones(19)])\n",
    "        self.ce_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Cross Entropy Loss expects class probabilities and the raw target labels (not one-hot)\n",
    "        #ce_loss = self.ce_loss(outputs, torch.argmax(targets, dim=1))\n",
    "        ce_loss = self.ce_loss(outputs, targets.squeeze(1).long()) \n",
    "        \n",
    "        # Dice Loss expects one-hot encoded targets and softmax probabilities\n",
    "        dice_loss_val = dice_loss(outputs, targets)\n",
    "        \n",
    "        # Weighted sum of the two losses\n",
    "        return self.ce_weight * ce_loss + self.dice_weight * dice_loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass CombinedLoss(nn.Module):\\n    def __init__(self, ce_weight=0.5, dice_weight=0.5):\\n        super(CombinedLoss, self).__init__()\\n        self.ce_weight = ce_weight\\n        self.dice_weight = dice_weight\\n        self.ce_loss = nn.BCEWithLogitsLoss()\\n\\n    def forward(self, outputs, targets):\\n    \\n        #Compute the combined loss.\\n       # Args:\\n        #    outputs: Logits from the model (shape: B x C x H x W).\\n        #    targets: Ground truth labels (shape: B x C x H x W, binary).\\n        #Returns:\\n          #  Combined loss value.\\n         \\n        # Compute BCEWithLogitsLoss\\n        ce_loss = self.ce_loss(outputs, targets)\\n        \\n        # Compute Dice Loss\\n        dice_loss_val = dice_loss(outputs, targets)\\n        \\n        # Weighted sum of the two losses\\n        return self.ce_weight * ce_loss + self.dice_weight * dice_loss_val\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BCEWithLogits\n",
    "\"\"\"\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, ce_weight=0.5, dice_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "    \n",
    "        #Compute the combined loss.\n",
    "       # Args:\n",
    "        #    outputs: Logits from the model (shape: B x C x H x W).\n",
    "        #    targets: Ground truth labels (shape: B x C x H x W, binary).\n",
    "        #Returns:\n",
    "          #  Combined loss value.\n",
    "         \n",
    "        # Compute BCEWithLogitsLoss\n",
    "        ce_loss = self.ce_loss(outputs, targets)\n",
    "        \n",
    "        # Compute Dice Loss\n",
    "        dice_loss_val = dice_loss(outputs, targets)\n",
    "        \n",
    "        # Weighted sum of the two losses\n",
    "        return self.ce_weight * ce_loss + self.dice_weight * dice_loss_val\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet Torch - Model Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom functools import partial\\n\\nloss_func = partial(dice_loss,num_classes = masks_no)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input Output channels\n",
    "input_channels = 1 #Hounsfield scale, do not modify\n",
    "output_channels = masks_no - 1 #-1 to exclude backround value 0, do not modify\\\n",
    "\n",
    "#Model Hyperparams \n",
    "depth = 5\n",
    "#start_filts = 32\n",
    "start_filts = 64\n",
    "\n",
    "up_mode = 'upsample'\n",
    "#lr = 0.001 #adjust according to batch size \n",
    "lr = 0.0001\n",
    "batchsize = 8\n",
    "#batchsize = 8 \n",
    "#loss_func = nn.BCEWithLogitsLoss() \n",
    "#loss_func = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "\n",
    "one_hot_labels = False #false for ce true for bce\n",
    "\n",
    "#weights = torch.cat([torch.tensor([0.1]), torch.ones(19)])\n",
    "#loss_func  = torch.nn.CrossEntropyLoss(weight = weights)\n",
    "\n",
    "loss_func = CombinedLoss()\n",
    "\n",
    "loss_func.to(device)\n",
    "\n",
    "# Assigning the dice_loss function as the loss function (without calling it)\n",
    "\"\"\"\n",
    "from functools import partial\n",
    "\n",
    "loss_func = partial(dice_loss,num_classes = masks_no)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet Torch - Model & Optimizer Init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet.UNet(in_channels= input_channels,num_classes=masks_no, depth= depth, start_filts=start_filts, up_mode=up_mode) \n",
    "\n",
    "model.to(device)\n",
    "model.to(torch.float32)  \n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.001) #AdamW\n",
    "#optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0001) #AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GradScaler\n",
    "#scaler = GradScaler() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST TRAINING SESSION ONLY\n",
    "epoch_no = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_1860126/2699515293.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#NOTE: a simple script to get the file w the highest idx in the directory wouldn\\'t be hard to iterate but for now\\n    #just focusing on training the model, maybe work on it further down the line \\n\\n#TODO also load scheduler when resuming training (when used)\\n\\n#last epoch trained path\\npath = \"/home/kanthoulis/spider/models/spider_seg_19\" #last epoch pre scheduler\\n\\n\\ncheckpoint= torch.load(path)\\nprint(checkpoint.keys())\\n#optim = torch.optim.Adam(model.parameters(), lr=lr)\\noptim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.001)\\n\\n\\ndef extract_number_from_path(path):\\n    match = re.search(r\\'(\\\\d+)$\\', path)\\n    return int(match.group(1)) if match else None\\n\\nepoch_no = extract_number_from_path(path) + 1 #number for plotting in tb\\nprint(epoch_no)\\n\\nmodel.load_state_dict(checkpoint[\\'model_dict\\'])\\noptim.load_state_dict(checkpoint[\\'optimizer_dict\\'])\\n#scaler.load_state_dict(checkpoint[\\'scaler_state_dict\\'])\\n\\nmodel.to(device)\\nmodel.to(torch.float32)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#NOTE: a simple script to get the file w the highest idx in the directory wouldn't be hard to iterate but for now\n",
    "    #just focusing on training the model, maybe work on it further down the line \n",
    "\n",
    "#TODO also load scheduler when resuming training (when used)\n",
    "\n",
    "#last epoch trained path\n",
    "path = \"/home/kanthoulis/spider/models/spider_seg_19\" #last epoch pre scheduler\n",
    "\n",
    "\n",
    "checkpoint= torch.load(path)\n",
    "print(checkpoint.keys())\n",
    "#optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "\n",
    "\n",
    "def extract_number_from_path(path):\n",
    "    match = re.search(r'(\\d+)$', path)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "epoch_no = extract_number_from_path(path) + 1 #number for plotting in tb\n",
    "print(epoch_no)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_dict'])\n",
    "optim.load_state_dict(checkpoint['optimizer_dict'])\n",
    "#scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "model.to(torch.float32)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet Torch - Datasets & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11484\n",
      "3197\n"
     ]
    }
   ],
   "source": [
    "#Datasets \n",
    "train_set = dataset.SpiderDatasetNumpy(train_label_slice_dir, train_img_slice_dir, one_hot_labels = one_hot_labels, augmentation = True)\n",
    "train_eval_set = dataset.SpiderDatasetNumpy(train_label_slice_dir, train_img_slice_dir, one_hot_labels = one_hot_labels, augmentation = False)\n",
    "test_set = dataset.SpiderDatasetNumpy(test_label_slice_dir, test_img_slice_dir, one_hot_labels = one_hot_labels, augmentation = False)\n",
    "\n",
    "#Dataloaders\n",
    "train_dataloader = DataLoader(train_set, batch_size = batchsize, shuffle=True)\n",
    "train_eval_dataloader = DataLoader(train_eval_set, batch_size = 1, shuffle=True) #for more accurate metrics\n",
    "test_dataloader = DataLoader(test_set, batch_size = 1, shuffle=True) #for more accurate metrics\n",
    "\n",
    "print(len(train_eval_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Scheduler setup\n",
    "T_max = len(train_dataloader)  # One full cycle is one epoch\n",
    "eta_min = 1e-6                 # Minimum learning rate\n",
    "\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer=optim,\n",
    "    T_max=T_max,\n",
    "    eta_min=eta_min\n",
    ")\n",
    "\n",
    "#scheduler.load_state_dict(checkpoint['scheduler_dict']) # UNCOMMENT AFTER SCHEDULER IS INIT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR Scheduler CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\\n\\n# Scheduler setup\\nT_0 = len(train_dataloader)  # One epoch per cycle\\nT_mult = 2                   # Each subsequent cycle is 2x longer than the previous\\neta_min = 1e-6               # Minimum learning rate\\n\\nscheduler = CosineAnnealingWarmRestarts(\\n    optimizer=optim,\\n    T_0=T_0,\\n    T_mult=T_mult,\\n    eta_min=eta_min\\n)\\n\\nscheduler.load_state_dict(checkpoint['scheduler_dict']) #UNCOMMENT AFTER SCHEDULER IS INIT\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Scheduler setup\n",
    "T_0 = len(train_dataloader)  # One epoch per cycle\n",
    "T_mult = 2                   # Each subsequent cycle is 2x longer than the previous\n",
    "eta_min = 1e-6               # Minimum learning rate\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer=optim,\n",
    "    T_0=T_0,\n",
    "    T_mult=T_mult,\n",
    "    eta_min=eta_min\n",
    ")\n",
    "\n",
    "scheduler.load_state_dict(checkpoint['scheduler_dict']) #UNCOMMENT AFTER SCHEDULER IS INIT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR Scheduler reduceonPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Define the ReduceLROnPlateau scheduler\\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\\n    optim, \\n    mode='min', \\n    factor=0.5, \\n    patience=3, \\n    verbose=False, \\n    min_lr=1e-6)\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Define the ReduceLROnPlateau scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=3, \n",
    "    verbose=False, \n",
    "    min_lr=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#scheduler.load_state_dict(checkpoint['scheduler_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps  = 4 #4*16 = 64\n",
    "#for batch size 8 4*8 = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set no. of epochs to train for training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy, Precision, Recall, Dice, F1Score\n",
    "\n",
    "\"\"\"\n",
    "accuracy_metric = Accuracy(task='multiclass', num_classes=masks_no, average = 'macro', ignore_index = 0).to(device)\n",
    "precision_metric = Precision(task='multiclass', num_classes=masks_no, average = 'macro', ignore_index = 0).to(device)\n",
    "recall_metric = Recall(task='multiclass', num_classes=masks_no, average = 'macro', ignore_index = 0).to(device)\n",
    "dice_metric = F1Score(task='multiclass', num_classes=masks_no, average = 'macro', ignore_index = 0).to(device)\n",
    "\"\"\"\n",
    "\n",
    "accuracy_metric = Accuracy(task='multiclass', num_classes=masks_no, average = 'macro').to(device)\n",
    "precision_metric = Precision(task='multiclass', num_classes=masks_no, average = 'macro').to(device)\n",
    "recall_metric = Recall(task='multiclass', num_classes=masks_no, average = 'macro').to(device)\n",
    "dice_metric = F1Score(task='multiclass', num_classes=masks_no, average = 'macro').to(device)\n",
    "\n",
    "accuracy_metric.reset()\n",
    "dice_metric.reset()\n",
    "precision_metric.reset()\n",
    "recall_metric.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also save valid metrics locally as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the directory where the file will be stored\n",
    "csv_directory = '/home/kanthoulis/spider/runs/'  # Replace with the correct path\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_filename = os.path.join(csv_directory, 'valid_metrics.csv') #CHANGE ACCORDINGLY\n",
    "\n",
    "# Define the header for the CSV file\n",
    "header = [\n",
    "    'epoch',  # Epoch\n",
    "    'avg_tloss', 'avg_vloss',  # Training/Validation Loss\n",
    "    'avg_taccu', 'avg_vaccu',  # Training/Validation Accuracy\n",
    "    'avg_tdice', 'avg_vdice',  # Training/Validation Dice\n",
    "    'avg_tprec', 'avg_vprec',  # Training/Validation Precision\n",
    "    'avg_trecall', 'avg_vrecall'  # Training/Validation Recall\n",
    "]\n",
    "\n",
    "def save_metrics_to_csv(epoch, metrics):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(csv_filename)\n",
    "    data = []\n",
    "\n",
    "    # Read existing data if file exists\n",
    "    if file_exists:\n",
    "        with open(csv_filename, mode='r', newline='') as file:\n",
    "            reader = csv.reader(file)\n",
    "            data = list(reader)\n",
    "\n",
    "    # Check if the header exists, otherwise add it\n",
    "    if not data or data[0] != header:\n",
    "        data = [header]\n",
    "\n",
    "    # Convert epoch to string for comparison\n",
    "    epoch_str = str(epoch)\n",
    "    row_found = False\n",
    "\n",
    "    # Update row if epoch exists\n",
    "    for i in range(1, len(data)):  # Skip header row\n",
    "        if data[i][0] == epoch_str:  \n",
    "            data[i] = [epoch_str] + metrics  # Overwrite existing row\n",
    "            row_found = True\n",
    "            break\n",
    "\n",
    "    # Append new row if epoch was not found\n",
    "    if not row_found:\n",
    "        data.append([epoch_str] + metrics)\n",
    "\n",
    "    # Write the updated data back to the CSV file\n",
    "    with open(csv_filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "from torchmetrics import Accuracy, Precision, Recall, Dice\n",
    "\n",
    "def evaluate_model(dataloader, model, device, loss_func, desc):\n",
    "    # Initialize torchmetrics instances\n",
    "    # Initialize running metrics\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    model.eval()  # Ensure model is in evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for _, data in enumerate(tqdm(dataloader, desc=desc)):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "                    #with autocast(device.type): \n",
    "            with autocast():# AMP handles precision here\n",
    "                outputs = model(inputs)\n",
    "                #loss = loss_func(outputs, labels.squeeze(1).long()) #ce only\n",
    "                loss = loss_func(outputs, labels) #dice and combloss\n",
    "\n",
    "            # Convert one-hot encoded labels to class indices\n",
    "            labels_preds = labels.squeeze(1).long()  # Shape: (16, 576, 576)\n",
    "            preds = torch.argmax(outputs, dim=1)        # Shape: (16, 576, 576)\n",
    "                        # Overall Metrics\n",
    "\n",
    "            preds = preds.to(device)\n",
    "            labels_preds = labels_preds.to(device)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            accuracy_metric.update(preds, labels_preds)\n",
    "            dice_metric.update(preds, labels_preds)\n",
    "            precision_metric.update(preds, labels_preds)\n",
    "            recall_metric.update(preds, labels_preds)\n",
    "\n",
    "           \n",
    "    # Calculate averages\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_accu = accuracy_metric.compute()\n",
    "    avg_dice = dice_metric.compute()\n",
    "    avg_prec = precision_metric.compute()\n",
    "    avg_recall = recall_metric.compute()\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"avg_accu\": avg_accu,\n",
    "        \"avg_dice\": avg_dice,\n",
    "        \"avg_prec\": avg_prec,\n",
    "        \"avg_recall\": avg_recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:39<00:00,  1.89it/s, loss=0.5726]\n",
      "/tmp/ipykernel_1860126/4201957452.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():# AMP handles precision here\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.4672582568107873 Valid 0.3355000061785565\n",
      "Accuracy: Train 0.07953856885433197 Valid 0.21643495559692383\n",
      "Dice: Train 0.09814274311065674 Valid 0.196956068277359\n",
      "Precision: Train 0.20715293288230896 Valid 0.23202691972255707\n",
      "Recall: Train 0.07953856885433197 Valid 0.21643495559692383\n",
      "EPOCH 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:31<00:00,  1.91it/s, loss=0.2107]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:24<00:00, 37.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.3024936845022822 Valid 0.23467866260408252\n",
      "Accuracy: Train 0.2865317463874817 Valid 0.3527800440788269\n",
      "Dice: Train 0.31793785095214844 Valid 0.3780660033226013\n",
      "Precision: Train 0.36771342158317566 Valid 0.4304944574832916\n",
      "Recall: Train 0.2865317463874817 Valid 0.3527800440788269\n",
      "EPOCH 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.1205]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.24596328597546951 Valid 0.19777481217583695\n",
      "Accuracy: Train 0.37508800625801086 Valid 0.39824140071868896\n",
      "Dice: Train 0.4069165587425232 Valid 0.4338792562484741\n",
      "Precision: Train 0.4547930061817169 Valid 0.5353554487228394\n",
      "Recall: Train 0.37508800625801086 Valid 0.39824140071868896\n",
      "EPOCH 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:33<00:00,  1.91it/s, loss=0.1651]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.21688677606102816 Valid 0.18415338454503494\n",
      "Accuracy: Train 0.4239771366119385 Valid 0.4551699161529541\n",
      "Dice: Train 0.4559570848941803 Valid 0.47858926653862\n",
      "Precision: Train 0.5096667408943176 Valid 0.5189157724380493\n",
      "Recall: Train 0.4239771366119385 Valid 0.4551699161529541\n",
      "EPOCH 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:50<00:00,  1.86it/s, loss=0.2606]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:30<00:00, 35.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.21253709674587157 Valid 0.19194818589715246\n",
      "Accuracy: Train 0.4357191324234009 Valid 0.4346088171005249\n",
      "Dice: Train 0.4662975072860718 Valid 0.45761820673942566\n",
      "Precision: Train 0.5114985704421997 Valid 0.5013737678527832\n",
      "Recall: Train 0.4357191324234009 Valid 0.4346088171005249\n",
      "EPOCH 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:46<00:00,  1.87it/s, loss=0.1793]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.22393163278034803 Valid 0.1993191827457096\n",
      "Accuracy: Train 0.4232335686683655 Valid 0.4069657325744629\n",
      "Dice: Train 0.4534246623516083 Valid 0.4496045708656311\n",
      "Precision: Train 0.4933391213417053 Valid 0.5190132856369019\n",
      "Recall: Train 0.4232335686683655 Valid 0.4069657325744629\n",
      "EPOCH 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.1744]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:24<00:00, 37.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.2295248989264862 Valid 0.21636965910873213\n",
      "Accuracy: Train 0.41676753759384155 Valid 0.3857266306877136\n",
      "Dice: Train 0.445854127407074 Valid 0.43862032890319824\n",
      "Precision: Train 0.484252393245697 Valid 0.5409775376319885\n",
      "Recall: Train 0.41676753759384155 Valid 0.3857266306877136\n",
      "EPOCH 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:33<00:00,  1.91it/s, loss=0.3100]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.22467712044176286 Valid 0.17729008668732607\n",
      "Accuracy: Train 0.43261635303497314 Valid 0.48485618829727173\n",
      "Dice: Train 0.4593307375907898 Valid 0.5057465434074402\n",
      "Precision: Train 0.5140489339828491 Valid 0.5534946918487549\n",
      "Recall: Train 0.43261635303497314 Valid 0.48485618829727173\n",
      "EPOCH 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.0887]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.2057804601386314 Valid 0.16181685751418798\n",
      "Accuracy: Train 0.4699535369873047 Valid 0.5083541870117188\n",
      "Dice: Train 0.4985347092151642 Valid 0.5309181213378906\n",
      "Precision: Train 0.561536431312561 Valid 0.6048661470413208\n",
      "Recall: Train 0.4699535369873047 Valid 0.5083541870117188\n",
      "EPOCH 10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:35<00:00,  1.90it/s, loss=0.3854]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.17717940147582203 Valid 0.15645997605651685\n",
      "Accuracy: Train 0.530238926410675 Valid 0.5567522644996643\n",
      "Dice: Train 0.5569156408309937 Valid 0.5557965040206909\n",
      "Precision: Train 0.6016942262649536 Valid 0.6066638231277466\n",
      "Recall: Train 0.530238926410675 Valid 0.5567522644996643\n",
      "EPOCH 11:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:33<00:00,  1.91it/s, loss=0.1545]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.15575321680739065 Valid 0.13865394823786886\n",
      "Accuracy: Train 0.576461672782898 Valid 0.6136633157730103\n",
      "Dice: Train 0.5973243117332458 Valid 0.6098101139068604\n",
      "Precision: Train 0.6635087132453918 Valid 0.6666874885559082\n",
      "Recall: Train 0.576461672782898 Valid 0.6136633157730103\n",
      "EPOCH 12:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:33<00:00,  1.90it/s, loss=0.1069]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.13426729295828216 Valid 0.13069115784309124\n",
      "Accuracy: Train 0.6288871765136719 Valid 0.6231656670570374\n",
      "Dice: Train 0.6485727429389954 Valid 0.6418377757072449\n",
      "Precision: Train 0.6862696409225464 Valid 0.6865309476852417\n",
      "Recall: Train 0.6288871765136719 Valid 0.6231656670570374\n",
      "EPOCH 13:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.1087]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.1316713869317453 Valid 0.1336141995542763\n",
      "Accuracy: Train 0.6377735733985901 Valid 0.6254016160964966\n",
      "Dice: Train 0.6575742959976196 Valid 0.635201632976532\n",
      "Precision: Train 0.6914305686950684 Valid 0.6538200378417969\n",
      "Recall: Train 0.6377735733985901 Valid 0.6254016160964966\n",
      "EPOCH 14:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:35<00:00,  1.90it/s, loss=0.2121]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.14053394529750587 Valid 0.14580461401642492\n",
      "Accuracy: Train 0.6226552724838257 Valid 0.5842063426971436\n",
      "Dice: Train 0.6447391510009766 Valid 0.6176431179046631\n",
      "Precision: Train 0.679995596408844 Valid 0.6800673007965088\n",
      "Recall: Train 0.6226552724838257 Valid 0.5842063426971436\n",
      "EPOCH 15:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.0895]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.15366424515864144 Valid 0.1379816472680171\n",
      "Accuracy: Train 0.592374324798584 Valid 0.5684943199157715\n",
      "Dice: Train 0.6175147294998169 Valid 0.6055495142936707\n",
      "Precision: Train 0.6606377363204956 Valid 0.7032018899917603\n",
      "Recall: Train 0.592374324798584 Valid 0.5684943199157715\n",
      "EPOCH 16:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.2938]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.15745743966990858 Valid 0.1392408827362197\n",
      "Accuracy: Train 0.5910133123397827 Valid 0.5760812759399414\n",
      "Dice: Train 0.615480363368988 Valid 0.5904579758644104\n",
      "Precision: Train 0.6534850597381592 Valid 0.6795802116394043\n",
      "Recall: Train 0.5910133123397827 Valid 0.5760812759399414\n",
      "EPOCH 17:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:36<00:00,  1.90it/s, loss=0.1339]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.15257095263279913 Valid 0.14574363852755182\n",
      "Accuracy: Train 0.603559136390686 Valid 0.568851113319397\n",
      "Dice: Train 0.6247637271881104 Valid 0.6142050623893738\n",
      "Precision: Train 0.6588138937950134 Valid 0.6984196305274963\n",
      "Recall: Train 0.603559136390686 Valid 0.568851113319397\n",
      "EPOCH 18:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.0691]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.13202146008807886 Valid 0.12685066276390558\n",
      "Accuracy: Train 0.6440542936325073 Valid 0.6502401232719421\n",
      "Dice: Train 0.6640138626098633 Valid 0.6551306247711182\n",
      "Precision: Train 0.6948948502540588 Valid 0.6759347915649414\n",
      "Recall: Train 0.6440542936325073 Valid 0.6502401232719421\n",
      "EPOCH 19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.1017]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.114527230496028 Valid 0.12348122650457462\n",
      "Accuracy: Train 0.6814279556274414 Valid 0.6416220664978027\n",
      "Dice: Train 0.698601245880127 Valid 0.661263108253479\n",
      "Precision: Train 0.7245445847511292 Valid 0.6896767616271973\n",
      "Recall: Train 0.6814279556274414 Valid 0.6416220664978027\n",
      "EPOCH 20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:35<00:00,  1.90it/s, loss=0.0976]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.10273459679537912 Valid 0.11664455949433958\n",
      "Accuracy: Train 0.7094918489456177 Valid 0.6639636754989624\n",
      "Dice: Train 0.7228895425796509 Valid 0.6778923273086548\n",
      "Precision: Train 0.7426161766052246 Valid 0.6968669891357422\n",
      "Recall: Train 0.7094918489456177 Valid 0.6639636754989624\n",
      "EPOCH 21:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:36<00:00,  1.90it/s, loss=0.0794]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.10149886530193612 Valid 0.13024499364021\n",
      "Accuracy: Train 0.7121108174324036 Valid 0.6473627686500549\n",
      "Dice: Train 0.7258269786834717 Valid 0.6577887535095215\n",
      "Precision: Train 0.7459348440170288 Valid 0.6783124208450317\n",
      "Recall: Train 0.7121108174324036 Valid 0.6473627686500549\n",
      "EPOCH 22:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:33<00:00,  1.91it/s, loss=0.0854]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.10884201416906623 Valid 0.13653069485497793\n",
      "Accuracy: Train 0.695931077003479 Valid 0.6064261198043823\n",
      "Dice: Train 0.7118829488754272 Valid 0.6453911066055298\n",
      "Precision: Train 0.7352761626243591 Valid 0.6990727186203003\n",
      "Recall: Train 0.695931077003479 Valid 0.6064261198043823\n",
      "EPOCH 23:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.0993]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.11913821695522926 Valid 0.12604456589258195\n",
      "Accuracy: Train 0.6767969131469727 Valid 0.6495158672332764\n",
      "Dice: Train 0.6933144330978394 Valid 0.6565225124359131\n",
      "Precision: Train 0.7167555093765259 Valid 0.6760962009429932\n",
      "Recall: Train 0.6767969131469727 Valid 0.6495158672332764\n",
      "EPOCH 24:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:36<00:00,  1.90it/s, loss=0.1349]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.12287733024167816 Valid 0.1309346444575624\n",
      "Accuracy: Train 0.6694794297218323 Valid 0.6467991471290588\n",
      "Dice: Train 0.6854721307754517 Valid 0.641017496585846\n",
      "Precision: Train 0.7072228789329529 Valid 0.6429762840270996\n",
      "Recall: Train 0.6694794297218323 Valid 0.6467991471290588\n",
      "EPOCH 25:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:36<00:00,  1.90it/s, loss=0.1561]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:28<00:00, 36.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.11878863807954412 Valid 0.1302912390051923\n",
      "Accuracy: Train 0.6771906614303589 Valid 0.6665551662445068\n",
      "Dice: Train 0.6928754448890686 Valid 0.6487617492675781\n",
      "Precision: Train 0.7145400643348694 Valid 0.6391265392303467\n",
      "Recall: Train 0.6771906614303589 Valid 0.6665551662445068\n",
      "EPOCH 26:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:35<00:00,  1.90it/s, loss=0.2184]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.10752719225917105 Valid 0.14848697896676047\n",
      "Accuracy: Train 0.7012603282928467 Valid 0.6463931798934937\n",
      "Dice: Train 0.71454918384552 Valid 0.6515538692474365\n",
      "Precision: Train 0.7320274114608765 Valid 0.6703444719314575\n",
      "Recall: Train 0.7012603282928467 Valid 0.6463931798934937\n",
      "EPOCH 27:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:35<00:00,  1.90it/s, loss=0.1104]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.09433533187467906 Valid 0.1133042126593706\n",
      "Accuracy: Train 0.7314279079437256 Valid 0.6911309361457825\n",
      "Dice: Train 0.7418968677520752 Valid 0.6868436932563782\n",
      "Precision: Train 0.7548025846481323 Valid 0.6840517520904541\n",
      "Recall: Train 0.7314279079437256 Valid 0.6911309361457825\n",
      "EPOCH 28:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.0598]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.08542072640545148 Valid 0.10938904094587518\n",
      "Accuracy: Train 0.7514467835426331 Valid 0.6908345818519592\n",
      "Dice: Train 0.7588686943054199 Valid 0.6951386332511902\n",
      "Precision: Train 0.7680191397666931 Valid 0.7003975510597229\n",
      "Recall: Train 0.7514467835426331 Valid 0.6908345818519592\n",
      "EPOCH 29:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:52<00:00,  1.86it/s, loss=0.0299]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:30<00:00, 35.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.08447520621380351 Valid 0.10979876744301256\n",
      "Accuracy: Train 0.7536903619766235 Valid 0.6845632791519165\n",
      "Dice: Train 0.7605161666870117 Valid 0.6956042647361755\n",
      "Precision: Train 0.7687214612960815 Valid 0.70890873670578\n",
      "Recall: Train 0.7536903619766235 Valid 0.6845632791519165\n",
      "EPOCH 30:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [13:10<00:00,  1.82it/s, loss=0.0690]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:31<00:00, 34.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.0888396769808735 Valid 0.1180536189670758\n",
      "Accuracy: Train 0.7445186376571655 Valid 0.658248782157898\n",
      "Dice: Train 0.7526713013648987 Valid 0.6721876263618469\n",
      "Precision: Train 0.7623957395553589 Valid 0.6948509216308594\n",
      "Recall: Train 0.7445186376571655 Valid 0.658248782157898\n",
      "EPOCH 31:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:53<00:00,  1.86it/s, loss=0.1012]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.09924161650271551 Valid 0.12260816889640602\n",
      "Accuracy: Train 0.7252607345581055 Valid 0.6479763984680176\n",
      "Dice: Train 0.7358919382095337 Valid 0.6674853563308716\n",
      "Precision: Train 0.748571515083313 Valid 0.694304347038269\n",
      "Recall: Train 0.7252607345581055 Valid 0.6479763984680176\n",
      "EPOCH 32:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.1579]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.10661233513820968 Valid 0.1261946194950089\n",
      "Accuracy: Train 0.7082334756851196 Valid 0.6483059525489807\n",
      "Dice: Train 0.7197227478027344 Valid 0.6505402326583862\n",
      "Precision: Train 0.7336543798446655 Valid 0.6603543758392334\n",
      "Recall: Train 0.7082334756851196 Valid 0.6483059525489807\n",
      "EPOCH 33:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:57<00:00,  1.85it/s, loss=0.0830]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.10152954273029623 Valid 0.11721761725733754\n",
      "Accuracy: Train 0.7187005281448364 Valid 0.6660230159759521\n",
      "Dice: Train 0.728687047958374 Valid 0.6760539412498474\n",
      "Precision: Train 0.79583740234375 Valid 0.6948844194412231\n",
      "Recall: Train 0.7187005281448364 Valid 0.6660230159759521\n",
      "EPOCH 34:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [13:06<00:00,  1.83it/s, loss=0.1210]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:31<00:00, 35.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.0919832410835622 Valid 0.11554936619245616\n",
      "Accuracy: Train 0.755500316619873 Valid 0.690682053565979\n",
      "Dice: Train 0.7717121839523315 Valid 0.6887704133987427\n",
      "Precision: Train 0.8314934372901917 Valid 0.6884074211120605\n",
      "Recall: Train 0.755500316619873 Valid 0.690682053565979\n",
      "EPOCH 35:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:58<00:00,  1.84it/s, loss=0.1207]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:30<00:00, 35.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.08139133400447189 Valid 0.110243239215064\n",
      "Accuracy: Train 0.7836210131645203 Valid 0.6962454319000244\n",
      "Dice: Train 0.8005503416061401 Valid 0.695378303527832\n",
      "Precision: Train 0.8520134687423706 Valid 0.695264458656311\n",
      "Recall: Train 0.7836210131645203 Valid 0.6962454319000244\n",
      "EPOCH 36:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:49<00:00,  1.87it/s, loss=0.0271]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.07513205339343476 Valid 0.10854207929790521\n",
      "Accuracy: Train 0.8061380386352539 Valid 0.7002121806144714\n",
      "Dice: Train 0.8220266103744507 Valid 0.7001315355300903\n",
      "Precision: Train 0.862029492855072 Valid 0.7005231380462646\n",
      "Recall: Train 0.8061380386352539 Valid 0.7002121806144714\n",
      "EPOCH 37:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:33<00:00,  1.90it/s, loss=0.0689]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.07445350320605058 Valid 0.10848636482524071\n",
      "Accuracy: Train 0.8059400320053101 Valid 0.6967545747756958\n",
      "Dice: Train 0.8226892948150635 Valid 0.6996830105781555\n",
      "Precision: Train 0.8665360808372498 Valid 0.7039849758148193\n",
      "Recall: Train 0.8059400320053101 Valid 0.6967545747756958\n",
      "EPOCH 38:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:35<00:00,  1.90it/s, loss=0.1746]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.07793206243690176 Valid 0.11267995977007177\n",
      "Accuracy: Train 0.7981729507446289 Valid 0.6979196071624756\n",
      "Dice: Train 0.8147242069244385 Valid 0.6912724375724792\n",
      "Precision: Train 0.8556987047195435 Valid 0.6862249374389648\n",
      "Recall: Train 0.7981729507446289 Valid 0.6979196071624756\n",
      "EPOCH 39:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:33<00:00,  1.91it/s, loss=0.0914]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.08465912800884097 Valid 0.11955678569585415\n",
      "Accuracy: Train 0.7829086780548096 Valid 0.6803510189056396\n",
      "Dice: Train 0.8011883497238159 Valid 0.6799072027206421\n",
      "Precision: Train 0.843229353427887 Valid 0.6844862103462219\n",
      "Recall: Train 0.7829086780548096 Valid 0.6803510189056396\n",
      "EPOCH 40:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:34<00:00,  1.90it/s, loss=0.0865]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.09182779750887896 Valid 0.12892783000312302\n",
      "Accuracy: Train 0.7684118151664734 Valid 0.6979399919509888\n",
      "Dice: Train 0.7893286943435669 Valid 0.6620619297027588\n",
      "Precision: Train 0.8349224328994751 Valid 0.638970136642456\n",
      "Recall: Train 0.7684118151664734 Valid 0.6979399919509888\n",
      "EPOCH 41:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:35<00:00,  1.90it/s, loss=0.0664]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.09017895155360606 Valid 0.1193997878498889\n",
      "Accuracy: Train 0.7744488716125488 Valid 0.665544331073761\n",
      "Dice: Train 0.7934678792953491 Valid 0.6774652004241943\n",
      "Precision: Train 0.8369101285934448 Valid 0.6943203210830688\n",
      "Recall: Train 0.7744488716125488 Valid 0.665544331073761\n",
      "EPOCH 42:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:33<00:00,  1.91it/s, loss=0.0889]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.08104470560539898 Valid 0.11477705569355091\n",
      "Accuracy: Train 0.7991726398468018 Valid 0.6915225982666016\n",
      "Dice: Train 0.816358745098114 Valid 0.6857608556747437\n",
      "Precision: Train 0.8522109985351562 Valid 0.6813356876373291\n",
      "Recall: Train 0.7991726398468018 Valid 0.6915225982666016\n",
      "EPOCH 43:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:33<00:00,  1.91it/s, loss=0.0987]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.0734259724113727 Valid 0.11015178726441081\n",
      "Accuracy: Train 0.8169094324111938 Valid 0.6978208422660828\n",
      "Dice: Train 0.8319496512413025 Valid 0.7005499601364136\n",
      "Precision: Train 0.865718424320221 Valid 0.7042099237442017\n",
      "Recall: Train 0.8169094324111938 Valid 0.6978208422660828\n",
      "EPOCH 44:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:35<00:00,  1.90it/s, loss=0.0466]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.06883535757764635 Valid 0.1081553628757004\n",
      "Accuracy: Train 0.8317500948905945 Valid 0.7068795561790466\n",
      "Dice: Train 0.8445819020271301 Valid 0.7052481174468994\n",
      "Precision: Train 0.8715482950210571 Valid 0.7044336795806885\n",
      "Recall: Train 0.8317500948905945 Valid 0.7068795561790466\n",
      "EPOCH 45:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1436/1436 [12:35<00:00,  1.90it/s, loss=0.0493]\n",
      "Evaluating Test Set: 100%|██████████| 3197/3197 [01:25<00:00, 37.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: Train 0.06844220857569185 Valid 0.10934338684995233\n",
      "Accuracy: Train 0.8317460417747498 Valid 0.7043145895004272\n",
      "Dice: Train 0.8454004526138306 Valid 0.6979129910469055\n",
      "Precision: Train 0.8739330172538757 Valid 0.6933956742286682\n",
      "Recall: Train 0.8317460417747498 Valid 0.7043145895004272\n",
      "EPOCH 46:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 540/1436 [04:43<07:50,  1.90it/s, loss=0.0725]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Here, we use enumerate(training_loader) instead of\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# iter(training_loader) so that we can track the batch\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# index and do some intra-epoch reporting\u001b[39;00m\n\u001b[1;32m     27\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Every data instance is an input + label pair\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter-venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/jupyter-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/jupyter-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/jupyter-venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/spider/training/dataset_torch_numpy.py:140\u001b[0m, in \u001b[0;36mSpiderDatasetNumpy.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    136\u001b[0m     label_a \u001b[38;5;241m=\u001b[39m label_deformed_a\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m#if(self.one_hot_labels == True):\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Apply value mapping to the label\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m label_a \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_a\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Convert Numpy arrays directly to torch tensors\u001b[39;00m\n\u001b[1;32m    143\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(image_a, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/jupyter-venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2522\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_stage_2(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 2522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_as_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter-venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2515\u001b[0m, in \u001b[0;36mvectorize._call_as_normal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2512\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[1;32m   2513\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[0;32m-> 2515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vectorize_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter-venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2605\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[38;5;66;03m# Convert args to object arrays first\u001b[39;00m\n\u001b[1;32m   2603\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [asanyarray(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m-> 2605\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mnout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2608\u001b[0m     res \u001b[38;5;241m=\u001b[39m asanyarray(outputs, dtype\u001b[38;5;241m=\u001b[39motypes[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/spider/training/one_hot.py:10\u001b[0m, in \u001b[0;36mvalue_map.<locals>.map_to_specified_set\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m      7\u001b[0m mapping_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(masks_array, val_range))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define a function to perform the mapping using the dictionary\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_to_specified_set\u001b[39m(value):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_dict\u001b[38;5;241m.\u001b[39mget(value, np\u001b[38;5;241m.\u001b[39mnan)  \u001b[38;5;66;03m# Return np.nan for values not found in the mapping_dict\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create a vectorized version of the mapping function\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#timestamp = datetime.now().strftime('%Y%m%d_%H')\n",
    "#writer = SummaryWriter('runs/spider_seg_unet_epochs={}_lr={}_batchsize={}_loss=BCEWithLogits_startfilts={}_upmode={}'.format(epochs,lr, batchsize,start_filts,up_mode))\n",
    "#tb_writer = SummaryWriter('/home/kanthoulis/spider/runs_19onehot/spider_seg') #CHANE PER TRAINING SESSION AVOID TB OVERWRITES\n",
    "epoch_number = epoch_no #set to no of last epoch completed to have consistent tb plots \n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    #----------------------Train Loop---------------------------------\n",
    "    model.train(True) #set model to train\n",
    "    #Instantiate training metrics per epoch\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    accuracy_metric.reset()\n",
    "    dice_metric.reset()\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "        \n",
    "    \n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "\n",
    "    progress_bar = tqdm(train_dataloader, desc=\"Training\", leave=True)\n",
    "    for i, data in enumerate(progress_bar):\n",
    "\n",
    "\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero gradients at the start of each accumulation step\n",
    "        if i % accumulation_steps == 0:\n",
    "            optim.zero_grad()\n",
    "\n",
    "        \"\"\"\n",
    "        # Use AMP for mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels) #dice and combloss\n",
    "            #loss = loss_func(outputs, labels.squeeze(1).long()) #ce only \n",
    "\n",
    "            # Check for NaNs/Infs in outputs and labels\n",
    "            if torch.any(torch.isnan(outputs)) or torch.any(torch.isinf(outputs)):\n",
    "                print(f\"NaNs or Infs detected in outputs at batch {i}\")\n",
    "            if torch.any(torch.isnan(labels)) or torch.any(torch.isinf(labels)):\n",
    "                print(f\"NaNs or Infs detected in labels at batch {i}\")\n",
    "\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        running_loss += loss.item()\n",
    "        # Scale the loss for accumulation\n",
    "        loss = loss / accumulation_steps  # Prevent gradient explosion\n",
    "        \n",
    "        # Backpropagate with AMP\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Update weights only after accumulation steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.unscale_(optim)  # Unscale before clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "            scaler.step(optim)  # Perform optimizer step\n",
    "            scaler.update()  # Update the scaler\n",
    "            scheduler.step()  # Adjust learning rate\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)  # dice and combloss\n",
    "        # loss = loss_func(outputs, labels.squeeze(1).long())  # ce only \n",
    "\n",
    "        # Check for NaNs/Infs in outputs and labels\n",
    "        if torch.any(torch.isnan(outputs)) or torch.any(torch.isinf(outputs)):\n",
    "            print(f\"NaNs or Infs detected in outputs at batch {i}\")\n",
    "        if torch.any(torch.isnan(labels)) or torch.any(torch.isinf(labels)):\n",
    "            print(f\"NaNs or Infs detected in labels at batch {i}\")\n",
    "\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        running_loss += loss.item()\n",
    "        # Scale the loss for accumulation\n",
    "        loss = loss / accumulation_steps  # Prevent gradient explosion\n",
    "        \n",
    "        # Backpropagation without AMP\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights only after accumulation steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "            optim.step()  # Perform optimizer step\n",
    "            scheduler.step()  # Adjust learning rate\n",
    "\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1).long()  # Shape: (16, 576, 576)\n",
    "        labels_preds = labels.squeeze(1).long()\n",
    "        # Convert one-hot encoded labels to class indices\n",
    "        #labels_preds = torch.argmax(labels, dim=1)  # Shape: (16, 576, 576)\n",
    "\n",
    "        \n",
    "        # Flatten for metric computation\n",
    "        preds_flat = preds.view(-1).long()    # Shape: (16 * 576 * 576,)\n",
    "        labels_flat = labels_preds.view(-1).long()  # Shape: (16 * 576 * 576,)\n",
    "        \n",
    "        # Ensure matching shapes\n",
    "        assert preds_flat.shape == labels_flat.shape, f\"Mismatch: {preds_flat.shape} vs {labels_flat.shape}\"\n",
    "\n",
    "        preds = preds.to(device)\n",
    "        labels_preds = labels_preds.to(device)\n",
    "\n",
    "        # General Metrics\n",
    "        accuracy_metric.update(preds, labels_preds)\n",
    "        dice_metric.update(preds, labels_preds)\n",
    "        precision_metric.update(preds, labels_preds)\n",
    "        recall_metric.update(preds, labels_preds)\n",
    "\n",
    "        # Gather data and report\n",
    "        #Intra training\n",
    "        \n",
    "\n",
    "    #-----------------Eval loop------------------------------------\n",
    "    #print(\"avg loss in epoch\", avg_loss)\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    avg_accu = accuracy_metric.compute()\n",
    "    avg_dice = dice_metric.compute()\n",
    "    avg_prec = precision_metric.compute()\n",
    "    avg_recall = recall_metric.compute()\n",
    "\n",
    "    accuracy_metric.reset()\n",
    "    dice_metric.reset()\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "    \n",
    "    running_vloss = 0.0\n",
    "\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "\n",
    "    #if(epoch_number + 1) % 5 == 0:\n",
    "    if(True):\n",
    "        model.eval()\n",
    "    \n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            test_metrics = evaluate_model(\n",
    "                dataloader=test_dataloader,\n",
    "                model=model,\n",
    "                device=device,\n",
    "                loss_func=loss_func,\n",
    "                desc=\"Evaluating Test Set\"\n",
    "            )\n",
    "    \n",
    "\n",
    "            avg_vloss = test_metrics[\"avg_loss\"]\n",
    "            avg_vaccu = test_metrics[\"avg_accu\"]\n",
    "            avg_vdice = test_metrics[\"avg_dice\"]\n",
    "            avg_vprec = test_metrics[\"avg_prec\"]\n",
    "            avg_vrecall = test_metrics[\"avg_recall\"]\n",
    "            \n",
    "            # Scheduler\n",
    "            # scheduler.step(avg_vloss)\n",
    "            \n",
    "            # Logging\n",
    "            print('Loss: Train {} Valid {}'.format(avg_loss, avg_vloss))\n",
    "            print('Accuracy: Train {} Valid {}'.format(avg_accu, avg_vaccu))\n",
    "            print('Dice: Train {} Valid {}'.format(avg_dice, avg_vdice))\n",
    "            print('Precision: Train {} Valid {}'.format(avg_prec, avg_vprec))\n",
    "            print('Recall: Train {} Valid {}'.format(avg_recall, avg_vrecall))\n",
    "            \n",
    "            \n",
    "\n",
    "            # Replace with your calculated metrics\n",
    "            metrics_for_csv = [\n",
    "                avg_loss, avg_vloss,    # Training/Validation Loss\n",
    "                avg_accu, avg_vaccu,    # Training/Validation Accuracy\n",
    "                avg_dice, avg_vdice,    # Training/Validation Dice\n",
    "                avg_prec, avg_vprec,    # Training/Validation Precision\n",
    "                avg_recall, avg_vrecall,  # Training/Validation Recall\n",
    "            ]\n",
    "\n",
    "\n",
    "            save_metrics_to_csv(epoch_number + 1, metrics_for_csv)\n",
    "                    \n",
    "    \n",
    "    #Change path to save model accordingly     \n",
    "    model_path = '/home/kanthoulis/spider/models/spider_seg_{}'.format(epoch_number) #CHANGE ACCORDINGLY\n",
    "\n",
    "    \n",
    "    torch.save({\n",
    "        'model_dict': model.state_dict(),\n",
    "        'optimizer_dict': optim.state_dict(),\n",
    "        'scheduler_dict': scheduler.state_dict(),\n",
    "        #'scaler_state_dict': scaler.state_dict() #amp\n",
    "    }, model_path)\n",
    "    \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHUT DOWN KERNELS AFTER TRAINING TO FREE UP MEMORY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-venv",
   "language": "python",
   "name": "jupyter-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
