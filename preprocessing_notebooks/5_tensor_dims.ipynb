{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfddf358-d5ca-4508-a829-abdce4f3b12f",
   "metadata": {},
   "source": [
    "Add necessary directories to PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "399c64e2-93d5-4d7d-a681-9e0e3dc06e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "path_prefix = \"/home/kanthoulis/spider/\"\n",
    "\n",
    "transforms_dir = path_prefix + \"transforms\"\n",
    "image_dir = path_prefix + \"image\"\n",
    "\n",
    "sys.path.append(transforms_dir)\n",
    "sys.path.append(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d612ae1-504e-4c8f-8b22-36a7860e52ac",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3513319b-22c5-477e-83fb-61f3324e6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from natsort import natsorted\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mri_slice\n",
    "import array_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a46d2-1932-46b1-ad4b-2ed1a0de4064",
   "metadata": {},
   "source": [
    "Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e4ffd5e-dde8-46cd-a465-840149f3e348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images count 45936\n",
      "label images count 45936\n"
     ]
    }
   ],
   "source": [
    "path_prefix = \"/home/kanthoulis/spider/dataset/\"\n",
    "\n",
    "train_img_slice_dir = pathlib.Path(path_prefix + \"train_augmented_image_slices\")\n",
    "train_label_slice_dir = pathlib.Path(path_prefix + \"train_augmented_label_slices\")\n",
    "\n",
    "image_path = train_img_slice_dir\n",
    "label_path = train_label_slice_dir\n",
    "\n",
    "image_dir_list = os.listdir(image_path)\n",
    "label_dir_list = os.listdir(label_path)\n",
    "\n",
    "#Sort directories so that each image has its corresponding label on same idx \n",
    "image_dir_list = natsorted(image_dir_list)\n",
    "label_dir_list = natsorted(label_dir_list)\n",
    "\n",
    "#Empty lists to hold row and col dimensions of images\n",
    "row_list = []\n",
    "col_list = []\n",
    "\n",
    "dirlen_image = len(os.listdir(image_path))\n",
    "dirlen_label = len(os.listdir(label_path))\n",
    "\n",
    "print(\"train images count\", dirlen_image)\n",
    "print(\"label images count\", dirlen_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1fdc1-83fa-4339-a198-0a5e60c9c247",
   "metadata": {},
   "source": [
    "Lopp through training dataset to get parameters for creating tensors in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02fa76b0-a9f1-4642-bd55-2a52256220f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 45936/45936 [01:00<00:00, 760.80image/s] \n",
      "Processing labels: 100%|██████████| 45936/45936 [02:19<00:00, 328.25label/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"row max:\", max(row_list))\\nprint(\"col max:\", max(col_list))\\n\\nprint(\"image tensor min\", image_tensor_min)\\nprint(\"image tensor max\", image_tensor_max)\\nprint(\"label tensor min\", label_tensor_min)\\nprint(\"label tensor max\", label_tensor_max)\\n\\nprint(\"amount of masks\", len(unique_masks_a))\\nprint(\"masks array\", unique_masks_a)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Loop through each image in the training set\n",
    "for idx in tqdm(range(0, dirlen_image), desc=\"Processing images\", unit=\"image\"):\n",
    "    #print(\"idx\", idx)\n",
    "    #Get image and corresponding label paths\n",
    "    image_file_path = image_path.joinpath(image_dir_list[idx])\n",
    "    #label_path = label_path.joinpath(label_dir_list[idx])#first part before joinpath is pathlib.Path, second part is the directory of the file \n",
    "    \n",
    "    #Read image and label\n",
    "    image = mri_slice.Mri_Slice(image_file_path)\n",
    "    #label = mri_slice.Mri_Slice(label_path)\n",
    "\n",
    "    #Get arrays\n",
    "    image_a = image.hu_a\n",
    "    #label_a = label.hu_a\n",
    "\n",
    "    #Comb through the dataset to find max ROI dimensions as well as min/max image values for tensor normalisation later \n",
    "    if(idx ==0):\n",
    "        image_tensor_min = np.min(image_a)\n",
    "        image_tensor_max = np.max(image_a)\n",
    "        #label_tensor_min = np.min(label_a)\n",
    "        #label_tensor_max = np.max(label_a)\n",
    "        \n",
    "        #unique_masks_a = np.unique(label_a)\n",
    "    else:\n",
    "        if(np.min(image_a) < image_tensor_min):\n",
    "            image_tensor_min = np.min(image_a)\n",
    "            image_tensor_min_dir = image_file_path\n",
    "        # if(np.min(label_a) < label_tensor_min):\n",
    "        #   label_tensor_min = np.min(label_a)\n",
    "        if(np.max(image_a) > image_tensor_max):\n",
    "            image_tensor_max = np.max(image_a)\n",
    "        # if(np.max(label_a) > label_tensor_max):\n",
    "        #   label_tensor_max = np.max(label_a)\n",
    "    \n",
    "    #Find amount of unique masks for one-hot encoding \n",
    "    #  current_masks_a = np.unique(label_a)\n",
    "    # if(len(current_masks_a) > len(unique_masks_a)):\n",
    "    #   unique_masks_a = current_masks_a\n",
    "    \n",
    "    #Add values to lists\n",
    "    row_list.append(image_a.shape[0]) \n",
    "    col_list.append(image_a.shape[1])  \n",
    "\n",
    "# Loop through each label\n",
    "for idx_label in tqdm(range(0, dirlen_label), desc=\"Processing labels\", unit=\"label\"):\n",
    "    #print(\"Processing Label idx:\", idx_label)\n",
    "    \n",
    "    # Get label path\n",
    "    label_file_path = label_path.joinpath(label_dir_list[idx_label])\n",
    "\n",
    "    # Read label\n",
    "    label = mri_slice.Mri_Slice(label_file_path)\n",
    "    \n",
    "    # Get label array\n",
    "    label_a = label.hu_a\n",
    "    \n",
    "    # Find min and max values for labels\n",
    "    if idx_label == 0:\n",
    "        # Initialize min/max values for labels\n",
    "        label_tensor_min = np.min(label_a)\n",
    "        label_tensor_max = np.max(label_a)\n",
    "        \n",
    "        # Initialize unique masks\n",
    "        unique_masks_a = np.unique(label_a)\n",
    "    else:\n",
    "        # Update min/max if necessary\n",
    "        if np.min(label_a) < label_tensor_min:\n",
    "            label_tensor_min = np.min(label_a)\n",
    "        if np.max(label_a) > label_tensor_max:\n",
    "            label_tensor_max = np.max(label_a)\n",
    "        \n",
    "        # Update unique masks for one-hot encoding if new masks are found\n",
    "        current_masks_a = np.unique(label_a)\n",
    "        if len(current_masks_a) > len(unique_masks_a):\n",
    "            unique_masks_a = current_masks_a\n",
    "\n",
    "# Get max values from lists\n",
    "row_dim_max = max(row_list)\n",
    "col_dim_max = max(col_list)\n",
    "\n",
    "# Nearest Multiples of 16 for Unet\n",
    "row_dim_max = ((row_dim_max + 15) // 16) * 16 \n",
    "col_dim_max = ((col_dim_max + 15) // 16) * 16\n",
    "\n",
    "# Prints\n",
    "'''\n",
    "print(\"row max:\", max(row_list))\n",
    "print(\"col max:\", max(col_list))\n",
    "\n",
    "print(\"image tensor min\", image_tensor_min)\n",
    "print(\"image tensor max\", image_tensor_max)\n",
    "print(\"label tensor min\", label_tensor_min)\n",
    "print(\"label tensor max\", label_tensor_max)\n",
    "\n",
    "print(\"amount of masks\", len(unique_masks_a))\n",
    "print(\"masks array\", unique_masks_a)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3efe175-fe94-47ad-8a69-e8ae92bea674",
   "metadata": {},
   "source": [
    "Unique masks only specific to Spider Grand Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e75508a-2de2-4923-a024-371a5a66582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_masks_a = np.array([0. ,   1. ,  2.   ,3.  , 4. ,  5. ,  6. ,  7. ,  8.,   9. ,100. ,201. ,202. ,203. ,204., 205. ,206. ,207. ,208. ,209.]) #setting manually to save time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf097af-3b0c-4b5c-ade7-3fbb4673ed07",
   "metadata": {},
   "source": [
    "Create and write tensor_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988b57dc-ad6e-4f31-be86-7b38bd342ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_max: <class 'int'>\n",
      "col_max: <class 'int'>\n",
      "image_tensor_min: <class 'numpy.float32'>\n",
      "image_tensor_max: <class 'numpy.float32'>\n",
      "label_tensor_min: <class 'float'>\n",
      "label_tensor_max: <class 'float'>\n",
      "masks_no: <class 'int'>\n",
      "masks_array: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Save to .json for easy access when training the model \n",
    "data = {\n",
    "    \"row_max\": row_dim_max,\n",
    "    \"col_max\": col_dim_max,\n",
    "    \"image_tensor_min\": image_tensor_min,\n",
    "    \"image_tensor_max\": image_tensor_max,\n",
    "    \"label_tensor_min\": 0.0, #setting manually to save time\n",
    "    \"label_tensor_max\": 209.0, #setting manually to save time\n",
    "    \"masks_no\": len(unique_masks_a),\n",
    "    \"masks_array\": unique_masks_a\n",
    "}\n",
    "\n",
    "# Print types of elements in the data dictionary to check for non-serializable types\n",
    "for key, value in data.items():\n",
    "    print(f\"{key}: {type(value)}\")\n",
    "\n",
    "# Function to convert numpy data types to native Python types\n",
    "def convert_to_native_types(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert all data to native Python types\n",
    "data = {key: convert_to_native_types(value) for key, value in data.items()}\n",
    "\n",
    "# Set the path for the JSON file\n",
    "\n",
    "file_path = \"/home/kanthoulis/spider/tensor_data/tensor_data.json\"\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac538f-e54d-4b8a-acc0-0864003c6107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-venv",
   "language": "python",
   "name": "jupyter-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
